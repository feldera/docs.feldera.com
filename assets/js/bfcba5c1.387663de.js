"use strict";(self.webpackChunkfeldera_docs=self.webpackChunkfeldera_docs||[]).push([[9830],{18764:(e,n,t)=>{t.d(n,{A:()=>a});const a=t.p+"assets/images/part3-arch-b769618b3a3223d509b2f71612619fc6.png"},25023:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>a,toc:()=>l});const a=JSON.parse('{"id":"use_cases/batch/part3","title":"Part 3. Working with Historical and Real Time Data","description":"Architecture Diagram showing Historical and Live Data Ingestion","source":"@site/docs/use_cases/batch/part3.md","sourceDirName":"use_cases/batch","slug":"/use_cases/batch/part3","permalink":"/use_cases/batch/part3","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"docsSidebar","previous":{"title":"Part 2. Convert the Batch Job into a Feldera Pipeline","permalink":"/use_cases/batch/part2"},"next":{"title":"Part 4. Sending Output to Multiple Destinations","permalink":"/use_cases/batch/part4"}}');var i=t(74848),s=t(28453);const r={},o="Part 3. Working with Historical and Real Time Data",c={},l=[{value:"Kafka and Delta Lake",id:"kafka-and-delta-lake",level:2},{value:"Connector Orchestration with Labels",id:"connector-orchestration-with-labels",level:3},{value:"Start Reading Kafka Messages from a Specific Point",id:"start-reading-kafka-messages-from-a-specific-point",level:3},{value:"Takeaways",id:"takeaways",level:2}];function d(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"part-3-working-with-historical-and-real-time-data",children:"Part 3. Working with Historical and Real Time Data"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"Architecture Diagram showing Historical and Live Data Ingestion",src:t(18764).A+"",width:"1696",height:"748"})}),"\n",(0,i.jsxs)(n.p,{children:["When a Feldera pipeline starts running, it often needs to ingest historical data\naccumulated in the source database over an extended period (months or years)\nbefore processing new real-time inputs. This process is known as ",(0,i.jsx)(n.strong,{children:"backfill"}),"."]}),"\n",(0,i.jsxs)(n.p,{children:["In some cases, both historical and real-time data can be ingested from the same\ndata source. For example, in ",(0,i.jsx)(n.a,{href:"part2",children:"Part 2"})," of this tutorial, we configured the\nDelta Lake connector to read the current snapshot of the table before following\nnew updates in the table's transaction log."]}),"\n",(0,i.jsx)(n.p,{children:"However, in many scenarios, historical and real-time data must be ingested from\ndifferent sources. Consider the following common setup:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Data is streamed into the system via Kafka."}),"\n",(0,i.jsx)(n.li,{children:"A periodic job stores Kafka messages in a database table, e.g., once per hour."}),"\n",(0,i.jsx)(n.li,{children:"The Kafka topic has a retention period of one day, meaning historical data older\nthan a day is only available in the database."}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"In this case the pipeine needs to backfill historical data from the database\nbefore switching to real-time ingest from Kafka."}),"\n",(0,i.jsx)(n.h2,{id:"kafka-and-delta-lake",children:"Kafka and Delta Lake"}),"\n",(0,i.jsxs)(n.p,{children:["In this section of the tutorial, we will implement this scenario by configuring\nthe ",(0,i.jsx)(n.code,{children:"LINEITEM"})," table to first read historical data from a Delta Lake table and\nthen ingest live data from Kafka. To achieve this, we will leverage three Feldera\nfeatures:"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Multiple Input Connectors"}),": A Feldera table can be configured with multiple\ninput connectors, allowing it to read from different sources."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.a,{href:"https://docs.feldera.com/connectors/orchestration/",children:(0,i.jsx)(n.strong,{children:"Connector Orchestration"})}),":\nBy default, all input connectors start running when the pipeline initializes.\nConnector orchestration enables users to activate or deactivate connectors on\ndemand, providing control over the timing and sequence of data ingestion."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.a,{href:"https://docs.feldera.com/connectors/sources/kafka#starting-from-a-specific-offset",children:(0,i.jsx)(n.strong,{children:"Kafka: Starting From a Specified Offset"})}),":\nFeldera supports reading a Kafka topic starting from a specific offset in a specific\npartition. This allows users to skip data previously synced from Kafka to the database.\nFor this example, we assume that our Kafka instance has a single partition\n",(0,i.jsx)(n.strong,{children:"(0)"})," and that data has been synced to Delta Lake up to ",(0,i.jsx)(n.strong,{children:"offset 41"}),".\nWe now want Feldera to start reading from ",(0,i.jsx)(n.strong,{children:"offset 42"}),"."]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:'CREATE TABLE LINEITEM (\n        L_ORDERKEY    INTEGER NOT NULL,\n        L_PARTKEY     INTEGER NOT NULL,\n        L_SUPPKEY     INTEGER NOT NULL,\n        L_LINENUMBER  INTEGER NOT NULL,\n        L_QUANTITY    DECIMAL(15,2) NOT NULL,\n        L_EXTENDEDPRICE  DECIMAL(15,2) NOT NULL,\n        L_DISCOUNT    DECIMAL(15,2) NOT NULL,\n        L_TAX         DECIMAL(15,2) NOT NULL,\n        L_RETURNFLAG  CHAR(1) NOT NULL,\n        L_LINESTATUS  CHAR(1) NOT NULL,\n        L_SHIPDATE    DATE NOT NULL,\n        L_COMMITDATE  DATE NOT NULL,\n        L_RECEIPTDATE DATE NOT NULL,\n        L_SHIPINSTRUCT CHAR(25) NOT NULL,\n        L_SHIPMODE     CHAR(10) NOT NULL,\n        L_COMMENT      VARCHAR(44) NOT NULL\n) WITH (\n \'connectors\' = \'[\n  {\n    "labels": ["lineitem.historical"],\n    "transport": {\n      "name": "delta_table_input",\n      "config": {\n        "uri": "s3://batchtofeldera/lineitem",\n        "aws_skip_signature": "true",\n        "aws_region": "ap-southeast-2",\n        "mode": "snapshot"\n      }\n    }\n  },\n  {\n    "labels": ["lineitem.live"],\n    "start_after": "lineitem.historical",\n    "transport": {\n        "name": "kafka_input",\n        "config": {\n            "topic": "lineitem",\n            "start_from": {"offsets": [42]},\n            "bootstrap.servers": "localhost:9092"\n        }\n    },\n    "format": {\n        "name": "json",\n        "config": {\n            "update_format": "insert_delete",\n            "array": false\n        }\n    }\n  }]\');\n'})}),"\n",(0,i.jsx)(n.h3,{id:"connector-orchestration-with-labels",children:"Connector Orchestration with Labels"}),"\n",(0,i.jsxs)(n.p,{children:["Connectors can be assigned arbitrary text labels for use in connector orchestration. These labels are typically chosen to reflect the connector's role in the pipeline. Here, we label the Delta Lake input connector  ",(0,i.jsx)(n.strong,{children:"lineitem.historical"}),"\nas it fetches the historical data from Delta Lake. We want the live data to be loaded only after\nthe historical data. To do this, in the Kafka connector definition, we specify\nthat it has to ",(0,i.jsx)(n.strong,{children:"start_after"})," the connector with label ",(0,i.jsx)(n.code,{children:"lineitem.historical"})," has completed:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-json",children:'// lineitem.live\n    "start_after": "lineitem.historical",\n'})}),"\n",(0,i.jsx)(n.admonition,{type:"note",children:(0,i.jsxs)(n.p,{children:["It is also possible to use the Feldera API and Feldera CLI tool ",(0,i.jsx)(n.code,{children:"fda"})," to ",(0,i.jsx)(n.strong,{children:"pause"})," and ",(0,i.jsx)(n.strong,{children:"resume"}),"\nconnectors (see: ",(0,i.jsx)(n.a,{href:"https://docs.feldera.com/connectors/orchestration/",children:"Input Connector Orchestration"}),")."]})}),"\n",(0,i.jsx)(n.h3,{id:"start-reading-kafka-messages-from-a-specific-point",children:"Start Reading Kafka Messages from a Specific Point"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"Architecture Diagram showing Kafka Messages Ingestion from a Specific Point",src:t(45626).A+"",width:"1660",height:"678"})}),"\n",(0,i.jsxs)(n.p,{children:["The Kafka input connector can be configured to start reading messages from a specific offset in the Kafka topic.\nTo this end, the user specifies the ",(0,i.jsx)(n.strong,{children:"start_from"})," property with partition offsets."]}),"\n",(0,i.jsxs)(n.p,{children:["To start reading topic ",(0,i.jsx)(n.strong,{children:"lineitem"}),", partition ",(0,i.jsx)(n.strong,{children:"0"})," from offset ",(0,i.jsx)(n.strong,{children:"42"}),", in the connector configuration, we do:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-json",children:'// lineitem.live\n// config:\n            "start_from": {"offsets": [42]}\n'})}),"\n",(0,i.jsx)(n.admonition,{type:"important",children:(0,i.jsxs)(n.p,{children:["In cases with ",(0,i.jsx)(n.strong,{children:"multiple partitions"}),", it is necessary to\n",(0,i.jsx)(n.strong,{children:"specify the offset for each partition"}),"."]})}),"\n",(0,i.jsx)(n.h2,{id:"takeaways",children:"Takeaways"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Feldera simplifies working with historical and real-time data within the same pipeine by specifying multiple\nconnectors for the same table to read from different data sources."}),"\n",(0,i.jsxs)(n.li,{children:["You can specify one connector to ",(0,i.jsx)(n.strong,{children:"start_after"})," another connector, ensuring that the\nhistorical data is read first."]}),"\n",(0,i.jsxs)(n.li,{children:["You can specify the partition and offset to ",(0,i.jsx)(n.strong,{children:"start_from"})," a specified point in Kafka."]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"In upcoming articles, we will discuss how to avoid backfilling historical data every time you\nrestart the pipeline and send outputs of the pipeline to multiple destinations."})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},28453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>o});var a=t(96540);const i={},s=a.createContext(i);function r(e){const n=a.useContext(s);return a.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),a.createElement(s.Provider,{value:n},e.children)}},45626:(e,n,t)=>{t.d(n,{A:()=>a});const a=t.p+"assets/images/part3-kafka-offset-0faaa769b4060ea1942107adde40caab.png"}}]);