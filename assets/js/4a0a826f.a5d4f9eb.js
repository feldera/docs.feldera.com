"use strict";(self.webpackChunkfeldera_docs=self.webpackChunkfeldera_docs||[]).push([[193],{28453:(e,t,n)=>{n.d(t,{R:()=>o,x:()=>d});var s=n(96540);const i={},r=s.createContext(i);function o(e){const t=s.useContext(r);return s.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function d(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),s.createElement(r.Provider,{value:t},e.children)}},72431:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>a,contentTitle:()=>d,default:()=>h,frontMatter:()=>o,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"connectors/sources/delta","title":"Delta Lake input connector","description":"This page describes configuration options specific to the Delta Lake connector.","source":"@site/docs/connectors/sources/delta.md","sourceDirName":"connectors/sources","slug":"/connectors/sources/delta","permalink":"/connectors/sources/delta","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"docsSidebar","previous":{"title":"HTTP GET (URL)","permalink":"/connectors/sources/http-get"},"next":{"title":"Apache Iceberg","permalink":"/connectors/sources/iceberg"}}');var i=n(74848),r=n(28453);const o={},d="Delta Lake input connector",a={},c=[{value:"Delta Lake input connector configuration",id:"delta-lake-input-connector-configuration",level:2},{value:"Storage parameters",id:"storage-parameters",level:3},{value:"Data type mapping",id:"data-type-mapping",level:2},{value:"Ingesting time series data from a Delta Lake",id:"ingesting-time-series-data-from-a-delta-lake",level:2},{value:"Example",id:"example",level:3},{value:"Additional examples",id:"additional-examples",level:2}];function l(e){const t={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(t.header,{children:(0,i.jsx)(t.h1,{id:"delta-lake-input-connector",children:"Delta Lake input connector"})}),"\n",(0,i.jsx)(t.admonition,{type:"note",children:(0,i.jsxs)(t.p,{children:["This page describes configuration options specific to the Delta Lake connector.\nSee ",(0,i.jsx)(t.a,{href:"/connectors/",children:"top-level connector documentation"})," for general information\nabout configuring input and output connectors."]})}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.a,{href:"https://delta.io/",children:"Delta Lake"})," is a popular open table format based on Parquet files.\nIt is typically used with the ",(0,i.jsx)(t.a,{href:"https://spark.apache.org/",children:"Apache Spark"})," runtime.\nData in a Delta Lake is organized in tables, stored in\na file system or an object stores like ",(0,i.jsx)(t.a,{href:"https://aws.amazon.com/s3/",children:"AWS S3"}),",\n",(0,i.jsx)(t.a,{href:"https://cloud.google.com/storage",children:"Google GCS"}),", or\n",(0,i.jsx)(t.a,{href:"https://azure.microsoft.com/en-us/products/storage/blobs",children:"Azure Blob Storage"}),"."]}),"\n",(0,i.jsxs)(t.p,{children:["The Delta Lake input connector supports checkpoint and resume and\nat-least-once ",(0,i.jsx)(t.a,{href:"/pipelines/fault-tolerance",children:"fault tolerance"}),", but not\nexactly once fault tolerance."]}),"\n",(0,i.jsx)(t.h2,{id:"delta-lake-input-connector-configuration",children:"Delta Lake input connector configuration"}),"\n",(0,i.jsxs)(t.table,{children:[(0,i.jsx)(t.thead,{children:(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.th,{children:"Property"}),(0,i.jsx)(t.th,{children:"Type"}),(0,i.jsx)(t.th,{children:"Default"}),(0,i.jsx)(t.th,{children:"Description"})]})}),(0,i.jsxs)(t.tbody,{children:[(0,i.jsxs)(t.tr,{children:[(0,i.jsxs)(t.td,{children:[(0,i.jsx)(t.code,{children:"uri"}),"*"]}),(0,i.jsx)(t.td,{children:"string"}),(0,i.jsx)(t.td,{}),(0,i.jsx)(t.td,{children:'Table URI, e.g., "s3://feldera-fraud-detection-data/demographics_train"'})]}),(0,i.jsxs)(t.tr,{children:[(0,i.jsxs)(t.td,{children:[(0,i.jsx)(t.code,{children:"mode"}),"*"]}),(0,i.jsx)(t.td,{children:"enum"}),(0,i.jsx)(t.td,{}),(0,i.jsxs)(t.td,{children:["Table read mode. Three options are available: ",(0,i.jsxs)("ul",{children:[" ",(0,i.jsxs)("li",{children:[(0,i.jsx)(t.code,{children:"snapshot"})," - read a snapshot of the table and stop."]})," ",(0,i.jsxs)("li",{children:[(0,i.jsx)(t.code,{children:"follow"})," - follow the changelog of the table, only ingesting changes (new and deleted rows)"]})," ",(0,i.jsxs)("li",{children:[(0,i.jsx)(t.code,{children:"snapshot_and_follow"})," - Read a snapshot of the table before switching to the ",(0,i.jsx)(t.code,{children:"follow"})," mode.  This mode implements the backfill pattern where we load historical data for the table before ingesting the stream of real-time updates."]}),(0,i.jsxs)("li",{children:[(0,i.jsx)(t.code,{children:"cdc"})," - Change-Data-Capture (CDC) mode. The table behaves as an append-only log where every row represents an insert or delete action.  The order of actions is determined by the ",(0,i.jsx)(t.code,{children:"cdc_order_by"})," property, and the type of each action is determined by the ",(0,i.jsx)(t.code,{children:"cdc_delete_filter"})," property. In this mode, the connector does not read the initial snapshot of the table and follows the transaction log starting from the version of the table specified by the ",(0,i.jsx)(t.code,{children:"version"})," or ",(0,i.jsx)(t.code,{children:"datetime"})," property."]})," "]})]})]}),(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:(0,i.jsx)(t.code,{children:"timestamp_column"})}),(0,i.jsx)(t.td,{children:"string"}),(0,i.jsx)(t.td,{}),(0,i.jsxs)(t.td,{children:["Table column that serves as an event timestamp. When this option is specified, and ",(0,i.jsx)(t.code,{children:"mode"})," is one of ",(0,i.jsx)(t.code,{children:"snapshot"})," or ",(0,i.jsx)(t.code,{children:"snapshot_and_follow"}),", table rows are ingested in the timestamp order, respecting the ",(0,i.jsx)(t.a,{href:"/sql/streaming#lateness-expressions",children:(0,i.jsx)(t.code,{children:"LATENESS"})})," property of the column: each ingested row has a timestamp no more than ",(0,i.jsx)(t.code,{children:"LATENESS"})," time units earlier than the most recent timestamp of any previously ingested row.  See details ",(0,i.jsx)(t.a,{href:"#ingesting-time-series-data-from-a-delta-lake",children:"below"}),"."]})]}),(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:(0,i.jsx)(t.code,{children:"filter"})}),(0,i.jsx)(t.td,{children:"string"}),(0,i.jsx)(t.td,{}),(0,i.jsxs)(t.td,{children:[(0,i.jsx)("p",{children:"Optional row filter."})," ",(0,i.jsxs)("p",{children:["When specified, only rows that satisfy the filter condition are read from the delta table. The condition must be a valid SQL Boolean expression that can be used in the ",(0,i.jsx)(t.code,{children:"where"})," clause of the ",(0,i.jsx)(t.code,{children:"select * from my_table where ..."})," query."]})]})]}),(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:(0,i.jsx)(t.code,{children:"snapshot_filter"})}),(0,i.jsx)(t.td,{children:"string"}),(0,i.jsx)(t.td,{}),(0,i.jsxs)(t.td,{children:[(0,i.jsx)("p",{children:"Optional snapshot filter."}),(0,i.jsxs)("p",{children:["This option is only valid when ",(0,i.jsx)(t.code,{children:"mode"})," is set to ",(0,i.jsx)(t.code,{children:"snapshot"})," or ",(0,i.jsx)(t.code,{children:"snapshot_and_follow"}),". When specified, only rows that satisfy the filter condition are included in the snapshot."]})," ",(0,i.jsxs)("p",{children:["The condition must be a valid SQL Boolean expression that can be used in  the ",(0,i.jsx)(t.code,{children:"where"})," clause of the ",(0,i.jsx)(t.code,{children:"select * from snapshot where ..."})," query."]}),(0,i.jsxs)("p",{children:["Unlike the ",(0,i.jsx)(t.code,{children:"filter"})," option, which applies to all records retrieved from the table, this filter only applies to rows in the initial snapshot of the table. For instance, it can be used to specify the range of event times to include in the snapshot, e.g.: ",(0,i.jsx)(t.code,{children:"ts BETWEEN TIMESTAMP '2005-01-01 00:00:00' AND TIMESTAMP '2010-12-31 23:59:59'"}),". This option can be used together with the ",(0,i.jsx)(t.code,{children:"filter"})," option. During the initial snapshot, only rows that satisfy both ",(0,i.jsx)(t.code,{children:"filter"})," and ",(0,i.jsx)(t.code,{children:"snapshot_filter"})," are retrieved from the Delta table. When subsequently following changes in the the transaction log (",(0,i.jsx)(t.code,{children:"mode = snapshot_and_follow"}),"), all rows that meet the ",(0,i.jsx)(t.code,{children:"filter"})," condition are ingested, regardless of ",(0,i.jsx)(t.code,{children:"snapshot_filter"}),". "]})]})]}),(0,i.jsxs)(t.tr,{children:[(0,i.jsxs)(t.td,{children:[(0,i.jsx)(t.code,{children:"version"}),", ",(0,i.jsx)(t.code,{children:"start_version"})]}),(0,i.jsx)(t.td,{children:"integer"}),(0,i.jsx)(t.td,{}),(0,i.jsxs)(t.td,{children:[(0,i.jsxs)("p",{children:["Optional table version.  When this option is set, the connector finds and opens the specified version of the table. In ",(0,i.jsx)(t.code,{children:"snapshot"})," and ",(0,i.jsx)(t.code,{children:"snapshot_and_follow"})," modes, it retrieves the snapshot of this version of the table.  In ",(0,i.jsx)(t.code,{children:"follow"}),", ",(0,i.jsx)(t.code,{children:"snapshot_and_follow"}),", and ",(0,i.jsx)(t.code,{children:"cdc"})," modes, it follows transaction log records ",(0,i.jsx)(t.strong,{children:"after"})," this version."]}),(0,i.jsxs)("p",{children:["Note: at most one of ",(0,i.jsx)(t.code,{children:"version"})," and ",(0,i.jsx)(t.code,{children:"datetime"})," options can be specified.  When neither of the two options is specified, the latest committed version of the table is used."]})]})]}),(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:(0,i.jsx)(t.code,{children:"datetime"})}),(0,i.jsx)(t.td,{children:"string"}),(0,i.jsx)(t.td,{}),(0,i.jsxs)(t.td,{children:[(0,i.jsxs)("p",{children:['Optional timestamp for the snapshot in the ISO-8601/RFC-3339 format, e.g., "2024-12-09T16:09:53+00:00". When this option is set, the connector finds and opens the version of the table as of the specified point in time (based on the server time recorded in the transaction log, not the event time encoded in the data).  In ',(0,i.jsx)(t.code,{children:"snapshot"})," and ",(0,i.jsx)(t.code,{children:"snapshot_and_follow"})," modes, it retrieves the snapshot of this version of the table.  In ",(0,i.jsx)(t.code,{children:"follow"}),", ",(0,i.jsx)(t.code,{children:"snapshot_and_follow"}),", and ",(0,i.jsx)(t.code,{children:"cdc"})," modes, it follows transaction log records ",(0,i.jsx)(t.strong,{children:"after"})," this version."]}),(0,i.jsxs)("p",{children:[" Note: at most one of ",(0,i.jsx)(t.code,{children:"version"})," and ",(0,i.jsx)(t.code,{children:"datetime"})," options can be specified.  When neither of the two options is specified, the latest committed version of the table is used."]})]})]}),(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:(0,i.jsx)(t.code,{children:"end_version"})}),(0,i.jsx)(t.td,{children:"integer"}),(0,i.jsx)(t.td,{}),(0,i.jsxs)(t.td,{children:[(0,i.jsx)("p",{children:"Optional final table version."}),(0,i.jsxs)("p",{children:["Valid only when the connector is configured in ",(0,i.jsx)(t.code,{children:"follow"}),", ",(0,i.jsx)(t.code,{children:"snapshot_and_follow"}),", or ",(0,i.jsx)(t.code,{children:"cdc"})," mode."]}),(0,i.jsx)("p",{children:"When set, the connector will stop scanning the table\u2019s transaction log after reaching this version or any greater version."}),(0,i.jsx)("p",{children:"This bound is inclusive: if the specified version appears in the log, it will be processed before signaling end-of-input."})]})]}),(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:(0,i.jsx)(t.code,{children:"cdc_delete_filer"})}),(0,i.jsx)(t.td,{children:"string"}),(0,i.jsx)(t.td,{}),(0,i.jsxs)(t.td,{children:[(0,i.jsx)("p",{children:"A predicate that determines whether the record represents a deletion."}),(0,i.jsxs)("p",{children:["This setting is only valid in the ",(0,i.jsx)(t.code,{children:"cdc"})," mode. It specifies a predicate applied to each row in the Delta table to determine whether the row represents a deletion event. Its value must be a valid Boolean SQL expression that can be used in a query of the form ",(0,i.jsx)(t.code,{children:"SELECT * from <table> WHERE <cdc_delete_filter>"}),"."]})]})]}),(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:(0,i.jsx)(t.code,{children:"cdc_order_by"})}),(0,i.jsx)(t.td,{children:"string"}),(0,i.jsx)(t.td,{}),(0,i.jsxs)(t.td,{children:[(0,i.jsx)("p",{children:"An expression that determines the ordering of updates in the Delta table."}),(0,i.jsxs)("p",{children:["This setting is only valid in the ",(0,i.jsx)(t.code,{children:"cdc"})," mode. It specifies a predicate applied to each row in the Delta table to determine the order in which updates in the table should be applied. Its value must be a valid SQL expression that can be used in a query of the form ",(0,i.jsx)(t.code,{children:"SELECT * from <table> ORDER BY <cdc_order_by>"}),"."]})]})]}),(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:(0,i.jsx)(t.code,{children:"num_parsers"})}),(0,i.jsx)(t.td,{children:"string"}),(0,i.jsx)(t.td,{}),(0,i.jsx)(t.td,{children:"The number of parallel parsing tasks the connector uses to process data read from the table. Increasing this value can enhance performance by allowing more concurrent processing. Recommended range: 1\u201310. The default is 4."})]}),(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:(0,i.jsx)(t.code,{children:"skip_unused_columns"})}),(0,i.jsx)(t.td,{children:"bool"}),(0,i.jsx)(t.td,{children:"false"}),(0,i.jsxs)(t.td,{children:[(0,i.jsxs)("p",{children:["Don't read unused columns from the Delta table.  When set to ",(0,i.jsx)(t.code,{children:"true"}),", this option instructs the connector to avoid reading columns from the Delta table that are not used in any view definitions. To be skipped, the columns must be either nullable or have default values. This can improve ingestion performance, especially for wide tables."]}),(0,i.jsx)("p",{children:"Note: The simplest way to exclude unused columns is to omit them from the Feldera SQL table declaration. The connector never reads columns that aren't declared in the SQL schema. Additionally, the SQL compiler emits warnings for declared but unused columns\u2014use these as a guide to optimize your schema."})]})]}),(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:(0,i.jsx)(t.code,{children:"max_concurrent_readers"})}),(0,i.jsx)(t.td,{children:"integer"}),(0,i.jsx)(t.td,{children:"6"}),(0,i.jsxs)(t.td,{children:[(0,i.jsx)("p",{children:"Maximum number of concurrent object store reads performed by all Delta Lake connectors."}),(0,i.jsx)("p",{children:"This setting is used to limit the number of concurrent reads of the object store in a pipeline with a large number of Delta Lake connectors. When multiple connectors are simultaneously reading from the object store, this can lead to transport timeouts."}),(0,i.jsx)("p",{children:"When enabled, this setting limits the number of concurrent reads across all connectors. This is a global setting that affects all Delta Lake connectors, and not just the connector where it is specified. It should therefore be used at most once in a pipeline.  If multiple connectors specify this setting, they must all use the same value."}),(0,i.jsx)("p",{children:"The default value is 6."})]})]})]})]}),"\n",(0,i.jsx)(t.p,{children:"[*]: Required fields"}),"\n",(0,i.jsx)(t.h3,{id:"storage-parameters",children:"Storage parameters"}),"\n",(0,i.jsx)(t.p,{children:"Along with the parameters listed above, there are additional configuration options for\nspecific storage backends.  Refer to backend-specific documentation for details:"}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsx)(t.li,{children:(0,i.jsx)(t.a,{href:"https://docs.rs/object_store/latest/object_store/aws/enum.AmazonS3ConfigKey.html",children:"Amazon S3 options"})}),"\n",(0,i.jsx)(t.li,{children:(0,i.jsx)(t.a,{href:"https://docs.rs/object_store/latest/object_store/azure/enum.AzureConfigKey.html",children:"Azure Blob Storage options"})}),"\n",(0,i.jsx)(t.li,{children:(0,i.jsx)(t.a,{href:"https://docs.rs/object_store/latest/object_store/gcp/enum.GoogleConfigKey.html",children:"Google Cloud Storage options"})}),"\n"]}),"\n",(0,i.jsx)(t.h2,{id:"data-type-mapping",children:"Data type mapping"}),"\n",(0,i.jsx)(t.p,{children:"The following table lists supported Delta Lake data types and corresponding Feldera types."}),"\n",(0,i.jsxs)(t.table,{children:[(0,i.jsx)(t.thead,{children:(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.th,{children:"Delta Lake type"}),(0,i.jsx)(t.th,{children:"Feldera SQL type"}),(0,i.jsx)(t.th,{children:"Comment"})]})}),(0,i.jsxs)(t.tbody,{children:[(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:(0,i.jsx)(t.code,{children:"BIGINT"})}),(0,i.jsx)(t.td,{children:(0,i.jsx)(t.code,{children:"BIGINT"})}),(0,i.jsx)(t.td,{})]}),(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:(0,i.jsx)(t.code,{children:"BINARY"})}),(0,i.jsx)(t.td,{children:(0,i.jsx)(t.code,{children:"VARBINARY"})}),(0,i.jsx)(t.td,{})]}),(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:(0,i.jsx)(t.code,{children:"BOOLEAN"})}),(0,i.jsxs)(t.td,{children:[(0,i.jsx)(t.code,{children:"BOOLEAN"}),"     \xa0"]}),(0,i.jsx)(t.td,{})]}),(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:(0,i.jsx)(t.code,{children:"DATE"})}),(0,i.jsx)(t.td,{children:(0,i.jsx)(t.code,{children:"DATE"})}),(0,i.jsx)(t.td,{})]}),(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:(0,i.jsx)(t.code,{children:"DOUBLE"})}),(0,i.jsx)(t.td,{children:(0,i.jsx)(t.code,{children:"DOUBLE"})}),(0,i.jsx)(t.td,{})]}),(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:(0,i.jsx)(t.code,{children:"FLOAT"})}),(0,i.jsx)(t.td,{children:(0,i.jsx)(t.code,{children:"REAL"})}),(0,i.jsx)(t.td,{})]}),(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:(0,i.jsx)(t.code,{children:"INT"})}),(0,i.jsx)(t.td,{children:(0,i.jsx)(t.code,{children:"INT"})}),(0,i.jsx)(t.td,{})]}),(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:(0,i.jsx)(t.code,{children:"SMALLINT"})}),(0,i.jsx)(t.td,{children:(0,i.jsx)(t.code,{children:"SMALLINT"})}),(0,i.jsx)(t.td,{})]}),(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:(0,i.jsx)(t.code,{children:"STRING"})}),(0,i.jsx)(t.td,{children:(0,i.jsx)(t.code,{children:"STRING"})}),(0,i.jsx)(t.td,{})]}),(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:(0,i.jsx)(t.code,{children:"DECIMAL(P,S)"})}),(0,i.jsx)(t.td,{children:(0,i.jsx)(t.code,{children:"DECIMAL(P,S)"})}),(0,i.jsxs)(t.td,{children:["The largest supported precision ",(0,i.jsx)(t.code,{children:"P"})," is 28."]})]}),(0,i.jsxs)(t.tr,{children:[(0,i.jsxs)(t.td,{children:[(0,i.jsx)(t.code,{children:"TIMESTAMP"}),", ",(0,i.jsx)(t.code,{children:"TIMESTAMP_NTZ"})]}),(0,i.jsx)(t.td,{children:(0,i.jsx)(t.code,{children:"TIMESTAMP"})}),(0,i.jsxs)(t.td,{children:["Timestamp values are rounded to the nearest millisecond.  Feldera currently does not support timestamps with time zones.  When using the ",(0,i.jsx)(t.code,{children:"TIMESTAMP"})," DeltaLake type, time zone information gets discarded."]})]}),(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:(0,i.jsx)(t.code,{children:"TINYINT"})}),(0,i.jsx)(t.td,{children:(0,i.jsx)(t.code,{children:"TINYINT"})}),(0,i.jsx)(t.td,{})]}),(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:(0,i.jsx)(t.code,{children:"MAP<K,V>"})}),(0,i.jsx)(t.td,{children:(0,i.jsx)(t.code,{children:"MAP<K,V>"})}),(0,i.jsx)(t.td,{})]}),(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:(0,i.jsx)(t.code,{children:"ARRAY<T>"})}),(0,i.jsx)(t.td,{children:(0,i.jsx)(t.code,{children:"T ARRAY"})}),(0,i.jsx)(t.td,{})]}),(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:(0,i.jsx)(t.code,{children:"STRUCT"})}),(0,i.jsxs)(t.td,{children:[(0,i.jsx)(t.code,{children:"ROW"})," or ",(0,i.jsx)(t.a,{href:"/sql/types#user-defined-types",children:"user-defined type"})]}),(0,i.jsxs)(t.td,{children:["structs can be encoded as either anonymous ",(0,i.jsx)(t.code,{children:"ROW"})," types or as named user-defined structs"]})]}),(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:(0,i.jsx)(t.code,{children:"VARIANT"})}),(0,i.jsx)(t.td,{children:(0,i.jsx)(t.code,{children:"VARIANT"})}),(0,i.jsx)(t.td,{})]})]})]}),"\n",(0,i.jsx)(t.h2,{id:"ingesting-time-series-data-from-a-delta-lake",children:"Ingesting time series data from a Delta Lake"}),"\n",(0,i.jsxs)(t.p,{children:["Feldera is optimized to efficiently process time series data by taking advantage\nof the fact that such data often arrives ordered by timestamp, i.e., every event\nhas the same or larger timestamp than the previous event. In some cases, events\ncan get reordered and delayed, but this delay is bounded, e.g., it may not\nexceed 1 hour. We refer to this bound as ",(0,i.jsx)(t.strong,{children:"lateness"})," and specify it by\nattaching the ",(0,i.jsx)(t.a,{href:"/sql/streaming#lateness-expressions",children:(0,i.jsx)(t.code,{children:"LATENESS"})})," attribute to the\ntimestamp column of the table declaration.  See our ",(0,i.jsx)(t.a,{href:"/tutorials/time-series",children:"Time Series Analysis\nGuide"})," for more details."]}),"\n",(0,i.jsxs)(t.p,{children:["When reading from a Delta Table that contains time series data, the user must\nensure that the initial snapshot of the table is ingested respecting the\n",(0,i.jsx)(t.code,{children:"LATENESS"})," annotation, e.g., if the table contains one year worth of data, and\nits lateness is equal to 1 month, then the connector must ingest all data for the\nfirst month before moving to the second month, and so on.  If this requirement\nis violated, the pipeline will drop records that arrive more that ",(0,i.jsx)(t.code,{children:"LATENESS"})," out\nof order."]}),"\n",(0,i.jsxs)(t.p,{children:["This can be achieved using the ",(0,i.jsx)(t.code,{children:"timestamp_column"})," property, which specifies the table column\nthat serves as an event timestamp. When this property is set, and ",(0,i.jsx)(t.code,{children:"mode"})," is\none of ",(0,i.jsx)(t.code,{children:"snapshot"})," or ",(0,i.jsx)(t.code,{children:"snapshot_and_follow"}),", table rows are ingested in the timestamp\norder, respecting the ",(0,i.jsx)(t.code,{children:"LATENESS"})," annotation on the column: each ingested row has a\ntimestamp no more than ",(0,i.jsx)(t.code,{children:"LATENESS"})," time units earlier than the most recent timestamp\nof any previously ingested row.  The ingestion is performed by partitioning the table\ninto timestamp ranges of width ",(0,i.jsx)(t.code,{children:"LATENESS"})," and ingesting ranges one by one in increasing timestamp order."]}),"\n",(0,i.jsx)(t.p,{children:"Requirements:"}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsxs)(t.li,{children:["The timestamp column must be of a supported type: integer, ",(0,i.jsx)(t.code,{children:"DATE"}),", or ",(0,i.jsx)(t.code,{children:"TIMESTAMP"}),"."]}),"\n",(0,i.jsxs)(t.li,{children:["The timestamp column must be declared with non-zero ",(0,i.jsx)(t.code,{children:"LATENESS"}),"."]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.code,{children:"LATENESS"})," must be a valid constant expression in the ",(0,i.jsx)(t.a,{href:"https://datafusion.apache.org/",children:"DataFusion\nSQL dialect"}),". The reason for this is that Feldera\nuses the Apache Datafusion engine to query Delta Lake.  In practice, most\nvalid Feldera SQL expressions are accepted by DataFusion."]}),"\n",(0,i.jsx)(t.li,{children:"For efficient ingest, the Delta table must be optimized for timestamp-based queries\nusing partitioning, Z-ordering, or liquid clustering."}),"\n"]}),"\n",(0,i.jsxs)(t.p,{children:["Note that the ",(0,i.jsx)(t.code,{children:"timestamp_column"})," property only controls the initial table snapshot.\nWhen ",(0,i.jsx)(t.code,{children:"mode"})," is set to ",(0,i.jsx)(t.code,{children:"follow"})," or ",(0,i.jsx)(t.code,{children:"snapshot_and_follow"})," and the connector is following\nthe transaction log of the table, it ingests changes in the order they appear in the\nlog.  It is the responsibility of the application that writes to the table\nto ensure that changes it applies to the table respect the ",(0,i.jsx)(t.code,{children:"LATENESS"})," annotations."]}),"\n",(0,i.jsx)(t.h3,{id:"example",children:"Example"}),"\n",(0,i.jsxs)(t.p,{children:["The following table contains a timestamp column of type ",(0,i.jsx)(t.code,{children:"TIMESTAMP"})," with ",(0,i.jsx)(t.code,{children:"LATENESS"})," equal\nto ",(0,i.jsx)(t.code,{children:"INTERVAL 30 days"}),". Assuming that the oldest timestamp in the table is ",(0,i.jsx)(t.code,{children:"2024-01-01T00:00:00"}),",\nthe connector will fetch all records with timestamps from ",(0,i.jsx)(t.code,{children:"2024-01-01"}),", then all records for\n",(0,i.jsx)(t.code,{children:"2024-01-02"}),", ",(0,i.jsx)(t.code,{children:"2024-01-03"}),", etc., until all records in the table have been ingested."]}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-sql",children:'CREATE TABLE transaction(\n    trans_date_trans_time TIMESTAMP NOT NULL LATENESS INTERVAL 1 day,\n    cc_num BIGINT,\n    merchant STRING,\n    category STRING,\n    amt DECIMAL(38, 2),\n    trans_num STRING,\n    unix_time BIGINT,\n    merch_lat DOUBLE,\n    merch_long DOUBLE,\n    is_fraud BIGINT\n) WITH (\n  \'connectors\' = \'[{\n    "transport": {\n      "name": "delta_table_input",\n      "config": {\n        "uri": "s3://feldera-fraud-detection-data/transaction_train",\n        "mode": "snapshot",\n        "aws_skip_signature": "true",\n        "timestamp_column": "trans_date_trans_time"\n      }\n    }\n  }\n]\');\n'})}),"\n",(0,i.jsx)(t.h2,{id:"additional-examples",children:"Additional examples"}),"\n",(0,i.jsxs)(t.p,{children:["Create a Delta Lake input connector to read a snapshot of a table from a public S3 bucket, using\n",(0,i.jsx)(t.code,{children:"unix_time"})," as the timestamp column.  The column stores event time in seconds since UNIX epoch\nand has lateness equal to 30 days (3600 seconds/hour * 24 hours/day * 30 days).\nNote the ",(0,i.jsx)(t.code,{children:"aws_skip_signature"})," flag, required to read from the bucket without authentication,"]}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-sql",children:'CREATE TABLE transaction(\n    trans_date_trans_time TIMESTAMP,\n    cc_num BIGINT,\n    merchant STRING,\n    category STRING,\n    amt DECIMAL(38, 2),\n    trans_num STRING,\n    unix_time BIGINT LATENESS 3600 * 24 * 30,\n    merch_lat DOUBLE,\n    merch_long DOUBLE,\n    is_fraud BIGINT\n) WITH (\n  \'connectors\' = \'[{\n    "transport": {\n      "name": "delta_table_input",\n      "config": {\n        "uri": "s3://feldera-fraud-detection-data/transaction_train",\n        "mode": "snapshot",\n        "aws_skip_signature": "true",\n        "timestamp_column": "unix_time"\n      }\n    }\n  }\n]\');\n'})}),"\n",(0,i.jsxs)(t.p,{children:["Read a full snapshot of version 10 of the table before ingesting the stream of\nchanges for versions 11 onward.  The initial snapshot will be sorted by the\n",(0,i.jsx)(t.code,{children:"unix_time"})," column.  Here and below we only show the contents of the\n",(0,i.jsx)(t.code,{children:"transport.config"})," field of the connector."]}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-json",children:'{\n  "uri": "s3://feldera-fraud-detection-data/transaction_infer",\n  "mode": "snapshot_and_follow",\n  "version": 10,\n  "timestamp_column": "unix_time",\n  "aws_skip_signature": "true"\n}\n'})}),"\n",(0,i.jsxs)(t.p,{children:["Read a full snapshot of a Delta table using the specified AWS access key. Note that\nthe ",(0,i.jsx)(t.code,{children:"aws_region"})," parameter is required in this case, because the Delta Lake Rust\nlibrary we use does not currently auto-detect the AWS region."]}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-json",children:'{\n  "uri": "s3://feldera-fraud-detection-demo/transaction_train",\n  "mode": "snapshot",\n  "aws_access_key_id": <AWS_ACCESS_KEY_ID>,\n  "aws_secret_access_key": <AWS_SECRET_ACCESS_KEY>,\n  "aws_region": "us-east-1"\n}\n'})})]})}function h(e={}){const{wrapper:t}={...(0,r.R)(),...e.components};return t?(0,i.jsx)(t,{...e,children:(0,i.jsx)(l,{...e})}):l(e)}}}]);