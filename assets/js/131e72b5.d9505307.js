"use strict";(self.webpackChunkfeldera_docs=self.webpackChunkfeldera_docs||[]).push([[9413],{14280:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>o,contentTitle:()=>d,default:()=>l,frontMatter:()=>c,metadata:()=>t,toc:()=>a});const t=JSON.parse('{"id":"formats/avro","title":"Avro Format","description":"This page describes configuration options specific to the Avro data format.","source":"@site/docs/formats/avro.md","sourceDirName":"formats","slug":"/formats/avro","permalink":"/formats/avro","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"docsSidebar","previous":{"title":"JSON Format","permalink":"/formats/json"},"next":{"title":"Parquet Format","permalink":"/formats/parquet"}}');var r=s(74848),i=s(28453);const c={},d="Avro Format",o={},a=[{value:"Avro input",id:"avro-input",level:2},{value:"Schema management and schema evolution",id:"schema-management-and-schema-evolution",level:3},{value:"Schema compatibility",id:"schema-compatibility",level:3},{value:"Configuration",id:"configuration",level:3},{value:"Examples",id:"examples",level:3},{value:"<a></a>Accessing Avro metadata",id:"accessing-avro-metadata",level:3},{value:"Avro output",id:"avro-output",level:2},{value:"Schema management",id:"schema-management",level:3},{value:"Views with unique keys",id:"views-with-unique-keys",level:3},{value:"Configuration",id:"configuration-1",level:3},{value:"Examples",id:"examples-1",level:3},{value:"Emitting change data capture (CDC) metadata",id:"emitting-change-data-capture-cdc-metadata",level:4}];function h(e){const n={a:"a",admonition:"admonition",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,i.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"avro-format",children:"Avro Format"})}),"\n",(0,r.jsx)(n.admonition,{type:"note",children:(0,r.jsxs)(n.p,{children:["This page describes configuration options specific to the Avro data format.\nSee ",(0,r.jsx)(n.a,{href:"/connectors/",children:"top-level connector documentation"})," for general information\nabout configuring input and output connectors."]})}),"\n",(0,r.jsxs)(n.p,{children:["Feldera supports sending and receiving data in the Avro format. We currently only support\nthe Avro format in conjunction with Kafka ",(0,r.jsx)(n.a,{href:"/connectors/sources/kafka",children:"source"})," and\n",(0,r.jsx)(n.a,{href:"/connectors/sinks/kafka",children:"sink"})," transport connectors.\nAvro is a strongly-typed format that requires a shared ",(0,r.jsx)(n.strong,{children:"schema"})," between the sender and receiver for successful\ndata encoding and decoding."]}),"\n",(0,r.jsxs)(n.p,{children:["Feldera supports the streaming variant of the Avro format, where message schemas are managed\nout-of-band by a ",(0,r.jsx)(n.strong,{children:"schema registry"})," service.  Instead of carrying the entire schema, Kafka messages\ninclude only schema identifiers:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Sender"}),": Before sending the first message with a new schema, the sender registers the schema\nin the schema registry."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Receiver"}),": Upon receiving a message, the receiver retrieves the associated schema from the\nregistry based on the schema identifier before decoding the message."]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"We support several Avro-based formats:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Raw"})," - every message contains a single Avro-encoded record that represents a row in a SQL\ntable or view."]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Raw input"}),": An input connector configured with the raw Avro format treats all\nincoming messages as inserts."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Raw output"}),": An output connector configured with the raw Avro format includes an operation type\n(",(0,r.jsx)(n.code,{children:'"op"'}),") as a header in each output Kafka message: ",(0,r.jsx)(n.code,{children:'"op": "insert"'})," represents an insertion, ",(0,r.jsx)(n.code,{children:'"op": "update"'}),"\nrepresents an update, and ",(0,r.jsx)(n.code,{children:'"op": "delete"'})," representa a deletion. The message key can optionally store\nthe primary key (see the ",(0,r.jsx)(n.code,{children:"key_mode"})," property)."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Debezium"})," (input only) - used to synchronize a Feldera table with an external database using\n",(0,r.jsx)(n.a,{href:"https://debezium.io/",children:"Debezium"}),".  See ",(0,r.jsx)(n.a,{href:"/connectors/sources/debezium",children:"Debezium source connector documentation"}),"\nfor more details."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Confluent JDBC"})," (output only) - used to send incremental changes computed by Feldera\nto an external database using the ",(0,r.jsx)(n.a,{href:"https://docs.confluent.io/kafka-connectors/jdbc/current/sink-connector/",children:"Confluent JDBC connector"}),".\nSee ",(0,r.jsx)(n.a,{href:"/connectors/sinks/confluent-jdbc",children:"Confluent JDBC sink documentation"})," for details."]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"avro-input",children:"Avro input"}),"\n",(0,r.jsx)(n.h3,{id:"schema-management-and-schema-evolution",children:"Schema management and schema evolution"}),"\n",(0,r.jsx)(n.p,{children:"To decode an Avro message, Feldera must obtain the Avro schema that was used to produce it.\nTypically, this schema is retrieved from the schema registry. Alternatively, users can manually\nprovide the schema as a JSON string as part of connector configuration."}),"\n",(0,r.jsxs)(n.p,{children:["When utilizing a schema registry, each message contains an embedded schema ID, which is used to\nlook up the corresponding schema in the registry. Usually, all messages in the input stream have\nthe same schema ID. However, the data producer can modify the message structure and its corresponding\nschema ID mid-stream. For instance, Debezium updates the schema whenever there is a change in the\nconnected database schema. This process is known as ",(0,r.jsx)(n.strong,{children:"schema evolution"}),"."]}),"\n",(0,r.jsx)(n.p,{children:"Feldera supports schema evolution, provided that all schemas in the input stream are compatible\nwith the SQL table declaration to which the stream is connected, as described below."}),"\n",(0,r.jsx)(n.h3,{id:"schema-compatibility",children:"Schema compatibility"}),"\n",(0,r.jsx)(n.p,{children:"The Avro schema consists of metadata specific to the format (e.g., raw or Debezium) and\na table record schema.  The record schema must match the schema of the SQL table that the\nconnector is attached to:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:["The Avro schema must be of type ",(0,r.jsx)(n.code,{children:"record"}),"."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:["For every non-nullable column in the table, a field with the same name and a compatible type must be present in the Avro schema\nNote that the Avro schema is allowed to contain fields that don't exist in the SQL table.  Such fields are ignored by the parser.\nConversely, the SQL table can contain ",(0,r.jsx)(n.strong,{children:"nullable"})," columns that are not present in the schema.  Such columns will\nbe set to ",(0,r.jsx)(n.code,{children:"NULL"})," during deserialization."]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"A SQL column and a field in the Avro schema are compatible if the following conditions are satisfied:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"If the Avro field is nullable, the SQL column is also nullable (however, a non-nullable\nfield can be deserialized into either nullable or non-nullable column)."}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"The SQL column type and Avro field type must match according to the following table:"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"SQL"}),(0,r.jsx)(n.th,{children:"Avro"}),(0,r.jsx)(n.th,{children:"Comment"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"BOOLEAN"})}),(0,r.jsxs)(n.td,{children:[(0,r.jsx)(n.code,{children:"boolean"}),"  \xa0"]}),(0,r.jsx)(n.td,{})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsxs)(n.td,{children:[(0,r.jsx)(n.code,{children:"TINYINT"}),", ",(0,r.jsx)(n.code,{children:"SMALLINT"}),", ",(0,r.jsx)(n.code,{children:"INT"})]}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"int"})}),(0,r.jsx)(n.td,{})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"BIGINT"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"long"})}),(0,r.jsx)(n.td,{})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"REAL"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"float"})}),(0,r.jsx)(n.td,{})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"DOUBLE"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"double"})}),(0,r.jsx)(n.td,{})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"DECIMAL(precision,scale)"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"decimal"})}),(0,r.jsx)(n.td,{children:"Precision and scale of the Avro decimal type must precisely match the SQL type."})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsxs)(n.td,{children:[(0,r.jsx)(n.code,{children:"CHAR"}),", ",(0,r.jsx)(n.code,{children:"VARCHAR"})]}),(0,r.jsxs)(n.td,{children:[(0,r.jsx)(n.code,{children:"string"}),", ",(0,r.jsx)(n.code,{children:"enum"})]}),(0,r.jsxs)(n.td,{children:["SQL string type can be deserialized from Avro strings, including strings whose logical type is set to ",(0,r.jsx)(n.code,{children:"uuid"}),". Avro enums are also deserialized into SQL strings."]})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"UUID"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"string"})}),(0,r.jsxs)(n.td,{children:["The Avro logical type can be ",(0,r.jsx)(n.em,{children:"optionally"})," set to ",(0,r.jsx)(n.code,{children:"uuid"}),"."]})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsxs)(n.td,{children:[(0,r.jsx)(n.code,{children:"BINARY"}),", ",(0,r.jsx)(n.code,{children:"VARBINARY"})]}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"bytes"})}),(0,r.jsx)(n.td,{})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"DATE"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"int"})}),(0,r.jsx)(n.td,{})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"TIME"})}),(0,r.jsxs)(n.td,{children:[(0,r.jsx)(n.code,{children:"long"})," or ",(0,r.jsx)(n.code,{children:"int"})]}),(0,r.jsxs)(n.td,{children:["logical type must be set to ",(0,r.jsx)(n.code,{children:"time-millis"})," or ",(0,r.jsx)(n.code,{children:"time-micros"})]})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"TIMESTAMP"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"long"})}),(0,r.jsxs)(n.td,{children:["logical type must be set to ",(0,r.jsx)(n.code,{children:"timestamp-millis"})," or ",(0,r.jsx)(n.code,{children:"timestamp-micros"})]})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"ARRAY"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"array"})}),(0,r.jsx)(n.td,{children:"Avro and SQL array element schemas must match"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"MAP"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"map"})}),(0,r.jsxs)(n.td,{children:["SQL map keys must be of type ",(0,r.jsx)(n.code,{children:"CHAR"})," or ",(0,r.jsx)(n.code,{children:"VARCHAR"}),"; Avro and SQL value schemas must match"]})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"VARIANT"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"string"})}),(0,r.jsxs)(n.td,{children:["values of type ",(0,r.jsx)(n.code,{children:"VARIANT"})," are deserialized from JSON-encoded strings (see ",(0,r.jsxs)(n.a,{href:"/sql/json",children:[(0,r.jsx)(n.code,{children:"VARIANT"})," documetation"]}),")"]})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"user-defined types"}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"record"})}),(0,r.jsx)(n.td,{children:"Avro record schema must match SQL user-defined type definition according to the same schema compatibility rules as for SQL tables"})]})]})]}),"\n",(0,r.jsx)(n.h3,{id:"configuration",children:"Configuration"}),"\n",(0,r.jsxs)(n.p,{children:["The following properties can be used to configure the Avro parser. All of these properties are optional.  However,\neither ",(0,r.jsx)(n.code,{children:"registry_urls"})," or ",(0,r.jsx)(n.code,{children:"schema"})," properties must be specified."]}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Property"}),(0,r.jsx)(n.th,{children:"Type"}),(0,r.jsx)(n.th,{children:"Default"}),(0,r.jsx)(n.th,{children:"Description"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"update_format"})}),(0,r.jsxs)(n.td,{children:[(0,r.jsx)(n.code,{children:'"raw"'})," or ",(0,r.jsx)(n.code,{children:'"debezium"'})]}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:'"raw"'})}),(0,r.jsx)(n.td,{children:"Format used to encode data change events in this stream"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"schema"})}),(0,r.jsx)(n.td,{children:"string"}),(0,r.jsx)(n.td,{}),(0,r.jsxs)(n.td,{children:["Avro schema used to encode all records in this stream, specified as a JSON-encoded string. When this property is set, the connector uses the provided schema instead of retrieving the schema from the schema registry. This setting is mutually exclusive with ",(0,r.jsx)(n.code,{children:"registry_urls"}),"."]})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"skip_schema_id"})}),(0,r.jsx)(n.td,{children:"Boolean"}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"false"})}),(0,r.jsxs)(n.td,{children:[(0,r.jsx)(n.code,{children:"true"})," if serialized messages only contain raw data without the header carrying schema ID. See ",(0,r.jsx)(n.a,{href:"https://docs.confluent.io/platform/current/schema-registry/fundamentals/serdes-develop/index.html#wire-format",children:"Confluent documentation"})," for more details"]})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"registry_urls"})}),(0,r.jsx)(n.td,{children:"array of strings"}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"[]"})}),(0,r.jsx)(n.td,{children:"List of schema registry URLs. When non-empty, the connector retrieves Avro message schemas from the registry."})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"registry_proxy"})}),(0,r.jsx)(n.td,{children:"string"}),(0,r.jsx)(n.td,{}),(0,r.jsxs)(n.td,{children:["Proxy that will be used to access the schema registry. Requires ",(0,r.jsx)(n.code,{children:"registry_urls"})," to be set."]})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"registry_timeout_secs"})}),(0,r.jsx)(n.td,{children:"string"}),(0,r.jsx)(n.td,{}),(0,r.jsxs)(n.td,{children:["Timeout in seconds used to connect to the registry. Requires ",(0,r.jsx)(n.code,{children:"registry_urls"})," to be set."]})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"registry_username"})}),(0,r.jsx)(n.td,{children:"string"}),(0,r.jsx)(n.td,{}),(0,r.jsxs)(n.td,{children:["Username used to authenticate with the registry.Requires ",(0,r.jsx)(n.code,{children:"registry_urls"})," to be set. This option is mutually exclusive with token-based authentication (see ",(0,r.jsx)(n.code,{children:"registry_authorization_token"}),")."]})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"registry_password"})}),(0,r.jsx)(n.td,{children:"string"}),(0,r.jsx)(n.td,{}),(0,r.jsxs)(n.td,{children:["Password used to authenticate with the registry. Requires ",(0,r.jsx)(n.code,{children:"registry_urls"})," to be set."]})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"registry_authorization_token"})}),(0,r.jsx)(n.td,{children:"string"}),(0,r.jsx)(n.td,{}),(0,r.jsxs)(n.td,{children:["Token used to authenticate with the registry. Requires ",(0,r.jsx)(n.code,{children:"registry_urls"})," to be set. This option is mutually exclusive with password-based authentication (see ",(0,r.jsx)(n.code,{children:"registry_username"})," and ",(0,r.jsx)(n.code,{children:"registry_password"}),")."]})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"threads"})}),(0,r.jsx)(n.td,{children:"integer"}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"4"})}),(0,r.jsxs)(n.td,{children:["Number of parallel worker threads used to encode messages. ",(0,r.jsxs)(n.strong,{children:["Only supported with ",(0,r.jsx)(n.a,{href:"/connectors/unique_keys",children:"indexed outputs"}),"."]})]})]})]})]}),"\n",(0,r.jsx)(n.h3,{id:"examples",children:"Examples"}),"\n",(0,r.jsx)(n.p,{children:"Configure the Avro parser to receive raw Avro records without embedded schema ids using a static user-provided schema."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-sql",children:'CREATE TABLE my_table (\n    id INT NOT NULL PRIMARY KEY,\n    ts TIMESTAMP\n) with (\n  \'connectors\' = \'[{\n    "transport": {\n      "name": "kafka_input",\n      "config": {\n        "topic": "my_topic",\n        "start_from": "earliest",\n        "bootstrap.servers": "127.0.0.1:19092"\n      }\n    },\n    "format": {\n      "name": "avro",\n      "config": {\n        "schema": "{\\"type\\":\\"record\\",\\"name\\":\\"ExampleSchema\\",\\"fields\\":[{\\"name\\":\\"id\\",\\"type\\":\\"int\\"},{\\"name\\":\\"ts\\",\\"type\\":[\\"null\\",{\\"type\\":\\"long\\",\\"logicalType\\":\\"timestamp-micros\\"}]}]}",\n        "skip_schema_id": true,\n        "update_format": "raw"\n      }\n    }\n}]\');\n'})}),"\n",(0,r.jsxs)(n.p,{children:["Configure the Avro parser to ingest data change events from Debezium (refer to ",(0,r.jsx)(n.a,{href:"/connectors/sources/debezium",children:"Debezium connector documentation"})," for additional details on setting up the Debezium source connector)."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-sql",children:'CREATE TABLE my_table (\n    id INT NOT NULL PRIMARY KEY,\n    ts TIMESTAMP\n) with (\n  \'connectors\' = \'[{\n    "transport": {\n      "name": "kafka_input",\n      "config": {\n        "topic": "my_topic",\n        "start_from": "earliest",\n        "bootstrap.servers": "127.0.0.1:19092"\n      }\n    },\n    "format": {\n      "name": "avro",\n      "config": {\n        "registry_urls": ["http://127.0.0.1:18081"],\n        "update_format": "debezium"\n      }\n    }\n}]\');\n'})}),"\n",(0,r.jsxs)(n.h3,{id:"accessing-avro-metadata",children:[(0,r.jsx)("a",{name:"metadata"}),"Accessing Avro metadata"]}),"\n",(0,r.jsx)(n.p,{children:"The Avro connector exposes the following metadata with every ingested record:"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Metadata attribute"}),(0,r.jsx)(n.th,{children:"SQL type"}),(0,r.jsxs)(n.th,{children:[(0,r.jsx)(n.code,{children:"CONNECTOR_METADATA()"})," field"]})]})}),(0,r.jsx)(n.tbody,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Avro schema ID"}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"INT UNSIGNED"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"avro_schema_id"})})]})})]}),"\n",(0,r.jsxs)(n.p,{children:["This metadata can be stored in table columns using the ",(0,r.jsx)(n.a,{href:"/sql/grammar#connector_metadata",children:(0,r.jsx)(n.code,{children:"CONNECTOR_METADATA()"})}),"\nSQL function.  In the following example we extract the ",(0,r.jsx)(n.code,{children:"avro_schema_id"})," attribute along with\n",(0,r.jsx)(n.a,{href:"/connectors/sources/kafka#accessing-kafka-metadata",children:"Kafka message metadata"})," from the Kafka connector:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-sql",children:'create table my_table(\n    x int,\n    avro_schema_id INT UNSIGNED DEFAULT CAST(CONNECTOR_METADATA()[\'avro_schema_id\'] as INT UNSIGNED),\n    kafka_offset BIGINT DEFAULT CAST(CONNECTOR_METADATA()[\'kafka_offset\'] AS BIGINT),\n    kafka_partition INT DEFAULT CAST(CONNECTOR_METADATA()[\'kafka_partition\'] AS INT)\n) with (\n    \'materialized\' = \'true\',\n    \'connectors\' = \'[{\n      "transport": {\n          "name": "kafka_input",\n          "config": {\n              "topic": "meta_topic",\n              "start_from": "earliest",\n              "bootstrap.servers": "localhost:19092",\n              "include_offset": true,\n              "include_partition": true\n          }\n      },\n      "format": {\n        "name": "avro",\n        "config": {\n          "update_format": "raw",\n          "registry_urls": ["http://localhost:18081"]\n        }\n      }\n    }]\');\n'})}),"\n",(0,r.jsx)(n.h2,{id:"avro-output",children:"Avro output"}),"\n",(0,r.jsx)(n.h3,{id:"schema-management",children:"Schema management"}),"\n",(0,r.jsxs)(n.p,{children:["The Avro encoder generates an Avro schema, which the consumer can use to decode messages produced\nby the encoder. For message formats that include key and value components with different schemas,\ne.g., the Confluent JDBC connector format, or the raw format with ",(0,r.jsx)(n.code,{children:'"key_mode": "key_fields"'}),", the encoder\ngenerates both key and value schemas. If the connector configuration specifies a schema registry,\nthe encoder publishes both key and value schemas in the registry (see ",(0,r.jsx)(n.a,{href:"#configuration",children:"Configuration"})," below)."]}),"\n",(0,r.jsx)(n.p,{children:"The encoder supports an alternative workflow where users provide the schema as a JSON string\nas part of connector configuration and the encoder produces messages using this schema instead of\ngenerating it automatically. If the connector configuration specifies a schema registry, the encoder\npublishes the user-provided schema to the registry."}),"\n",(0,r.jsx)(n.h3,{id:"views-with-unique-keys",children:"Views with unique keys"}),"\n",(0,r.jsx)(n.p,{children:"The Avro encoder supports views with unique keys:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"It uses the key columns to construct the Kafka message key."}),"\n",(0,r.jsx)(n.li,{children:"It can combine insert and delete updates for the same key into a single atomic update."}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["Use the ",(0,r.jsx)(n.code,{children:"CREATE INDEX"})," statement to define a unique key for the view, and the ",(0,r.jsx)(n.code,{children:"index"})," property\nof the connector to associate the index with the connector."]}),"\n",(0,r.jsxs)(n.p,{children:["See the ",(0,r.jsx)(n.a,{href:"/connectors/unique_keys#views-with-unique-keys",children:"documentation"})," for details."]}),"\n",(0,r.jsx)(n.h3,{id:"configuration-1",children:"Configuration"}),"\n",(0,r.jsxs)(n.p,{children:["The following properties can be used to configure the Avro encoder. All of these properties are optional.\nHowever, exactly one of ",(0,r.jsx)(n.code,{children:"registry_urls"})," and ",(0,r.jsx)(n.code,{children:"schema"})," properties must be specified."]}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Property"}),(0,r.jsx)(n.th,{children:"Type"}),(0,r.jsx)(n.th,{children:"Default"}),(0,r.jsx)(n.th,{children:"Description"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"update_format"})}),(0,r.jsxs)(n.td,{children:[(0,r.jsx)(n.code,{children:'"raw"'})," or ",(0,r.jsx)(n.code,{children:'"confluent_jdbc"'})]}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:'"raw"'})}),(0,r.jsx)(n.td,{children:"Format used to encode data change events in this stream."})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"schema"})}),(0,r.jsx)(n.td,{children:"string"}),(0,r.jsx)(n.td,{}),(0,r.jsx)(n.td,{children:"Avro schema used to encode output records. When specified, the encoder will use this schema; otherwise it will automatically generate an Avro schema based on the SQL view definition. Specified as a string containing schema definition in JSON format. This schema must match precisely the SQL view definition, modulo nullability of columns."})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"key_mode"})}),(0,r.jsxs)(n.td,{children:[(0,r.jsx)(n.code,{children:'"none"'})," or ",(0,r.jsx)(n.code,{children:'"key_fields"'})]}),(0,r.jsxs)(n.td,{children:[(0,r.jsx)(n.code,{children:"key_fields"})," when the ",(0,r.jsx)(n.code,{children:"index"})," property of the connector is configured and ",(0,r.jsx)(n.code,{children:"none"})," otherwise."]}),(0,r.jsxs)(n.td,{children:[(0,r.jsxs)("p",{children:["Determines how the message key is generated when the Avro encoder is configured in the ",(0,r.jsx)(n.code,{children:"raw"})," mode."]}),(0,r.jsxs)("p",{children:["Set to ",(0,r.jsx)(n.code,{children:"none"})," to generate messages without a key."]}),(0,r.jsxs)("p",{children:["Set to ",(0,r.jsx)(n.code,{children:'"key_fields"'})," to use the unique key columns of the view as the message key. This setting is supported when the output connector is configured with the ",(0,r.jsx)(n.code,{children:"index"})," property. It utilizes the values of the index columns specified in the associated ",(0,r.jsx)(n.code,{children:"CREATE INDEX"})," statement as the Avro message key. A separate Avro schema will be created and registered in the schema registry for the key component of the message."]})]})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"namespace"})}),(0,r.jsx)(n.td,{children:"string"}),(0,r.jsx)(n.td,{}),(0,r.jsx)(n.td,{children:"Avro namespace for the generated Avro schemas."})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"subject_name_strategy"})}),(0,r.jsxs)(n.td,{children:[(0,r.jsx)(n.code,{children:'"topic_name"'}),", ",(0,r.jsx)(n.code,{children:'"record_name"'}),", or ",(0,r.jsx)(n.code,{children:'"topic_record_name"'})]}),(0,r.jsxs)(n.td,{children:[(0,r.jsx)(n.code,{children:"topic_name"})," for ",(0,r.jsx)(n.code,{children:"confluent_jdbc"})," update format or ",(0,r.jsx)(n.code,{children:"record_name"})," for ",(0,r.jsx)(n.code,{children:"raw"})," update format"]}),(0,r.jsxs)(n.td,{children:[(0,r.jsx)("p",{children:"Subject name strategy used to publish Avro schemas used by the connector in the schema registry."}),(0,r.jsxs)("p",{children:[(0,r.jsx)(n.code,{children:"topic_name"}),": the subject name is derived from the Kafka topic name. For update formats with both key and value components, use subject names ",(0,r.jsx)(n.code,{children:"{topic_name}-key"})," and ",(0,r.jsx)(n.code,{children:"{topic_name}-value"})," for key and value schemas respectively. For update formats without a key (e.g., ",(0,r.jsx)(n.code,{children:"raw"}),", with ",(0,r.jsx)(n.code,{children:"key_mode=none"}),"), publish value schema under the subject name ",(0,r.jsx)(n.code,{children:"{topic_name}"}),". Only applicable when using Kafka as a transport."]}),(0,r.jsxs)("p",{children:[(0,r.jsx)(n.code,{children:"record_name"}),": the name of the SQL relation name that the schema is derived from is used as the subject name: the SQL view name for the message value schema or the SQL index name for the message key schema."]}),(0,r.jsxs)("p",{children:[(0,r.jsx)(n.code,{children:"topic_record_name"}),": combines both the topic name and the record name to form the subject. For update formats with both key and value components, use subject names ",(0,r.jsx)(n.code,{children:"{topic_name}-{record_name}-key"})," and ",(0,r.jsx)(n.code,{children:"{topic_name}-{record_name}-value"})," for key and value schemas respectively. For update formats without a key, publish value schema under the subject name ",(0,r.jsx)(n.code,{children:"{topic_name}-{record_name}"}),". Only applicable when using Kafka as a transport."]})]})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"skip_schema_id"})}),(0,r.jsx)(n.td,{children:"Boolean"}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"false"})}),(0,r.jsxs)(n.td,{children:["Set to ",(0,r.jsx)(n.code,{children:"true"})," if serialized messages should only contain raw data without the header carrying schema ID. ",(0,r.jsx)(n.code,{children:"False"})," by default. See ",(0,r.jsx)(n.a,{href:"https://docs.confluent.io/platform/current/schema-registry/fundamentals/serdes-develop/index.html#wire-format",children:"https://docs.confluent.io/platform/current/schema-registry/fundamentals/serdes-develop/index.html#wire-format"})]})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"registry_urls"})}),(0,r.jsx)(n.td,{children:"array of strings"}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"[]"})}),(0,r.jsx)(n.td,{children:"List of schema registry URLs. When non-empty, the connector retrieves Avro message schemas from the registry."})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"registry_proxy"})}),(0,r.jsx)(n.td,{children:"string"}),(0,r.jsx)(n.td,{}),(0,r.jsxs)(n.td,{children:["Proxy that will be used to access the schema registry. Requires ",(0,r.jsx)(n.code,{children:"registry_urls"})," to be set."]})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"registry_timeout_secs"})}),(0,r.jsx)(n.td,{children:"string"}),(0,r.jsx)(n.td,{}),(0,r.jsxs)(n.td,{children:["Timeout in seconds used to connect to the registry. Requires ",(0,r.jsx)(n.code,{children:"registry_urls"})," to be set."]})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"registry_username"})}),(0,r.jsx)(n.td,{children:"string"}),(0,r.jsx)(n.td,{}),(0,r.jsxs)(n.td,{children:["Username used to authenticate with the registry.Requires ",(0,r.jsx)(n.code,{children:"registry_urls"})," to be set. This option is mutually exclusive with token-based authentication (see ",(0,r.jsx)(n.code,{children:"registry_authorization_token"}),")."]})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"registry_password"})}),(0,r.jsx)(n.td,{children:"string"}),(0,r.jsx)(n.td,{}),(0,r.jsxs)(n.td,{children:["Password used to authenticate with the registry. Requires ",(0,r.jsx)(n.code,{children:"registry_urls"})," to be set."]})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"registry_authorization_token"})}),(0,r.jsx)(n.td,{children:"string"}),(0,r.jsx)(n.td,{}),(0,r.jsxs)(n.td,{children:["Token used to authenticate with the registry. Requires ",(0,r.jsx)(n.code,{children:"registry_urls"})," to be set. This option is mutually exclusive with password-based authentication (see ",(0,r.jsx)(n.code,{children:"registry_username"})," and ",(0,r.jsx)(n.code,{children:"registry_password"}),")."]})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"cdc_field"})}),(0,r.jsx)(n.td,{children:"string"}),(0,r.jsx)(n.td,{}),(0,r.jsxs)(n.td,{children:[(0,r.jsx)("p",{children:"Optional name of the field used for Change Data Capture (CDC) annotations."})," ",(0,r.jsxs)("p",{children:["Use this setting with data sinks that expect operation type (insert, delete, or update) encoded as a column in the Avro record, such as the ",(0,r.jsx)(n.a,{href:"/connectors/sinks/iceberg",children:"Iceberg Sink Kafka Connector"}),"."]})," ",(0,r.jsxs)("p",{children:[" When set (e.g., ",(0,r.jsx)(n.code,{children:'"cdc_field": "op"'}),"), the specified field will be added to each record to indicate the type of change: ",(0,r.jsxs)("ul",{children:[(0,r.jsxs)("li",{children:[(0,r.jsx)(n.code,{children:'"I"'})," for insert operations"]})," ",(0,r.jsxs)("li",{children:[(0,r.jsx)(n.code,{children:'"U"'})," for upserts"]})," ",(0,r.jsxs)("li",{children:[(0,r.jsx)(n.code,{children:'"D"'})," for deletions"]})]})," "]})," ",(0,r.jsxs)("p",{children:["If not set, CDC metadata will not be included in the records. Only works with the ",(0,r.jsx)(n.code,{children:"raw"})," update format."]})]})]})]})]}),"\n",(0,r.jsx)(n.h3,{id:"examples-1",children:"Examples"}),"\n",(0,r.jsx)(n.p,{children:"Configure the Avro encoder to send raw Avro records using a static user-provided schema.  The configuration does not\nspecify a schema registry URL, so the encoder will not try to publish the schema."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-sql",children:'create table my_table (\n   id bigint,\n   name string\n);\n\ncreate view my_view\nwith (\n  \'connectors\' = \'[{\n    "transport": {\n      "name": "kafka_output",\n      "config": {\n        "bootstrap.servers": "127.0.0.1:19092",\n        "topic": "my_topic",\n        "auto.offset.reset": "earliest"\n      }\n    },\n    "format": {\n      "name": "avro",\n      "config": {\n        "update_format": "raw",\n        "schema": "{\\"type\\":\\"record\\",\\"name\\":\\"ExampleSchema\\",\\"fields\\":[{\\"name\\":\\"id\\",\\"type\\":\\"int\\"},{\\"name\\":\\"ts\\",\\"type\\":[\\"null\\",{\\"type\\":\\"long\\",\\"logicalType\\":\\"timestamp-micros\\"}]}]}"\n      }\n    }\n  }]\'\n)\nas select * from my_table;\n'})}),"\n",(0,r.jsxs)(n.p,{children:["Configure the Avro encoder to send raw Avro records. The connector is associated with a SQL index\n",(0,r.jsx)(n.code,{children:"my_index"})," using the ",(0,r.jsx)(n.code,{children:"index"})," property. This index specifies that the ",(0,r.jsx)(n.code,{children:"id"})," field serves\nas the unique key for the view.  The connector will use this key to combine delete and insert\noperations with the same unique key into atomic updates. The encoder will generate two separate\nschemas for the key and value components of the message and publish them in the schema registry."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-sql",children:'create table my_table (\n   id bigint,\n   name string\n);\n\ncreate view my_view\nwith (\n  \'connectors\' = \'[{\n    "index": "my_index",\n    "transport": {\n      "name": "kafka_output",\n      "config": {\n        "bootstrap.servers": "127.0.0.1:19092",\n        "topic": "my_topic"\n      }\n    },\n    "format": {\n      "name": "avro",\n      "config": {\n        "update_format": "raw",\n        "registry_urls": ["http://127.0.0.1:18081"]\n      }\n    }\n  }]\'\n)\nas select * from my_table;\n\ncreate index my_index on my_view(id);\n'})}),"\n",(0,r.jsxs)(n.p,{children:["Configure the Avro encoder to output changes in the format expected by the\n",(0,r.jsx)(n.a,{href:"https://docs.confluent.io/kafka-connectors/jdbc/current/sink-connector/",children:"Confluent JDBC Kakfa Connect connector"}),".\nThe connector is associated with a SQL index ",(0,r.jsx)(n.code,{children:"my_index"})," using the ",(0,r.jsx)(n.code,{children:"index"})," property. This index specifies that\nthe ",(0,r.jsx)(n.code,{children:"id"})," field serves as the unique key for the view.  The connector will use this key to combine delete and insert\noperations with the same unique key into atomic updates.  The encoder will generate two separate schemas for the\nkey and value components of the message and publish them in the schema registry."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-sql",children:'create table my_table (\n   id bigint,\n   name string\n);\n\ncreate view my_view\nwith (\n  \'connectors\' = \'[{\n    "index": "my_index",\n    "transport": {\n      "name": "kafka_output",\n      "config": {\n        "bootstrap.servers": "127.0.0.1:19092",\n        "topic": "my_topic"\n      }\n    },\n    "format": {\n      "name": "avro",\n      "config": {\n        "update_format": "confluent_jdbc",\n        "registry_urls": ["http://127.0.0.1:18081"]\n      }\n    }\n  }]\'\n)\nas select * from my_table;\n\ncreate index my_index on my_view(id);\n'})}),"\n",(0,r.jsx)(n.h4,{id:"emitting-change-data-capture-cdc-metadata",children:"Emitting change data capture (CDC) metadata"}),"\n",(0,r.jsxs)(n.p,{children:["Avro output encoder can be configured to include CDC metadata by setting\nthe parameter ",(0,r.jsx)(n.code,{children:"cdc_field"}),". In this example, we set the ",(0,r.jsx)(n.code,{children:"cdc_field"}),' to "op".\nEach output record will contain the field "op" which will have one of the\nfollowing values:']}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"I"}),": ",(0,r.jsx)(n.strong,{children:"Insert"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"D"}),": ",(0,r.jsx)(n.strong,{children:"Delete"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"U"}),": ",(0,r.jsx)(n.strong,{children:"Upsert"})]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["The ",(0,r.jsx)(n.a,{href:"/connectors/sinks/iceberg",children:"Iceberg Sink Kafka Connector"})," utilizes the CDC\nmetadata to maintain the materialized view in Iceberg."]}),"\n",(0,r.jsx)(n.p,{children:"Example:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-sql",children:'create materialized view pizzas with (\n   \'connectors\' = \'[\n    {\n      "index": "idx1",\n      "transport": {\n          "name": "kafka_output",\n          "config": {\n              "bootstrap.servers": "localhost:29092",\n              "topic": "pizzas"\n          }\n      },\n      "format": {\n          "name": "avro",\n          "config": {\n              "registry_urls": ["http://localhost:18081"],\n              "update_format": "raw",\n              "cdc_field": "op",\n              "subject_name_strategy": "topic_name"\n          }\n      }\n    }\n   ]\'\n) as select * from tbl order by order_number desc limit 10;\ncreate index idx1 on pizzas(order_number);\n'})})]})}function l(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(h,{...e})}):h(e)}},28453:(e,n,s)=>{s.d(n,{R:()=>c,x:()=>d});var t=s(96540);const r={},i=t.createContext(r);function c(e){const n=t.useContext(i);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function d(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:c(e.components),t.createElement(i.Provider,{value:n},e.children)}}}]);