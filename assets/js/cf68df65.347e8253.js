"use strict";(self.webpackChunkfeldera_docs=self.webpackChunkfeldera_docs||[]).push([[2803],{28453:(e,n,r)=>{r.d(n,{R:()=>i,x:()=>a});var o=r(96540);const t={},s=o.createContext(t);function i(e){const n=o.useContext(s);return o.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:i(e.components),o.createElement(s.Provider,{value:n},e.children)}},99877:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>h,frontMatter:()=>i,metadata:()=>o,toc:()=>d});const o=JSON.parse('{"id":"operations/memory","title":"Memory Usage","description":"Feldera pipelines primarily use memory for the following purposes:","source":"@site/docs/operations/memory.md","sourceDirName":"operations","slug":"/operations/memory","permalink":"/operations/memory","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"docsSidebar","previous":{"title":"Troubleshooting","permalink":"/operations/guide"},"next":{"title":"Pipeline Metrics","permalink":"/operations/metrics"}}');var t=r(74848),s=r(28453);const i={},a="Memory Usage",c={},d=[];function l(e){const n={a:"a",code:"code",h1:"h1",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"memory-usage",children:"Memory Usage"})}),"\n",(0,t.jsx)(n.p,{children:"Feldera pipelines primarily use memory for the following purposes:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Input"})," records ingested by connectors, but not yet processed by\nthe circuit."]}),"\n",(0,t.jsxs)(n.p,{children:["The maximum queue length is controlled by the per-connector\n",(0,t.jsx)(n.a,{href:"/connectors#max_queued_records",children:(0,t.jsx)(n.code,{children:"max_queued_records"})})," property.  This should be large enough to\nhide the latency of communication, but small enough to avoid wasting\nmemory.  The default of 1,000,000 limits the memory used by a\nconnector to 1 GB for input records that average 1 kB in size.  This\nis ordinarily a good compromise, but it can be too high if records\nare very large or if there are many input connectors.  In those\ncases, reduce the values."]}),"\n",(0,t.jsxs)(n.p,{children:["The web console shows the total number of records buffered across\nall connectors, which is also exposed as the\n",(0,t.jsx)(n.a,{href:"/operations/metrics#records_input_buffered",children:(0,t.jsx)(n.code,{children:"records_input_buffered"})})," metric.  The number of bytes buffered is\nexposed as ",(0,t.jsx)(n.a,{href:"/operations/metrics#records_input_buffered_bytes",children:(0,t.jsx)(n.code,{children:"records_input_buffered_bytes"})}),".  The number of records\nand bytes buffered by individual connectors are exposed as\n",(0,t.jsx)(n.a,{href:"/operations/metrics#input_connector_buffered_records",children:(0,t.jsx)(n.code,{children:"input_connector_buffered_records"})})," and\n",(0,t.jsx)(n.a,{href:"/operations/metrics#input_connector_buffered_records_bytes",children:(0,t.jsx)(n.code,{children:"input_connector_buffered_records_bytes"})}),", respectively."]}),"\n",(0,t.jsxs)(n.p,{children:["Some input connectors can use substantial additional memory, beyond\nthat needed to buffer records, for their internal operations.  This\nis particularly true of the ",(0,t.jsx)(n.a,{href:"/connectors/sources/kafka",children:"Kafka input connector"}),".  These\nconnectors report their additional memory use as\n",(0,t.jsx)(n.a,{href:"/operations/metrics#input_connector_extra_memory_bytes",children:(0,t.jsx)(n.code,{children:"input_connector_extra_memory_bytes"})}),"."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Output"})," records produced by the circuit, but not yet processed by\nconnectors.  Output records can be in memory or on storage."]}),"\n",(0,t.jsxs)(n.p,{children:["As for input connectors, ",(0,t.jsx)(n.a,{href:"/connectors#max_queued_records",children:(0,t.jsx)(n.code,{children:"max_queued_records"})})," limits the maximum\nnumber of records buffered.  This should be large enough to avoid\nstalling the pipeline.  The value applies to output records whether\nin memory or on storage.  The default is 1,000,000."]}),"\n",(0,t.jsxs)(n.p,{children:["The number of batches of buffered output records is exposed as\n",(0,t.jsx)(n.a,{href:"/operations/metrics#output_buffered_batches",children:(0,t.jsx)(n.code,{children:"output_buffered_batches"})}),", but since each batch contains a\nvariable number of records, this does not directly relate to memory\nuse.  The number of records buffered by individual connectors are\nexposed as ",(0,t.jsx)(n.a,{href:"/operations/metrics#output_connector_buffered_records",children:(0,t.jsx)(n.code,{children:"output_connector_buffered_records"})}),"."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Index batches in memory"}),".  The pipeline initially adds batches of\nrecords to its in-memory indexes and then merges them into larger\nbatches in the background.  When a merged batch is large enough,\nit is written to storage.  The minimum size to write a batch to\ndisk is configurable as ",(0,t.jsx)(n.code,{children:"min_storage_bytes"})," under ",(0,t.jsx)(n.code,{children:"storage"})," in the\npipeline ",(0,t.jsx)(n.a,{href:"/pipelines/configuration#runtime-configuration",children:"Runtime configuration"}),".  The default is 10 MiB, which is\nusually a good choice.  Configuring a smaller value may save memory\nbut at a performance cost."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Cache of index batches on storage"}),".  When a merge flushes an\nindex batch to storage, it can later be cached in memory.  A\npipeline's memory usage is mostly independent of the size of its\nstate, so that a pipeline can have a 1 TB storage footprint, but\nonly use a few GB or RAM."]}),"\n",(0,t.jsxs)(n.p,{children:["The maximum size of the cache can be configured with ",(0,t.jsx)(n.code,{children:"cache_mib"}),"\nunder ",(0,t.jsx)(n.code,{children:"storage"})," in the pipeline ",(0,t.jsx)(n.a,{href:"/pipelines/configuration#runtime-configuration",children:"Runtime configuration"}),".  The\ndefault is 512 MiB per worker, or 4 GiB for the default 8-worker\nconfiguration.  The default is usually a good choice unless a large\nnumber of workers would make it too large for the available memory."]}),"\n",(0,t.jsxs)(n.p,{children:["The current and maximum size of the cache are exposed as metrics\n",(0,t.jsx)(n.a,{href:"/operations/metrics#storage_cache_usage_bytes",children:(0,t.jsx)(n.code,{children:"storage_cache_usage_bytes"})})," and\n",(0,t.jsx)(n.a,{href:"/operations/metrics#storage_cache_usage_limit_bytes_total",children:(0,t.jsx)(n.code,{children:"storage_cache_usage_limit_bytes_total"})}),", respectively."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Bloom filters"})," for batches on storage.  By default, these use\napproximately 19 bits of memory per key on storage (about 2.2 MiB\nper million keys).  Bloom filters stay in memory, rather than being\npart of the cache, so they can become a large cost when many records\nare in storage."]}),"\n",(0,t.jsx)(n.p,{children:"The amount of memory used by Bloom filters is visible in circuit\nprofiles."}),"\n",(0,t.jsxs)(n.p,{children:["The number of bits per key can be tuned by setting\n",(0,t.jsx)(n.code,{children:"bloom_false_positive_rate"})," in ",(0,t.jsx)(n.code,{children:"dev_tweaks"})," in the pipeline ",(0,t.jsx)(n.a,{href:"/pipelines/configuration#runtime-configuration",children:"Runtime\nconfiguration"}),".  This can also be used to disable Bloom filters\nentirely.  Reducing the number of bits per key, or disabling Bloom\nfilters, can reduce performance."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"In-flight batches"}),".  As the pipeline processes a particular\ncollection of input batches, it passes batches of data from one\noperator to another.  Several data batches can be in flight at any\ngiven time.  The number and size of these batches depends on the\ndata, the number of records passed in by the input connectors, the\nSQL program, the query plan generated by Feldera's SQL compiler, and\nhow Feldera schedules execution of the query plan.  Most data\nbatches only live as long as it takes for them to be processed by an\noperator; they may be transformed into part of the operator's\noutput, or be added to an index (see below), or be passed to an\noutput connector, or simply be discarded."]}),"\n",(0,t.jsxs)(n.p,{children:["Since in-flight batches are transient, and because the pipeline\ninternally breaks large batches into smaller batches, they do not\nusually become a memory problem.  If they do, one may reduce\n",(0,t.jsx)(n.a,{href:"/connectors#max_batch_size",children:(0,t.jsx)(n.code,{children:"max_batch_size"})})," for input connectors, limiting the size of input\nbatches.  Another approach is to set ",(0,t.jsx)(n.code,{children:"min_step_storage_bytes"})," in\n",(0,t.jsx)(n.code,{children:"storage"})," in the pipeline ",(0,t.jsx)(n.a,{href:"/pipelines/configuration#runtime-configuration",children:"Runtime configuration"}),", to force\nin-flight batches to storage, although this is likely to reduce\nperformance."]}),"\n"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(l,{...e})}):l(e)}}}]);