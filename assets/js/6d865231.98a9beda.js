"use strict";(self.webpackChunkfeldera_docs=self.webpackChunkfeldera_docs||[]).push([[2442],{28453:(e,n,t)=>{t.d(n,{R:()=>s,x:()=>o});var a=t(96540);const r={},i=a.createContext(r);function s(e){const n=a.useContext(i);return a.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:s(e.components),a.createElement(i.Provider,{value:n},e.children)}},65876:(e,n,t)=>{t.d(n,{A:()=>a});const a=t.p+"assets/images/inference_animation-f458e3e7561314cef9e0bdf64086032c.gif"},70823:(e,n,t)=>{t.d(n,{A:()=>a});const a=t.p+"assets/images/training_animation-8bb0c4521b49c1fd577ac604306d70ac.gif"},75522:(e,n,t)=>{t.d(n,{A:()=>a});const a=t.p+"assets/images/tables-8f1f954e601d8499ee5c165d342a0242.png"},97758:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>d,contentTitle:()=>o,default:()=>u,frontMatter:()=>s,metadata:()=>a,toc:()=>c});const a=JSON.parse('{"id":"use_cases/fraud_detection/fraud_detection","title":"Use Case: Real-time Feature Engineering for Credit Card Fraud Detection","description":"In this article, we use Feldera to build a real-time credit card fraud detection system. This falls under the umbrella","source":"@site/docs/use_cases/fraud_detection/fraud_detection.md","sourceDirName":"use_cases/fraud_detection","slug":"/use_cases/fraud_detection/","permalink":"/use_cases/fraud_detection/","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"docsSidebar","previous":{"title":"Part 4. Sending Output to Multiple Destinations","permalink":"/use_cases/batch/part4"},"next":{"title":"Incremental, live web applications","permalink":"/use_cases/real_time_apps/part1"}}');var r=t(74848),i=t(28453);const s={},o="Use Case: Real-time Feature Engineering for Credit Card Fraud Detection",d={},c=[{value:"Credit card fraud detection",id:"credit-card-fraud-detection",level:2},{value:"Input data",id:"input-data",level:2},{value:"Model training and testing",id:"model-training-and-testing",level:2},{value:"Writing feature queries",id:"writing-feature-queries",level:3},{value:"Training and testing",id:"training-and-testing",level:3},{value:"Real-time inference",id:"real-time-inference",level:2},{value:"Takeaways",id:"takeaways",level:2},{value:"Resources",id:"resources",level:2}];function l(e){const n={a:"a",admonition:"admonition",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components},{Details:a}=n;return a||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"use-case-real-time-feature-engineering-for-credit-card-fraud-detection",children:"Use Case: Real-time Feature Engineering for Credit Card Fraud Detection"})}),"\n",(0,r.jsx)(n.p,{children:"In this article, we use Feldera to build a real-time credit card fraud detection system. This falls under the umbrella\nof real-time feature engineering, where we transform raw data (credit card and user activity) into features that better\nrepresent the underlying problem to an ML model. We\nwill go through the following steps of building the application:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Writing SQL queries that define several interesting features, based on data\nenrichment and rolling aggregates."}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Using these queries to compute feature vectors with Feldera and train an ML\nmodel on a historical data set stored in a Delta Lake."}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:["Using the same queries to compute feature vectors over a real-time stream of\ncredit card transaction data. ",(0,r.jsx)(n.strong,{children:"By simply connecting new data sources and\nsinks, the SQL queries used for training the model on batch inputs work\nseamlessly on streaming data for real-time inference."})]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["The entire use case is implemented as a Python script written using the Feldera\n",(0,r.jsx)(n.a,{href:"pathname:///python/",children:"Python SDK"}),". It is available from our ",(0,r.jsx)(n.a,{href:"https://github.com/feldera/feldera/blob/main/demo/project_demo10-FraudDetectionDeltaLake/run.py",children:"github\nrepository"}),"\nand can be run from the command line or from your favorite Python notebook\nenvironment."]}),"\n",(0,r.jsx)(n.h2,{id:"credit-card-fraud-detection",children:"Credit card fraud detection"}),"\n",(0,r.jsx)(n.p,{children:"Credit card fraud detection is a classic application of real-time feature\nengineering. Here, data comes in a stream of transactions, each with attributes\nlike card number, purchase time, vendor, and amount. Additionally, the fraud\ndetector has access to a slowly changing table with demographics information\nabout cardholders, such as age and address."}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{alt:"Input tables and the output view",src:t(75522).A+"",width:"2453",height:"1099"})}),"\n",(0,r.jsx)(n.h2,{id:"input-data",children:"Input data"}),"\n",(0,r.jsxs)(n.p,{children:["We used a publicly available ",(0,r.jsx)(n.a,{href:"https://github.com/namebrandon/Sparkov_Data_Generation",children:"Synthetic Credit Card Transaction\nGenerator"})," to generate\ntwo labeled datasets, both with 1000 user profiles. We will use the first\ndataset for model training and testing, and the second dataset -- for real-time\ninference. We stored the datasets in the ",(0,r.jsx)(n.a,{href:"https://delta.io/",children:"Delta Lake format"}),"\nin two public S3 buckets:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Training dataset:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Demographics table: ",(0,r.jsx)(n.code,{children:"s3://feldera-fraud-detection-data/demographics_train/"})]}),"\n",(0,r.jsxs)(n.li,{children:["Transaction table: ",(0,r.jsx)(n.code,{children:"s3://feldera-fraud-detection-data/transaction_train/"})]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Inference dataset:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Demographics table: ",(0,r.jsx)(n.code,{children:"s3://feldera-fraud-detection-data/demographics_train/"})]}),"\n",(0,r.jsxs)(n.li,{children:["Transaction table: ",(0,r.jsx)(n.code,{children:"s3://feldera-fraud-detection-data/transaction_train/"})]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"model-training-and-testing",children:"Model training and testing"}),"\n",(0,r.jsx)(n.p,{children:"Finding an optimal set of features to train a good ML model is an iterative\nprocess. At every step, the data scientist trains and tests a model using\ncurrently selected feature queries on an array of labeled historical data. The\nresults of each experiment drive the next refinement of feature queries. In\nthis scenario, feature vectors are computed in batch mode over a slice of\nhistorical data, e.g., data collected over a two-week timeframe."}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{alt:"Feature engineering as an iterative process",src:t(70823).A+"",width:"1160",height:"459"})}),"\n",(0,r.jsx)(n.p,{children:"Below we walk the reader through one iteration of this process: we define a\nset of features, train a model using these features, and test its accuracy."}),"\n",(0,r.jsx)(n.h3,{id:"writing-feature-queries",children:"Writing feature queries"}),"\n",(0,r.jsx)(n.p,{children:"We define several features over our input tables:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Data enrichment:","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"We add demographic attributes, such as zip code, to each transaction"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["Rolling aggregates:","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"average spending per transaction in the past day, week, and month"}),"\n",(0,r.jsx)(n.li,{children:"average spending per transaction over a 3-month timeframe on the same day of the week"}),"\n",(0,r.jsx)(n.li,{children:"number of transactions made with this credit card in the last 24 hours"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["Other:","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"is_weekend"})," - transaction took place on a weekend"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"is_night"})," - transaction took place before 6am"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"d"})," - day of week"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["The following Python function creates a SQL program,\nconsisting of two tables with raw input data (",(0,r.jsx)(n.code,{children:"TRANSACTION"})," and ",(0,r.jsx)(n.code,{children:"DEMOGRAPHICS"}),") and\nthe ",(0,r.jsx)(n.code,{children:"FEATURE"})," view, which computes the above features over these tables."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"def build_program(transactions_connectors: str, demographics_connectors: str, features_connectors: str) -> str:\n    return f\"\"\"-- Credit card transactions\n    CREATE TABLE TRANSACTION(\n        trans_date_trans_time TIMESTAMP,\n        cc_num BIGINT,\n        merchant STRING,\n        category STRING,\n        amt DECIMAL(38, 2),\n        trans_num STRING,\n        unix_time BIGINT,\n        merch_lat DOUBLE,\n        merch_long DOUBLE,\n        is_fraud BIGINT\n    ) WITH ('connectors' = '{transactions_connectors}');\n\n    -- Demographics data.\n    CREATE TABLE DEMOGRAPHICS(\n        cc_num BIGINT,\n        first STRING,\n        last STRING,\n        gender STRING,\n        street STRING,\n        city STRING,\n        state STRING,\n        zip BIGINT,\n        lat DOUBLE,\n        long DOUBLE,\n        city_pop BIGINT,\n        job STRING,\n        dob DATE\n    ) WITH ('connectors' = '{demographics_connectors}');\n\n    -- Feature query written in the Feldera SQL dialect.\n    CREATE VIEW FEATURE\n    WITH ('connectors' = '{features_connectors}')\n    AS\n        SELECT\n           t.cc_num,\n           dayofweek(trans_date_trans_time) as d,\n           CASE\n             WHEN dayofweek(trans_date_trans_time) IN(6, 7) THEN true\n             ELSE false\n           END AS is_weekend,\n           hour(trans_date_trans_time) as hour_of_day,\n           CASE\n             WHEN hour(trans_date_trans_time) <= 6 THEN true\n             ELSE false\n           END AS is_night,\n           -- Average spending per day, per week, and per month.\n           AVG(amt) OVER window_1_day AS avg_spend_pd,\n           AVG(amt) OVER window_7_day AS avg_spend_pw,\n           AVG(amt) OVER window_30_day AS avg_spend_pm,\n           -- Average spending over the last three months for the same day of the week.\n           COALESCE(\n            AVG(amt) OVER (\n              PARTITION BY t.cc_num, EXTRACT(DAY FROM trans_date_trans_time)\n              ORDER BY unix_time\n              RANGE BETWEEN 7776000 PRECEDING and CURRENT ROW\n            ), 0) AS avg_spend_p3m_over_d,\n           -- Number of transactions in the last 24 hours.\n           COUNT(*) OVER window_1_day AS trans_freq_24,\n           amt, unix_time, zip, city_pop, is_fraud\n        FROM transaction as t\n        JOIN demographics as d\n        ON t.cc_num = d.cc_num\n        WINDOW\n          window_1_day AS (PARTITION BY t.cc_num ORDER BY unix_time RANGE BETWEEN 86400 PRECEDING AND CURRENT ROW),\n          window_7_day AS (PARTITION BY t.cc_num ORDER BY unix_time RANGE BETWEEN 604800 PRECEDING AND CURRENT ROW),\n          window_30_day AS (PARTITION BY t.cc_num ORDER BY unix_time RANGE BETWEEN 2592000 PRECEDING AND CURRENT ROW);\n      \"\"\"\n"})}),"\n",(0,r.jsx)(n.h3,{id:"training-and-testing",children:"Training and testing"}),"\n",(0,r.jsxs)(n.p,{children:["We define helper functions for model training and evaluation using the\n",(0,r.jsx)(n.a,{href:"https://xgboost.readthedocs.io/",children:"XGBoost"})," framework."]}),"\n",(0,r.jsxs)(a,{children:[(0,r.jsx)("summary",{children:" Click to see full Python code "}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"from xgboost import XGBClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\n\n\n# Split input dataframe into train and test sets\ndef get_train_test_data(dataframe, feature_cols, target_col, train_test_split_ratio, random_seed):\n    X = dataframe[feature_cols]\n    y = dataframe[target_col]\n    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=train_test_split_ratio,\n                                                        random_state=random_seed)\n\n    return X_train, X_test, y_train, y_test\n\n\n# Train a decision tree classifier using xgboost.\n# Other ML frameworks and types of ML models can be readily used with Feldera.\ndef train_model(dataframe, config):\n    max_depth = 12\n    n_estimators = 100\n\n    X_train, X_test, y_train, y_test = get_train_test_data(\n        dataframe,\n        config['feature_cols'],\n        config['target_col'],\n        config['train_test_split_ratio'],\n        config['random_seed'])\n\n    model = XGBClassifier(\n        max_depth=max_depth,\n        n_estimators=n_estimators,\n        objective=\"binary:logistic\")\n\n    model.fit(X_train, y_train.values.ravel())\n    return model, X_test, y_test\n\n\n# Evaluate prediction accuracy against ground truth.\ndef eval_metrics(y, predictions):\n    cm = confusion_matrix(y, predictions)\n    print(\"Confusion matrix:\")\n    print(cm)\n\n    if len(cm) < 2 or cm[1][1] == 0:  # checking if there are no true positives\n        print('No fraudulent transaction to evaluate')\n        return\n    else:\n        precision = cm[1][1] / (cm[1][1] + cm[0][1])\n        recall = cm[1][1] / (cm[1][1] + cm[1][0])\n        f1 = (2 * (precision * recall) / (precision + recall))\n\n    print(f\"Precision: {precision * 100:.2f}%\")\n    print(f\"Recall: {recall * 100:.2f}%\")\n    print(f\"F1 Score: {f1 * 100:.2f}%\")\n"})})]}),"\n",(0,r.jsxs)(n.p,{children:["The following Python snippet connects to a Feldera service\nand creates a Feldera pipeline to read transaction and demographics\ndata from Delta tables stored in S3 and evaluate the feature query\ndefined above on this data. We use the\n",(0,r.jsx)(n.a,{href:"pathname:///python/feldera.html#feldera.pipeline.Pipeline.listen",children:(0,r.jsx)(n.code,{children:"listen"})}),"\nAPI to read the computed features into a ",(0,r.jsx)(n.a,{href:"https://pandas.pydata.org/",children:"Pandas"}),"\ndataframe. We split this dataframe into train and test sets. We\nuse the former to train an XGBoost model and the latter to measure model\naccuracy."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import pandas as pd\n\n# Connect to the Feldera sandbox.\n# Use the \'Settings\' menu at try.feldera.com to generate an API key\nclient = FelderaClient("https://try.feldera.com", api_key= < FELDERA_API_KEY >)\n\n# Load DEMOGRAPHICS data from a Delta table stored in an S3 bucket.\ndemographics_connectors = [{\n    "transport": {\n        "name": "delta_table_input",\n        "config": {\n            "uri": "s3://feldera-fraud-detection-data/demographics_train",\n            "mode": "snapshot",\n            "aws_skip_signature": "true"\n        }\n    }\n}]\n\n# Load credit card TRANSACTION data.\ntransactions_connectors = [{\n    "transport": {\n        "name": "delta_table_input",\n        "config": {\n            "uri": "s3://feldera-fraud-detection-data/transaction_train",\n            "mode": "snapshot",\n            "aws_skip_signature": "true",\n            "timestamp_column": "unix_time"\n        }\n    }\n}]\n\nsql = build_program(json.dumps(transactions_connectors), json.dumps(demographics_connectors), \'[]\')\n\npipeline = PipelineBuilder(client, name="fraud_detection_training", sql=sql).create_or_replace()\n\nhfeature = pipeline.listen("feature")\n\n# Process full snapshot of the input tables and compute a dataset\n# with feature vectors for use in model training and testing.\npipeline.start()\npipeline.wait_for_completion(shutdown=True)\n\nfeatures_pd = hfeature.to_pandas()\nprint(f"Computed {len(features_pd)} feature vectors")\n\nprint("Training the model")\n\nfeature_cols = list(features_pd.columns.drop(\'is_fraud\'))\n\nconfig = {\n    \'feature_cols\': feature_cols,\n    \'target_col\': [\'is_fraud\'],\n    \'random_seed\': 45,\n    \'train_test_split_ratio\': 0.8\n}\n\ntrained_model, X_test, y_test = train_model(features_pd, config)\n\nprint("Testing the trained model")\n\ny_pred = trained_model.predict(X_test)\neval_metrics(y_test, y_pred)\n'})}),"\n",(0,r.jsx)(n.admonition,{type:"note",children:(0,r.jsxs)(n.p,{children:["Here we use the ",(0,r.jsx)(n.a,{href:"https://try.feldera.com",children:"Feldera online sandbox"}),".\nYou can also use a local instance of Feldera running in a Docker\ncontainer on ",(0,r.jsx)(n.a,{href:"http://127.0.0.1:8080",children:"http://127.0.0.1:8080"}),".\nSee ",(0,r.jsx)(n.a,{href:"/get-started/docker",children:"instructions"})," for running Feldera in Docker."]})}),"\n",(0,r.jsx)(n.admonition,{type:"note",children:(0,r.jsxs)(n.p,{children:["The full\n",(0,r.jsx)(n.a,{href:"https://github.com/feldera/feldera/blob/main/demo/project_demo10-FraudDetectionDeltaLake/run.py",children:"Python script"}),"\nfor this use case also contains the code to write computed features\nto a Delta Lake, which might be preferrable when working with larger\ndatasets. This functionality requires write credentials to an S3 bucket\nor some other object store and is therefore disabled by default."]})}),"\n",(0,r.jsx)(n.h2,{id:"real-time-inference",children:"Real-time inference"}),"\n",(0,r.jsx)(n.p,{children:"During real-time feature computation, raw data arrives from a streaming\nsource like Kafka. Feldera can ingest data directly from such sources, but in\nthis case we will assume that Kafka is connected to a Delta table, and configure\nFeldera to ingest the data by following the transaction log of the table."}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{alt:"Real-time feature computation for model inference",src:t(65876).A+"",width:"1287",height:"380"})}),"\n",(0,r.jsx)(n.p,{children:"There are several advantages to using this setup in production, but it is also great\nfor demos, as it allows us to stream data from a pre-filled table without having to\nmanage a Kafka queue."}),"\n",(0,r.jsx)(n.p,{children:"We define a Python function to feed a Pandas dataframe to\nour trained ML model for inference."}),"\n",(0,r.jsx)("summary",{children:" Click to see full Python code "}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'def inference(trained_model, df):\n    print(f"\\nReceived {len(df)} feature vectors.")\n    if len(df) == 0:\n        return\n\n    feature_cols_inf = list(df.columns.drop(\'is_fraud\'))\n    X_inf = df[feature_cols_inf].values  # convert to numpy array\n    y_inf = df["is_fraud"].values\n    predictions_inf = trained_model.predict(X_inf)\n\n    eval_metrics(y_inf, predictions_inf)\n'})}),"\n",(0,r.jsxs)(n.p,{children:["The next Python snippet builds another Feldera pipeline to evaluate the feature\nquery over streaming data and send computed feature vectors to the ML model for\ninference. It is almost identical to our training setup, except that this time\nwe read data from the inference dataset. In addition, we configure the input\nconnector for the ",(0,r.jsx)(n.code,{children:"TRANSACTION"})," table to ingest transaction data in the\n",(0,r.jsx)(n.a,{href:"/connectors/sources/delta#delta-lake-input-connector-configuration",children:"snapshot-and-follow"})," mode.\nIn this mode, the connector reads the initial snapshot of the table before following\nthe stream of changes in its transaction log. This ",(0,r.jsx)(n.strong,{children:"backfill"})," pattern is necessary\nto correctly evaluate features that depend on historical data such as rolling\nsums and averages."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# How long to run the inference pipeline for.\nINFERENCE_TIME_SECONDS = 30\n\nprint(f"\\nRunning the inference pipeline for {INFERENCE_TIME_SECONDS} seconds")\n\n# Load DEMOGRAPHICS data from a Delta table.\ndemographics_connectors = [{\n    "transport": {\n        "name": "delta_table_input",\n        "config": {\n            "uri": "s3://feldera-fraud-detection-data/demographics_infer",\n            "mode": "snapshot",\n            "aws_skip_signature": "true"\n        }\n    }\n}]\n\n# Read TRANSACTION data from a Delta table.\n# Configure the Delta Lake connector to read the initial snapshot of\n# the table before following the stream of changes in its transaction log.\ntransactions_connectors = [{\n    "transport": {\n        "name": "delta_table_input",\n        "config": {\n            "uri": "s3://feldera-fraud-detection-data/transaction_infer",\n            "mode": "snapshot_and_follow",\n            "version": 10,\n            "timestamp_column": "unix_time",\n            "aws_skip_signature": "true"\n        }\n    }\n}]\n\nsql = build_program(json.dumps(transactions_connectors), json.dumps(demographics_connectors), \'[]\')\npipeline = PipelineBuilder(client, name="fraud_detection_inference", sql=sql).create_or_replace()\n\npipeline.foreach_chunk("feature", lambda df, chunk: inference(trained_model, df))\n\n# Start the pipeline to continuously process the input stream of credit card\n# transactions and output newly computed feature vectors to a Delta table.\npipeline.start()\n\ntime.sleep(INFERENCE_TIME_SECONDS)\n\nprint(f"Shutting down the inference pipeline after {INFERENCE_TIME_SECONDS} seconds")\npipeline.shutdown()\n'})}),"\n",(0,r.jsx)(n.p,{children:"Running this code produces output similar to the following:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Running the inference pipeline for 30 seconds\n\nReceived 8185 feature vectors.\nConfusion matrix:\n[[7965   98]\n [  26   96]]\nPrecision: 49.48%\nRecall: 78.69%\nF1 Score: 60.76%\n\nReceived 8183 feature vectors.\nConfusion matrix:\n[[7993   10]\n [  34  146]]\nPrecision: 93.59%\nRecall: 81.11%\nF1 Score: 86.90%\n\nReceived 10000 feature vectors.\nConfusion matrix:\n[[9823    7]\n [  53  117]]\nPrecision: 94.35%\nRecall: 68.82%\nF1 Score: 79.59%\n\nReceived 6373 feature vectors.\nConfusion matrix:\n[[6251   10]\n [  27   85]]\nPrecision: 89.47%\nRecall: 75.89%\nF1 Score: 82.13%\n\n...\n"})}),"\n",(0,r.jsxs)(n.p,{children:["While the pipeline is running, we can monitor its progress in the Feldera Web Console.\nOpen [try.feldera.com] in your browser and select the ",(0,r.jsx)(n.code,{children:"fraud_detection_inference"})," pipeline."]}),"\n",(0,r.jsx)(n.h2,{id:"takeaways",children:"Takeaways"}),"\n",(0,r.jsxs)(n.p,{children:["In this example we used Feldera to evaluate ",(0,r.jsx)(n.em,{children:"the same feature queries"})," first\nover historical (batch) data and then over a combination of historical and\nstreaming inputs. Feldera's ability to operate on any combination of batch and\nstreaming sources is crucial for real-time feature engineering, as it eliminates\nthe need to develop multiple implementations of the same queries for development\nand production environments.  ",(0,r.jsx)(n.strong,{children:"In fact, Feldera does not distinguish between\nthe two"}),". Internally, it represents all inputs as changes (inserts, deletes\nand updates) to input tables. It processes changes in the same way and produces\nthe same outputs, whether they arrive frequently in small groups (aka streaming)\nor occasionally in bigger groups (aka batch)."]}),"\n",(0,r.jsxs)(n.p,{children:["Upon receiving a set of changes, Feldera updates its output views without full\nre-computation, by doing work proportional to the size of the change rather than\nthe size of the entire database. This ",(0,r.jsx)(n.strong,{children:"incremental evaluation"})," makes Feldera\nefficient for both streaming and batch inputs."]}),"\n",(0,r.jsxs)(n.p,{children:["Finally, we would like to emphaize that ",(0,r.jsx)(n.strong,{children:"Feldera is strongly consistent"}),". If\nwe pause our inference pipeline and inspect the contents of the output view\nproduced by Feldera so far, it will be ",(0,r.jsx)(n.strong,{children:"precisely the same as if we ran the\nquery on all the inputs received so far as one large batch"}),". Unpause the\npipeline and run it a little longer. The pipeline will receive some additional\ninputs and produce additional outputs, but it still preserves the same\ninput/output guarantee. This property, known as ",(0,r.jsx)(n.strong,{children:"strong consistency"}),", ensures\nthat the prediction accuracy of your ML model will not be affected by incorrect\ninput."]}),"\n",(0,r.jsx)(n.h2,{id:"resources",children:"Resources"}),"\n",(0,r.jsxs)(n.p,{children:["A version of this use case focusing on integration with the Delta Lake and Spark ecosystem\nis published as a ",(0,r.jsx)(n.a,{href:"https://www.feldera.com/blog/feature-engineering-part2/",children:"blog post"}),"."]})]})}function u(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(l,{...e})}):l(e)}}}]);