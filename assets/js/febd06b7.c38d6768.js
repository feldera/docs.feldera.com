"use strict";(self.webpackChunkfeldera_docs=self.webpackChunkfeldera_docs||[]).push([[9129],{3014:(e,t,n)=>{n.r(t),n.d(t,{contentTitle:()=>a,default:()=>d,frontMatter:()=>o,metadata:()=>i,toc:()=>l});const i=JSON.parse('{"type":"api","id":"fully-update-a-pipeline-if-it-already-exists-otherwise-create-a-new-pipeline","title":"Fully update a pipeline if it already exists, otherwise create a new pipeline.","description":"","slug":"/fully-update-a-pipeline-if-it-already-exists-otherwise-create-a-new-pipeline","frontMatter":{},"api":{"tags":["Pipeline management"],"operationId":"put_pipeline","parameters":[{"name":"pipeline_name","in":"path","description":"Unique pipeline name","required":true,"schema":{"type":"string"}}],"requestBody":{"content":{"application/json":{"schema":{"type":"object","description":"Create a new pipeline (POST), or fully update an existing pipeline (PUT).\\nFields which are optional and not provided will be set to their empty type value\\n(for strings: an empty string `\\"\\"`, for objects: an empty dictionary `{}`).","required":["name","program_code"],"properties":{"description":{"type":"string","nullable":true},"name":{"type":"string"},"program_code":{"type":"string"},"program_config":{"nullable":true,"type":"object","description":"Program configuration.","properties":{"cache":{"type":"boolean","description":"If `true` (default), when a prior compilation with the same checksum\\nalready exists, the output of that (i.e., binary) is used.\\nSet `false` to always trigger a new compilation, which might take longer\\nand as well can result in overriding an existing binary.","default":true},"profile":{"default":null,"nullable":true,"type":"string","description":"Enumeration of possible compilation profiles that can be passed to the Rust compiler\\nas an argument via `cargo build --profile <>`. A compilation profile affects among\\nother things the compilation speed (how long till the program is ready to be run)\\nand runtime speed (the performance while running).","enum":["dev","unoptimized","optimized"]},"runtime_version":{"type":"string","description":"Override runtime version of the pipeline being executed.\\n\\nWarning: This option is experimental and may change in the future.\\nShould only be used for CI/testing purposes, and requires network access.\\n\\nA runtime version can be specified in the form of a version\\nor SHA taken from the `feldera/feldera` repository main branch.\\n\\nExamples: `v0.96.0` or `f4dcac0989ca0fda7d2eb93602a49d007cb3b0ae`\\n\\nA platform of version `0.x.y` may be capable of running future and past\\nruntimes with versions `>=0.x.y` and `<=0.x.y` until breaking API changes happen,\\nthe exact bounds for each platform version are unspecified until we reach a\\nstable version. Compatibility is only guaranteed if platform and runtime version\\nare exact matches.\\n\\nNote that any enterprise features are currently considered to be part of\\nthe platform.\\n\\nIf not set (null), the runtime version will be the same as the platform version.","default":null,"nullable":true}}},"runtime_config":{"nullable":true,"type":"object","description":"Global pipeline configuration settings. This is the publicly\\nexposed type for users to configure pipelines.","properties":{"checkpoint_during_suspend":{"type":"boolean","description":"Deprecated: setting this true or false does not have an effect anymore.","default":true},"clock_resolution_usecs":{"type":"integer","format":"int64","description":"Real-time clock resolution in microseconds.\\n\\nThis parameter controls the execution of queries that use the `NOW()` function.  The output of such\\nqueries depends on the real-time clock and can change over time without any external\\ninputs.  The pipeline will update the clock value and trigger incremental recomputation\\nat most each `clock_resolution_usecs` microseconds.\\n\\nIt is set to 1 second (1,000,000 microseconds) by default.\\n\\nSet to `null` to disable periodic clock updates.","default":1000000,"nullable":true,"minimum":0},"cpu_profiler":{"type":"boolean","description":"Enable CPU profiler.\\n\\nThe default value is `true`.","default":true},"dev_tweaks":{"type":"object","description":"Optional settings for tweaking Feldera internals.\\n\\nThe available key-value pairs change from one version of Feldera to\\nanother, so users should not depend on particular settings being\\navailable, or on their behavior.","default":{},"additionalProperties":{}},"fault_tolerance":{"default":{"model":"none","checkpoint_interval_secs":60},"type":"object","description":"Fault-tolerance configuration.\\n\\nThe default [FtConfig] (via [FtConfig::default]) disables fault tolerance,\\nwhich is the configuration that one gets if [RuntimeConfig] omits fault\\ntolerance configuration.\\n\\nThe default value for [FtConfig::model] enables fault tolerance, as\\n`Some(FtModel::default())`.  This is the configuration that one gets if\\n[RuntimeConfig] includes a fault tolerance configuration but does not\\nspecify a particular model.","properties":{"checkpoint_interval_secs":{"type":"integer","format":"int64","description":"Interval between automatic checkpoints, in seconds.\\n\\nThe default is 60 seconds.  Values less than 1 or greater than 3600 will\\nbe forced into that range.","nullable":true,"minimum":0},"model":{"oneOf":[{"type":"string","description":"Fault tolerance model.\\n\\nThe ordering is significant: we consider [Self::ExactlyOnce] to be a \\"higher\\nlevel\\" of fault tolerance than [Self::AtLeastOnce].","enum":["at_least_once","exactly_once"]},{"type":"string","enum":["none"]}],"default":"exactly_once"}}},"http_workers":{"type":"integer","format":"int64","description":"Sets the number of available runtime threads for the http server.\\n\\nIn most cases, this does not need to be set explicitly and\\nthe default is sufficient. Can be increased in case the\\npipeline HTTP API operations are a bottleneck.\\n\\nIf not specified, the default is set to `workers`.","default":null,"nullable":true,"minimum":0},"init_containers":{"description":"Specification of additional (sidecar) containers.","nullable":true},"io_workers":{"type":"integer","format":"int64","description":"Sets the number of available runtime threads for async IO tasks.\\n\\nThis affects some networking and file I/O operations\\nespecially adapters and ad-hoc queries.\\n\\nIn most cases, this does not need to be set explicitly and\\nthe default is sufficient. Can be increased in case\\ningress, egress or ad-hoc queries are a bottleneck.\\n\\nIf not specified, the default is set to `workers`.","default":null,"nullable":true,"minimum":0},"logging":{"type":"string","description":"Log filtering directives.\\n\\nIf set to a valid [tracing-subscriber] filter, this controls the log\\nmessages emitted by the pipeline process.  Otherwise, or if the filter\\nhas invalid syntax, messages at \\"info\\" severity and higher are written\\nto the log and all others are discarded.\\n\\n[tracing-subscriber]: https://docs.rs/tracing-subscriber/latest/tracing_subscriber/filter/struct.EnvFilter.html#directives","default":null,"nullable":true},"max_buffering_delay_usecs":{"type":"integer","format":"int64","description":"Maximal delay in microseconds to wait for `min_batch_size_records` to\\nget buffered by the controller, defaults to 0.","default":0,"minimum":0},"max_parallel_connector_init":{"type":"integer","format":"int64","description":"The maximum number of connectors initialized in parallel during pipeline\\nstartup.\\n\\nAt startup, the pipeline must initialize all of its input and output connectors.\\nDepending on the number and types of connectors, this can take a long time.\\nTo accelerate the process, multiple connectors are initialized concurrently.\\nThis option controls the maximum number of connectors that can be initialized\\nin parallel.\\n\\nThe default is 10.","default":null,"nullable":true,"minimum":0},"min_batch_size_records":{"type":"integer","format":"int64","description":"Minimal input batch size.\\n\\nThe controller delays pushing input records to the circuit until at\\nleast `min_batch_size_records` records have been received (total\\nacross all endpoints) or `max_buffering_delay_usecs` microseconds\\nhave passed since at least one input records has been buffered.\\nDefaults to 0.","default":0,"minimum":0},"pin_cpus":{"type":"array","items":{"type":"integer","minimum":0},"description":"Optionally, a list of CPU numbers for CPUs to which the pipeline may pin\\nits worker threads.  Specify at least twice as many CPU numbers as\\nworkers.  CPUs are generally numbered starting from 0.  The pipeline\\nmight not be able to honor CPU pinning requests.\\n\\nCPU pinning can make pipelines run faster and perform more consistently,\\nas long as different pipelines running on the same machine are pinned to\\ndifferent CPUs.","default":[]},"provisioning_timeout_secs":{"type":"integer","format":"int64","description":"Timeout in seconds for the `Provisioning` phase of the pipeline.\\nSetting this value will override the default of the runner.","default":null,"nullable":true,"minimum":0},"resources":{"default":{"cpu_cores_min":null,"cpu_cores_max":null,"memory_mb_min":null,"memory_mb_max":null,"storage_mb_max":null,"storage_class":null},"type":"object","properties":{"cpu_cores_max":{"type":"integer","format":"int64","description":"The maximum number of CPU cores to reserve\\nfor an instance of this pipeline","default":null,"nullable":true,"minimum":0},"cpu_cores_min":{"type":"integer","format":"int64","description":"The minimum number of CPU cores to reserve\\nfor an instance of this pipeline","default":null,"nullable":true,"minimum":0},"memory_mb_max":{"type":"integer","format":"int64","description":"The maximum memory in Megabytes to reserve\\nfor an instance of this pipeline","default":null,"nullable":true,"minimum":0},"memory_mb_min":{"type":"integer","format":"int64","description":"The minimum memory in Megabytes to reserve\\nfor an instance of this pipeline","default":null,"nullable":true,"minimum":0},"storage_class":{"type":"string","description":"Storage class to use for an instance of this pipeline.\\nThe class determines storage performance such as IOPS and throughput.","default":null,"nullable":true},"storage_mb_max":{"type":"integer","format":"int64","description":"The total storage in Megabytes to reserve\\nfor an instance of this pipeline","default":null,"nullable":true,"minimum":0}}},"storage":{"default":{"backend":{"name":"default"},"min_storage_bytes":null,"min_step_storage_bytes":null,"compression":"default","cache_mib":null},"nullable":true,"type":"object","description":"Storage configuration for a pipeline.","properties":{"backend":{"default":{"name":"default"},"oneOf":[{"type":"object","required":["name"],"properties":{"name":{"type":"string","enum":["default"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","description":"Configuration for local file system access.","properties":{"async_threads":{"type":"boolean","description":"Whether to use background threads for file I/O.\\n\\nBackground threads should improve performance, but they can reduce\\nperformance if too few cores are available. This is provided for\\ndebugging and fine-tuning and should ordinarily be left unset.","default":null,"nullable":true},"ioop_delay":{"type":"integer","format":"int64","description":"Per-I/O operation sleep duration, in milliseconds.\\n\\nThis is for simulating slow storage devices.  Do not use this in\\nproduction.","default":null,"nullable":true,"minimum":0},"sync":{"default":null,"nullable":true,"type":"object","required":["bucket","start_from_checkpoint"],"properties":{"access_key":{"type":"string","description":"The access key used to authenticate with the storage provider.\\n\\nIf not provided, rclone will fall back to environment-based credentials, such as\\n`RCLONE_S3_ACCESS_KEY_ID`. In Kubernetes environments using IRSA (IAM Roles for Service Accounts),\\nthis can be left empty to allow automatic authentication via the pod\'s service account.","nullable":true},"bucket":{"type":"string","description":"The name of the storage bucket.\\n\\nThis may include a path to a folder inside the bucket (e.g., `my-bucket/data`)."},"endpoint":{"type":"string","description":"The endpoint URL for the storage service.\\n\\nThis is typically required for custom or local S3-compatible storage providers like MinIO.\\nExample: `http://localhost:9000`\\n\\nRelevant rclone config key: [`endpoint`](https://rclone.org/s3/#s3-endpoint)","nullable":true},"provider":{"type":"string","description":"The name of the cloud storage provider (e.g., `\\"AWS\\"`, `\\"Minio\\"`).\\n\\nUsed for provider-specific behavior in rclone.\\nIf omitted, defaults to `\\"Other\\"`.\\n\\nSee [rclone S3 provider documentation](https://rclone.org/s3/#s3-provider)","nullable":true},"region":{"type":"string","description":"The region that this bucket is in.\\n\\nLeave empty for Minio or the default region (`us-east-1` for AWS).","nullable":true},"secret_key":{"type":"string","description":"The secret key used together with the access key for authentication.\\n\\nIf not provided, rclone will fall back to environment-based credentials, such as\\n`RCLONE_S3_SECRET_ACCESS_KEY`. In Kubernetes environments using IRSA (IAM Roles for Service Accounts),\\nthis can be left empty to allow automatic authentication via the pod\'s service account.","nullable":true},"start_from_checkpoint":{"type":"boolean","description":"If `true`, will try to pull the latest checkpoint from the configured\\nobject store and resume from that point."}}}}},"name":{"type":"string","enum":["file"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","required":["url"],"properties":{"url":{"type":"string","description":"URL.\\n\\nThe following URL schemes are supported:\\n\\n* S3:\\n- `s3://<bucket>/<path>`\\n- `s3a://<bucket>/<path>`\\n- `https://s3.<region>.amazonaws.com/<bucket>`\\n- `https://<bucket>.s3.<region>.amazonaws.com`\\n- `https://ACCOUNT_ID.r2.cloudflarestorage.com/bucket`\\n* Google Cloud Storage:\\n- `gs://<bucket>/<path>`\\n* Microsoft Azure Blob Storage:\\n- `abfs[s]://<container>/<path>` (according to [fsspec](https://github.com/fsspec/adlfs))\\n- `abfs[s]://<file_system>@<account_name>.dfs.core.windows.net/<path>`\\n- `abfs[s]://<file_system>@<account_name>.dfs.fabric.microsoft.com/<path>`\\n- `az://<container>/<path>` (according to [fsspec](https://github.com/fsspec/adlfs))\\n- `adl://<container>/<path>` (according to [fsspec](https://github.com/fsspec/adlfs))\\n- `azure://<container>/<path>` (custom)\\n- `https://<account>.dfs.core.windows.net`\\n- `https://<account>.blob.core.windows.net`\\n- `https://<account>.blob.core.windows.net/<container>`\\n- `https://<account>.dfs.fabric.microsoft.com`\\n- `https://<account>.dfs.fabric.microsoft.com/<container>`\\n- `https://<account>.blob.fabric.microsoft.com`\\n- `https://<account>.blob.fabric.microsoft.com/<container>`\\n\\nSettings derived from the URL will override other settings."}},"additionalProperties":{"type":"string","description":"Additional options as key-value pairs.\\n\\nThe following keys are supported:\\n\\n* S3:\\n- `access_key_id`: AWS Access Key.\\n- `secret_access_key`: AWS Secret Access Key.\\n- `region`: Region.\\n- `default_region`: Default region.\\n- `endpoint`: Custom endpoint for communicating with S3,\\ne.g. `https://localhost:4566` for testing against a localstack\\ninstance.\\n- `token`: Token to use for requests (passed to underlying provider).\\n- [Other keys](https://docs.rs/object_store/latest/object_store/aws/enum.AmazonS3ConfigKey.html#variants).\\n* Google Cloud Storage:\\n- `service_account`: Path to the service account file.\\n- `service_account_key`: The serialized service account key.\\n- `google_application_credentials`: Application credentials path.\\n- [Other keys](https://docs.rs/object_store/latest/object_store/gcp/enum.GoogleConfigKey.html).\\n* Microsoft Azure Blob Storage:\\n- `access_key`: Azure Access Key.\\n- `container_name`: Azure Container Name.\\n- `account`: Azure Account.\\n- `bearer_token_authorization`: Static bearer token for authorizing requests.\\n- `client_id`: Client ID for use in client secret or Kubernetes federated credential flow.\\n- `client_secret`: Client secret for use in client secret flow.\\n- `tenant_id`: Tenant ID for use in client secret or Kubernetes federated credential flow.\\n- `endpoint`: Override the endpoint for communicating with blob storage.\\n- [Other keys](https://docs.rs/object_store/latest/object_store/azure/enum.AzureConfigKey.html#variants).\\n\\nOptions set through the URL take precedence over those set with these\\noptions."}},"name":{"type":"string","enum":["object"]}}}],"description":"Backend storage configuration.","discriminator":{"propertyName":"name"}},"cache_mib":{"type":"integer","description":"The maximum size of the in-memory storage cache, in MiB.\\n\\nIf set, the specified cache size is spread across all the foreground and\\nbackground threads. If unset, each foreground or background thread cache\\nis limited to 256 MiB.","default":null,"nullable":true,"minimum":0},"compression":{"default":"default","type":"string","description":"Storage compression algorithm.","enum":["default","none","snappy"]},"min_step_storage_bytes":{"type":"integer","description":"For a batch of data passed through the pipeline during a single step,\\nthe minimum estimated number of bytes to write it to storage.\\n\\nThis is provided for debugging and fine-tuning and should ordinarily be\\nleft unset.  A value of 0 will write even empty batches to storage, and\\nnonzero values provide a threshold.  `usize::MAX`, the default,\\neffectively disables storage for such batches.  If it is set to another\\nvalue, it should ordinarily be greater than or equal to\\n`min_storage_bytes`.","default":null,"nullable":true,"minimum":0},"min_storage_bytes":{"type":"integer","description":"For a batch of data maintained as part of a persistent index during a\\npipeline run, the minimum estimated number of bytes to write it to\\nstorage.\\n\\nThis is provided for debugging and fine-tuning and should ordinarily be\\nleft unset.\\n\\nA value of 0 will write even empty batches to storage, and nonzero\\nvalues provide a threshold.  `usize::MAX` would effectively disable\\nstorage for such batches.  The default is 1,048,576 (1 MiB).","default":null,"nullable":true,"minimum":0}}},"tracing":{"type":"boolean","description":"Enable pipeline tracing.","default":false},"tracing_endpoint_jaeger":{"type":"string","description":"Jaeger tracing endpoint to send tracing information to.","default":"127.0.0.1:6831"},"workers":{"type":"integer","format":"int32","description":"Number of DBSP worker threads.\\n\\nEach DBSP \\"foreground\\" worker thread is paired with a \\"background\\"\\nthread for LSM merging, making the total number of threads twice the\\nspecified number.\\n\\nThe typical sweet spot for the number of workers is between 4 and 16.\\nEach worker increases overall memory consumption for data structures\\nused during a step.","default":8,"minimum":0}}},"udf_rust":{"type":"string","nullable":true},"udf_toml":{"type":"string","nullable":true}}},"example":{"name":"example1","description":"Description of the pipeline example1","runtime_config":{"workers":16,"storage":{"backend":{"name":"default"},"min_storage_bytes":null,"min_step_storage_bytes":null,"compression":"default","cache_mib":null},"fault_tolerance":{"model":"none","checkpoint_interval_secs":60},"cpu_profiler":true,"tracing":false,"tracing_endpoint_jaeger":"","min_batch_size_records":0,"max_buffering_delay_usecs":0,"resources":{"cpu_cores_min":null,"cpu_cores_max":null,"memory_mb_min":null,"memory_mb_max":null,"storage_mb_max":null,"storage_class":null},"clock_resolution_usecs":1000000,"pin_cpus":[],"provisioning_timeout_secs":null,"max_parallel_connector_init":null,"init_containers":null,"checkpoint_during_suspend":true,"http_workers":null,"io_workers":null,"dev_tweaks":{},"logging":null},"program_code":"CREATE TABLE table1 ( col1 INT );","udf_rust":null,"udf_toml":null,"program_config":{"profile":"optimized","cache":true,"runtime_version":null}}}},"required":true},"responses":{"200":{"description":"Pipeline successfully updated","content":{"application/json":{"schema":{"type":"object","description":"Pipeline information.\\nIt both includes fields which are user-provided and system-generated.","required":["id","name","description","created_at","version","platform_version","runtime_config","program_code","udf_rust","udf_toml","program_config","program_version","program_status","program_status_since","program_error","deployment_status","deployment_status_since","deployment_desired_status","refresh_version","storage_status"],"properties":{"created_at":{"type":"string","format":"date-time"},"deployment_desired_status":{"type":"string","enum":["Stopped","Paused","Running","Suspended"]},"deployment_error":{"nullable":true,"type":"object","description":"Information returned by REST API endpoints on error.","required":["message","error_code","details"],"properties":{"details":{"type":"object","description":"Detailed error metadata.\\nThe contents of this field is determined by `error_code`."},"error_code":{"type":"string","description":"Error code is a string that specifies this error type.","example":"CodeSpecifyingErrorType"},"message":{"type":"string","description":"Human-readable error message.","example":"Explanation of the error that occurred."}}},"deployment_status":{"type":"string","description":"Pipeline status.\\n\\nThis type represents the state of the pipeline tracked by the pipeline\\nrunner and observed by the API client via the `GET /v0/pipelines/{name}` endpoint.\\n\\n### The lifecycle of a pipeline\\n\\nThe following automaton captures the lifecycle of the pipeline.\\nIndividual states and transitions of the automaton are described below.\\n\\n* States labeled with the hourglass symbol (\u231b) are **timed** states. The\\nautomaton stays in timed state until the corresponding operation completes\\nor until it transitions to become failed after the pre-defined timeout\\nperiod expires.\\n\\n* State transitions labeled with API endpoint names (`/start`, `/pause`,\\n`/stop`) are triggered by invoking corresponding endpoint,\\ne.g., `POST /v0/pipelines/{name}/start`. Note that these only express\\ndesired state, and are applied asynchronously by the automata.\\n\\n```text\\nStopped \u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Stopping \u25c4\u2500\u2500\u2500\u2500\u2500 All states can transition\\n\u2502                    \u25b2            to Stopping by either:\\n/start or /pause \u2502                    \u2502            (1) user calling /stop?force=true, or;\\n\u25bc                    \u2502            (2) pipeline encountering a fatal\\n\u231bProvisioning          Suspending            resource or runtime error,\\n\u2502                    \u25b2                having the system call /stop?force=true\\n\u25bc                    \u2502 /stop          effectively\\n\u231bInitializing \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524  ?force=false\\n\u2502                    \u2502\\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2510\\n\u2502         \u25bc                          \u2502\\n\u2502       Paused  \u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u25ba Unavailable \u2502\\n\u2502        \u2502   \u25b2                \u25b2      \u2502\\n\u2502 /start \u2502   \u2502  /pause        \u2502      \u2502\\n\u2502        \u25bc   \u2502                \u2502      \u2502\\n\u2502       Running \u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2502\\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\\n```\\n\\n### Desired and actual status\\n\\nWe use the desired state model to manage the lifecycle of a pipeline.\\nIn this model, the pipeline has two status attributes associated with\\nit at runtime: the **desired** status, which represents what the user\\nwould like the pipeline to do, and the **current** status, which\\nrepresents the actual state of the pipeline.  The pipeline runner\\nservice continuously monitors both fields and steers the pipeline\\ntowards the desired state specified by the user.\\n\\nOnly four of the states in the pipeline automaton above can be\\nused as desired statuses: `Paused`, `Running`, `Suspended` and\\n`Stopped`. These statuses are selected by invoking REST endpoints\\nshown in the diagram (respectively, `/pause`, `/start`, and `/stop`).\\n\\nThe user can monitor the current state of the pipeline via the\\n`GET /v0/pipelines/{name}` endpoint. In a typical scenario,\\nthe user first sets the desired state, e.g., by invoking the\\n`/start` endpoint, and then polls the `GET /v0/pipelines/{name}`\\nendpoint to monitor the actual status of the pipeline until its\\n`deployment_status` attribute changes to `Running` indicating\\nthat the pipeline has been successfully initialized and is\\nprocessing data, or `Stopped` with `deployment_error` being set.","enum":["Stopped","Provisioning","Initializing","Paused","Running","Unavailable","Suspending","Stopping"]},"deployment_status_since":{"type":"string","format":"date-time"},"description":{"type":"string"},"id":{"type":"string","format":"uuid","description":"Pipeline identifier."},"name":{"type":"string"},"platform_version":{"type":"string"},"program_code":{"type":"string"},"program_config":{"type":"object","description":"Program configuration.","properties":{"cache":{"type":"boolean","description":"If `true` (default), when a prior compilation with the same checksum\\nalready exists, the output of that (i.e., binary) is used.\\nSet `false` to always trigger a new compilation, which might take longer\\nand as well can result in overriding an existing binary.","default":true},"profile":{"default":null,"nullable":true,"type":"string","description":"Enumeration of possible compilation profiles that can be passed to the Rust compiler\\nas an argument via `cargo build --profile <>`. A compilation profile affects among\\nother things the compilation speed (how long till the program is ready to be run)\\nand runtime speed (the performance while running).","enum":["dev","unoptimized","optimized"]},"runtime_version":{"type":"string","description":"Override runtime version of the pipeline being executed.\\n\\nWarning: This option is experimental and may change in the future.\\nShould only be used for CI/testing purposes, and requires network access.\\n\\nA runtime version can be specified in the form of a version\\nor SHA taken from the `feldera/feldera` repository main branch.\\n\\nExamples: `v0.96.0` or `f4dcac0989ca0fda7d2eb93602a49d007cb3b0ae`\\n\\nA platform of version `0.x.y` may be capable of running future and past\\nruntimes with versions `>=0.x.y` and `<=0.x.y` until breaking API changes happen,\\nthe exact bounds for each platform version are unspecified until we reach a\\nstable version. Compatibility is only guaranteed if platform and runtime version\\nare exact matches.\\n\\nNote that any enterprise features are currently considered to be part of\\nthe platform.\\n\\nIf not set (null), the runtime version will be the same as the platform version.","default":null,"nullable":true}}},"program_error":{"type":"object","description":"Log, warning and error information about the program compilation.","properties":{"rust_compilation":{"nullable":true,"type":"object","description":"Rust compilation information.","required":["exit_code","stdout","stderr"],"properties":{"exit_code":{"type":"integer","format":"int32","description":"Exit code of the `cargo` compilation command."},"stderr":{"type":"string","description":"Output printed to stderr by the `cargo` compilation command."},"stdout":{"type":"string","description":"Output printed to stdout by the `cargo` compilation command."}}},"sql_compilation":{"nullable":true,"type":"object","description":"SQL compilation information.","required":["exit_code","messages"],"properties":{"exit_code":{"type":"integer","format":"int32","description":"Exit code of the SQL compiler."},"messages":{"type":"array","items":{"type":"object","description":"A SQL compiler error.\\n\\nThe SQL compiler returns a list of errors in the following JSON format if\\nit\'s invoked with the `-je` option.\\n\\n```ignore\\n[ {\\n\\"start_line_number\\" : 2,\\n\\"start_column\\" : 4,\\n\\"end_line_number\\" : 2,\\n\\"end_column\\" : 8,\\n\\"warning\\" : false,\\n\\"error_type\\" : \\"PRIMARY KEY cannot be nullable\\",\\n\\"message\\" : \\"PRIMARY KEY column \'C\' has type INTEGER, which is nullable\\",\\n\\"snippet\\" : \\"    2|   c INT PRIMARY KEY\\\\n         ^^^^^\\\\n    3|);\\\\n\\"\\n} ]\\n```","required":["start_line_number","start_column","end_line_number","end_column","warning","error_type","message"],"properties":{"end_column":{"type":"integer","minimum":0},"end_line_number":{"type":"integer","minimum":0},"error_type":{"type":"string"},"message":{"type":"string"},"snippet":{"type":"string","nullable":true},"start_column":{"type":"integer","minimum":0},"start_line_number":{"type":"integer","minimum":0},"warning":{"type":"boolean"}}},"description":"Messages (warnings and errors) generated by the SQL compiler."}}},"system_error":{"type":"string","description":"System error that occurred.\\n- Set `Some(...)` upon transition to `SystemError`\\n- Set `None` upon transition to `Pending`","nullable":true}}},"program_info":{"nullable":true,"type":"object","description":"Program information is the result of the SQL compilation.","required":["schema","udf_stubs","input_connectors","output_connectors"],"properties":{"input_connectors":{"type":"object","description":"Input connectors derived from the schema.","additionalProperties":{"description":"Describes an input connector configuration","type":"object","required":["stream"],"properties":{"stream":{"type":"string","description":"The name of the input stream of the circuit that this endpoint is\\nconnected to."},"enable_output_buffer":{"type":"boolean","description":"Enable output buffering.\\n\\nThe output buffering mechanism allows decoupling the rate at which the pipeline\\npushes changes to the output transport from the rate of input changes.\\n\\nBy default, output updates produced by the pipeline are pushed directly to\\nthe output transport. Some destinations may prefer to receive updates in fewer\\nbigger batches. For instance, when writing Parquet files, producing\\none bigger file every few minutes is usually better than creating\\nsmall files every few milliseconds.\\n\\nTo achieve such input/output decoupling, users can enable output buffering by\\nsetting the `enable_output_buffer` flag to `true`.  When buffering is enabled, output\\nupdates produced by the pipeline are consolidated in an internal buffer and are\\npushed to the output transport when one of several conditions is satisfied:\\n\\n* data has been accumulated in the buffer for more than `max_output_buffer_time_millis`\\nmilliseconds.\\n* buffer size exceeds `max_output_buffer_size_records` records.\\n\\nThis flag is `false` by default.","default":false},"max_output_buffer_size_records":{"type":"integer","description":"Maximum number of updates to be kept in the output buffer.\\n\\nThis parameter bounds the maximal size of the buffer.\\nNote that the size of the buffer is not always equal to the\\ntotal number of updates output by the pipeline. Updates to the\\nsame record can overwrite or cancel previous updates.\\n\\nBy default, the buffer can grow indefinitely until one of\\nthe other output conditions is satisfied.\\n\\nNOTE: this configuration option requires the `enable_output_buffer` flag\\nto be set.","default":18446744073709552000,"minimum":0},"max_output_buffer_time_millis":{"type":"integer","description":"Maximum time in milliseconds data is kept in the output buffer.\\n\\nBy default, data is kept in the buffer indefinitely until one of\\nthe other output conditions is satisfied.  When this option is\\nset the buffer will be flushed at most every\\n`max_output_buffer_time_millis` milliseconds.\\n\\nNOTE: this configuration option requires the `enable_output_buffer` flag\\nto be set.","default":18446744073709552000,"minimum":0},"format":{"nullable":true,"type":"object","description":"Data format specification used to parse raw data received from the\\nendpoint or to encode data sent to the endpoint.","required":["name"],"properties":{"config":{"type":"object","description":"Format-specific parser or encoder configuration."},"name":{"type":"string","description":"Format name, e.g., \\"csv\\", \\"json\\", \\"bincode\\", etc."}}},"index":{"type":"string","description":"Name of the index that the connector is attached to.\\n\\nThis property is valid for output connectors only.  It is used with data\\ntransports and formats that expect output updates in the form of key/value\\npairs, where the key typically represents a unique id associated with the\\ntable or view.\\n\\nTo support such output formats, an output connector can be attached to an\\nindex created using the SQL CREATE INDEX statement.  An index of a table\\nor view contains the same updates as the table or view itself, indexed by\\none or more key columns.\\n\\nSee individual connector documentation for details on how they work\\nwith indexes.","nullable":true},"labels":{"type":"array","items":{"type":"string"},"description":"Arbitrary user-defined text labels associated with the connector.\\n\\nThese labels can be used in conjunction with the `start_after` property\\nto control the start order of connectors."},"max_batch_size":{"type":"integer","format":"int64","description":"Maximum batch size, in records.\\n\\nThis is the maximum number of records to process in one batch through\\nthe circuit.  The time and space cost of processing a batch is\\nasymptotically superlinear in the size of the batch, but very small\\nbatches are less efficient due to constant factors.\\n\\nThis should usually be less than `max_queued_records`, to give the\\nconnector a round-trip time to restart and refill the buffer while\\nbatches are being processed.\\n\\nSome input adapters might not honor this setting.\\n\\nThe default is 10,000.","minimum":0},"max_queued_records":{"type":"integer","format":"int64","description":"Backpressure threshold.\\n\\nMaximal number of records queued by the endpoint before the endpoint\\nis paused by the backpressure mechanism.\\n\\nFor input endpoints, this setting bounds the number of records that have\\nbeen received from the input transport but haven\'t yet been consumed by\\nthe circuit since the circuit, since the circuit is still busy processing\\nprevious inputs.\\n\\nFor output endpoints, this setting bounds the number of records that have\\nbeen produced by the circuit but not yet sent via the output transport endpoint\\nnor stored in the output buffer (see `enable_output_buffer`).\\n\\nNote that this is not a hard bound: there can be a small delay between\\nthe backpressure mechanism is triggered and the endpoint is paused, during\\nwhich more data may be queued.\\n\\nThe default is 1 million.","minimum":0},"paused":{"type":"boolean","description":"Create connector in paused state.\\n\\nThe default is `false`."},"start_after":{"type":"array","items":{"type":"string"},"description":"Start the connector after all connectors with specified labels.\\n\\nThis property is used to control the start order of connectors.\\nThe connector will not start until all connectors with the specified\\nlabels have finished processing all inputs.","nullable":true},"transport":{"oneOf":[{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","description":"Configuration for reading data from a file with `FileInputTransport`","required":["path"],"properties":{"buffer_size_bytes":{"type":"integer","description":"Read buffer size.\\n\\nDefault: when this parameter is not specified, a platform-specific\\ndefault is used.","nullable":true,"minimum":0},"follow":{"type":"boolean","description":"Enable file following.\\n\\nWhen `false`, the endpoint outputs an `InputConsumer::eoi`\\nmessage and stops upon reaching the end of file.  When `true`, the\\nendpoint will keep watching the file and outputting any new content\\nappended to it."},"path":{"type":"string","description":"File path."}}},"name":{"type":"string","enum":["file_input"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","description":"Configuration for writing data to a file with `FileOutputTransport`.","required":["path"],"properties":{"path":{"type":"string","description":"File path."}}},"name":{"type":"string","enum":["file_output"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","description":"Configuration for reading data from Kafka topics with `InputTransport`.","required":["topic"],"properties":{"group_join_timeout_secs":{"type":"integer","format":"int32","description":"Maximum timeout in seconds to wait for the endpoint to join the Kafka\\nconsumer group during initialization.","minimum":0},"log_level":{"nullable":true,"type":"string","description":"Kafka logging levels.","enum":["emerg","alert","critical","error","warning","notice","info","debug"]},"partitions":{"type":"array","items":{"type":"integer","format":"int32"},"description":"The list of Kafka partitions to read from.\\n\\nOnly the specified partitions will be consumed. If this field is not set,\\nthe connector will consume from all available partitions.\\n\\nIf `start_from` is set to `offsets` and this field is provided, the\\nnumber of partitions must exactly match the number of offsets, and the\\norder of partitions must correspond to the order of offsets.\\n\\nIf offsets are provided for all partitions, this field can be omitted.","nullable":true},"poller_threads":{"type":"integer","description":"Set to 1 or more to fix the number of threads used to poll\\n`rdkafka`. Multiple threads can increase performance with small Kafka\\nmessages; for large messages, one thread is enough. In either case, too\\nmany threads can harm performance. If unset, the default is 3, which\\nhelps with small messages but will not harm performance with large\\nmessagee","nullable":true,"minimum":0},"region":{"type":"string","description":"The AWS region to use while connecting to AWS Managed Streaming for Kafka (MSK).","nullable":true},"start_from":{"oneOf":[{"type":"string","description":"Start from the beginning of the topic.","enum":["earliest"]},{"type":"string","description":"Start from the current end of the topic.\\n\\nThis will only read any data that is added to the topic after the\\nconnector initializes.","enum":["latest"]},{"type":"object","required":["offsets"],"properties":{"offsets":{"type":"array","items":{"type":"integer","format":"int64"},"description":"Start from particular offsets in the topic.\\n\\nThe number of offsets must match the number of partitions in the topic."}}}],"description":"Where to begin reading a Kafka topic."},"topic":{"type":"string","description":"Topic to subscribe to."}},"additionalProperties":{"type":"string","description":"Options passed directly to `rdkafka`.\\n\\n[`librdkafka` options](https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md)\\nused to configure the Kafka consumer.\\n\\nThis input connector does not use consumer groups, so options related to\\nconsumer groups are rejected, including:\\n\\n* `group.id`, if present, is ignored.\\n* `auto.offset.reset` (use `start_from` instead).\\n* \\"enable.auto.commit\\", if present, must be set to \\"false\\".\\n* \\"enable.auto.offset.store\\", if present, must be set to \\"false\\"."}},"name":{"type":"string","enum":["kafka_input"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","description":"Configuration for writing data to a Kafka topic with `OutputTransport`.","required":["topic"],"properties":{"fault_tolerance":{"nullable":true,"type":"object","description":"Fault tolerance configuration for Kafka output connector.","properties":{"consumer_options":{"type":"object","description":"Options passed to `rdkafka` for consumers only, as documented at\\n[`librdkafka`\\noptions](https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md).\\n\\nThese options override `kafka_options` for consumers, and may be empty.","default":{},"additionalProperties":{"type":"string"}},"producer_options":{"type":"object","description":"Options passed to `rdkafka` for producers only, as documented at\\n[`librdkafka`\\noptions](https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md).\\n\\nThese options override `kafka_options` for producers, and may be empty.","default":{},"additionalProperties":{"type":"string"}}}},"headers":{"type":"array","items":{"type":"object","description":"Kafka message header.","required":["key"],"properties":{"key":{"type":"string"},"value":{"nullable":true,"type":"string","format":"binary","description":"Kafka header value encoded as a UTF-8 string or a byte array."}}},"description":"Kafka headers to be added to each message produced by this connector."},"initialization_timeout_secs":{"type":"integer","format":"int32","description":"Maximum timeout in seconds to wait for the endpoint to connect to\\na Kafka broker.\\n\\nDefaults to 60.","minimum":0},"kafka_service":{"type":"string","description":"If specified, this service is used to provide defaults for the Kafka options.","nullable":true},"log_level":{"nullable":true,"type":"string","description":"Kafka logging levels.","enum":["emerg","alert","critical","error","warning","notice","info","debug"]},"region":{"type":"string","description":"The AWS region to use while connecting to AWS Managed Streaming for Kafka (MSK).","nullable":true},"topic":{"type":"string","description":"Topic to write to."}},"additionalProperties":{"type":"string","description":"Options passed directly to `rdkafka`.\\n\\nSee [`librdkafka` options](https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md)\\nused to configure the Kafka producer."}},"name":{"type":"string","enum":["kafka_output"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","description":"Google Pub/Sub input connector configuration.","required":["subscription"],"properties":{"connect_timeout_seconds":{"type":"integer","format":"int32","description":"gRPC connection timeout.","nullable":true,"minimum":0},"credentials":{"type":"string","description":"The content of a Google Cloud credentials JSON file.\\n\\nWhen this option is specified, the connector will use the provided credentials for\\nauthentication.  Otherwise, it will use Application Default Credentials (ADC) configured\\nin the environment where the Feldera service is running.  See\\n[Google Cloud documentation](https://cloud.google.com/docs/authentication/provide-credentials-adc)\\nfor information on configuring application default credentials.\\n\\nWhen running Feldera in an environment where ADC are not configured,\\ne.g., a Docker container, use this option to ship Google Cloud credentials from another environment.\\nFor example, if you use the\\n[`gcloud auth application-default login`](https://cloud.google.com/pubsub/docs/authentication#client-libs)\\ncommand for authentication in your local development environment, ADC are stored in the\\n`.config/gcloud/application_default_credentials.json` file in your home directory.","nullable":true},"emulator":{"type":"string","description":"Set in order to use a Pub/Sub [emulator](https://cloud.google.com/pubsub/docs/emulator)\\ninstead of the production service, e.g., \'localhost:8681\'.","nullable":true},"endpoint":{"type":"string","description":"Override the default service endpoint \'pubsub.googleapis.com\'","nullable":true},"pool_size":{"type":"integer","format":"int32","description":"gRPC channel pool size.","nullable":true,"minimum":0},"project_id":{"type":"string","description":"Google Cloud project_id.\\n\\nWhen not specified, the connector will use the project id associated\\nwith the authenticated account.","nullable":true},"snapshot":{"type":"string","description":"Reset subscription\'s backlog to a given snapshot on startup,\\nusing the Pub/Sub `Seek` API.\\n\\nThis option is mutually exclusive with the `timestamp` option.","nullable":true},"subscription":{"type":"string","description":"Subscription name."},"timeout_seconds":{"type":"integer","format":"int32","description":"gRPC request timeout.","nullable":true,"minimum":0},"timestamp":{"type":"string","description":"Reset subscription\'s backlog to a given timestamp on startup,\\nusing the Pub/Sub `Seek` API.\\n\\nThe value of this option is an ISO 8601-encoded UTC time, e.g., \\"2024-08-17T16:39:57-08:00\\".\\n\\nThis option is mutually exclusive with the `snapshot` option.","nullable":true}}},"name":{"type":"string","enum":["pub_sub_input"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","description":"Configuration for reading data from an HTTP or HTTPS URL with\\n`UrlInputTransport`.","required":["path"],"properties":{"path":{"type":"string","description":"URL."},"pause_timeout":{"type":"integer","format":"int32","description":"Timeout before disconnection when paused, in seconds.\\n\\nIf the pipeline is paused, or if the input adapter reads data faster\\nthan the pipeline can process it, then the controller will pause the\\ninput adapter. If the input adapter stays paused longer than this\\ntimeout, it will drop the network connection to the server. It will\\nautomatically reconnect when the input adapter starts running again.","minimum":0}}},"name":{"type":"string","enum":["url_input"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","description":"Configuration for reading data from AWS S3.","required":["region","bucket_name"],"properties":{"aws_access_key_id":{"type":"string","description":"AWS Access Key id. This property must be specified unless `no_sign_request` is set to `true`.","nullable":true},"aws_secret_access_key":{"type":"string","description":"Secret Access Key. This property must be specified unless `no_sign_request` is set to `true`.","nullable":true},"bucket_name":{"type":"string","description":"S3 bucket name to access."},"endpoint_url":{"type":"string","description":"The endpoint URL used to communicate with this service. Can be used to make this connector\\ntalk to non-AWS services with an S3 API.","nullable":true},"key":{"type":"string","description":"Read a single object specified by a key.","nullable":true},"max_concurrent_fetches":{"type":"integer","format":"int32","description":"Controls the number of S3 objects fetched in parallel.\\n\\nIncreasing this value can improve throughput by enabling greater concurrency.\\nHowever, higher concurrency may lead to timeouts or increased memory usage due to in-memory buffering.\\n\\nRecommended range: 1\u201310. Default: 8.","minimum":0},"no_sign_request":{"type":"boolean","description":"Do not sign requests. This is equivalent to the `--no-sign-request` flag in the AWS CLI."},"prefix":{"type":"string","description":"Read all objects whose keys match a prefix. Set to an empty string to read all objects in the bucket.","nullable":true},"region":{"type":"string","description":"AWS region."}}},"name":{"type":"string","enum":["s3_input"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","description":"Delta table input connector configuration.","required":["uri","mode"],"properties":{"cdc_delete_filter":{"type":"string","description":"A predicate that determines whether the record represents a deletion.\\n\\nThis setting is only valid in the `cdc` mode. It specifies a predicate applied to\\neach row in the Delta table to determine whether the row represents a deletion event.\\nIts value must be a valid Boolean SQL expression that can be used in a query of the\\nform `SELECT * from <table> WHERE <cdc_delete_filter>`.","nullable":true},"cdc_order_by":{"type":"string","description":"An expression that determines the ordering of updates in the Delta table.\\n\\nThis setting is only valid in the `cdc` mode. It specifies a predicate applied to\\neach row in the Delta table to determine the order in which updates in the table should\\nbe applied. Its value must be a valid SQL expression that can be used in a query of the\\nform `SELECT * from <table> ORDER BY <cdc_order_by>`.","nullable":true},"datetime":{"type":"string","description":"Optional timestamp for the snapshot in the ISO-8601/RFC-3339 format, e.g.,\\n\\"2024-12-09T16:09:53+00:00\\".\\n\\nWhen this option is set, the connector finds and opens the version of the table as of the\\nspecified point in time (based on the server time recorded in the transaction log, not the\\nevent time encoded in the data).  In `snapshot` and `snapshot_and_follow` modes, it\\nretrieves the snapshot of this version of the table.  In `follow`, `snapshot_and_follow`, and\\n`cdc` modes, it follows transaction log records **after** this version.\\n\\nNote: at most one of `version` and `datetime` options can be specified.\\nWhen neither of the two options is specified, the latest committed version of the table\\nis used.","nullable":true},"end_version":{"type":"integer","format":"int64","description":"Optional final table version.\\n\\nValid only when the connector is configured in `follow`, `snapshot_and_follow`, or `cdc` mode.\\n\\nWhen set, the connector will stop scanning the table\u2019s transaction log after reaching this version or any greater version.\\nThis bound is inclusive: if the specified version appears in the log, it will be processed before signaling end-of-input.","nullable":true},"filter":{"type":"string","description":"Optional row filter.\\n\\nWhen specified, only rows that satisfy the filter condition are read from the delta table.\\nThe condition must be a valid SQL Boolean expression that can be used in\\nthe `where` clause of the `select * from my_table where ...` query.","nullable":true},"max_concurrent_readers":{"type":"integer","format":"int32","description":"Maximum number of concurrent object store reads performed by all Delta Lake connectors.\\n\\nThis setting is used to limit the number of concurrent reads of the object store in a\\npipeline with a large number of Delta Lake connectors. When multiple connectors are simultaneously\\nreading from the object store, this can lead to transport timeouts.\\n\\nWhen enabled, this setting limits the number of concurrent reads across all connectors.\\nThis is a global setting that affects all Delta Lake connectors, and not just the connector\\nwhere it is specified. It should therefore be used at most once in a pipeline.  If multiple\\nconnectors specify this setting, they must all use the same value.\\n\\nThe default value is 6.","nullable":true,"minimum":0},"mode":{"type":"string","description":"Delta table read mode.\\n\\nThree options are available:\\n\\n* `snapshot` - read a snapshot of the table and stop.\\n\\n* `follow` - continuously ingest changes to the table, starting from a specified version\\nor timestamp.\\n\\n* `snapshot_and_follow` - read a snapshot of the table before switching to continuous ingestion\\nmode.","enum":["snapshot","follow","snapshot_and_follow","cdc"]},"num_parsers":{"type":"integer","format":"int32","description":"The number of parallel parsing tasks the connector uses to process data read from the\\ntable. Increasing this value can enhance performance by allowing more concurrent processing.\\nRecommended range: 1\u201310. The default is 4.","minimum":0},"skip_unused_columns":{"type":"boolean","description":"Don\'t read unused columns from the Delta table.\\n\\nWhen set to `true`, this option instructs the connector to avoid reading\\ncolumns from the Delta table that are not used in any view definitions.\\nTo be skipped, the columns must be either nullable or have default\\nvalues. This can improve ingestion performance, especially for wide\\ntables.\\n\\nNote: The simplest way to exclude unused columns is to omit them from the Feldera SQL table\\ndeclaration. The connector never reads columns that aren\'t declared in the SQL schema.\\nAdditionally, the SQL compiler emits warnings for declared but unused columns\u2014use these as\\na guide to optimize your schema."},"snapshot_filter":{"type":"string","description":"Optional snapshot filter.\\n\\nThis option is only valid when `mode` is set to `snapshot` or `snapshot_and_follow`.\\n\\nWhen specified, only rows that satisfy the filter condition are included in the\\nsnapshot.  The condition must be a valid SQL Boolean expression that can be used in\\nthe `where` clause of the `select * from snapshot where ...` query.\\n\\nUnlike the `filter` option, which applies to all records retrieved from the table, this\\nfilter only applies to rows in the initial snapshot of the table.\\nFor instance, it can be used to specify the range of event times to include in the snapshot,\\ne.g.: `ts BETWEEN TIMESTAMP \'2005-01-01 00:00:00\' AND TIMESTAMP \'2010-12-31 23:59:59\'`.\\n\\nThis option can be used together with the `filter` option. During the initial snapshot,\\nonly rows that satisfy both `filter` and `snapshot_filter` are retrieved from the Delta table.\\nWhen subsequently following changes in the the transaction log (`mode = snapshot_and_follow`),\\nall rows that meet the `filter` condition are ingested, regardless of `snapshot_filter`.","nullable":true},"timestamp_column":{"type":"string","description":"Table column that serves as an event timestamp.\\n\\nWhen this option is specified, and `mode` is one of `snapshot` or `snapshot_and_follow`,\\ntable rows are ingested in the timestamp order, respecting the\\n[`LATENESS`](https://docs.feldera.com/sql/streaming#lateness-expressions)\\nproperty of the column: each ingested row has a timestamp no more than `LATENESS`\\ntime units earlier than the most recent timestamp of any previously ingested row.\\nThe ingestion is performed by partitioning the table into timestamp ranges of width\\n`LATENESS`. Each range is processed sequentially, in increasing timestamp order.\\n\\n# Example\\n\\nConsider a table with timestamp column of type `TIMESTAMP` and lateness attribute\\n`INTERVAL 1 DAY`. Assuming that the oldest timestamp in the table is\\n`2024-01-01T00:00:00``, the connector will fetch all records with timestamps\\nfrom `2024-01-01`, then all records for `2024-01-02`, `2024-01-03`, etc., until all records\\nin the table have been ingested.\\n\\n# Requirements\\n\\n* The timestamp column must be of a supported type: integer, `DATE`, or `TIMESTAMP`.\\n* The timestamp column must be declared with non-zero `LATENESS`.\\n* For efficient ingest, the table must be optimized for timestamp-based\\nqueries using partitioning, Z-ordering, or liquid clustering.","nullable":true},"uri":{"type":"string","description":"Table URI.\\n\\nExample: \\"s3://feldera-fraud-detection-data/demographics_train\\""},"version":{"type":"integer","format":"int64","description":"Optional table version.\\n\\nWhen this option is set, the connector finds and opens the specified version of the table.\\nIn `snapshot` and `snapshot_and_follow` modes, it retrieves the snapshot of this version of\\nthe table.  In `follow`, `snapshot_and_follow`, and `cdc` modes, it follows transaction log records\\n**after** this version.\\n\\nNote: at most one of `version` and `datetime` options can be specified.\\nWhen neither of the two options is specified, the latest committed version of the table\\nis used.","nullable":true}},"additionalProperties":{"type":"string","description":"Storage options for configuring backend object store.\\n\\nFor specific options available for different storage backends, see:\\n* [Azure options](https://docs.rs/object_store/latest/object_store/azure/enum.AzureConfigKey.html)\\n* [Amazon S3 options](https://docs.rs/object_store/latest/object_store/aws/enum.AmazonS3ConfigKey.html)\\n* [Google Cloud Storage options](https://docs.rs/object_store/latest/object_store/gcp/enum.GoogleConfigKey.html)"}},"name":{"type":"string","enum":["delta_table_input"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","description":"Delta table output connector configuration.","required":["uri"],"properties":{"mode":{"type":"string","description":"Delta table write mode.\\n\\nDetermines how the Delta table connector handles an existing table at the target location.","enum":["append","truncate","error_if_exists"]},"uri":{"type":"string","description":"Table URI."}},"additionalProperties":{"type":"string","description":"Storage options for configuring backend object store.\\n\\nFor specific options available for different storage backends, see:\\n* [Azure options](https://docs.rs/object_store/latest/object_store/azure/enum.AzureConfigKey.html)\\n* [Amazon S3 options](https://docs.rs/object_store/latest/object_store/aws/enum.AmazonS3ConfigKey.html)\\n* [Google Cloud Storage options](https://docs.rs/object_store/latest/object_store/gcp/enum.GoogleConfigKey.html)"}},"name":{"type":"string","enum":["delta_table_output"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","description":"Redis output connector configuration.","required":["connection_string"],"properties":{"connection_string":{"type":"string","description":"The URL format: `redis://[<username>][:<password>@]<hostname>[:port][/[<db>][?protocol=<protocol>]]`\\nThis is parsed by the [redis](https://docs.rs/redis/latest/redis/#connection-parameters) crate."},"key_separator":{"type":"string","description":"Separator used to join multiple components into a single key.\\n\\":\\" by default."}}},"name":{"type":"string","enum":["redis_output"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"description":"Iceberg input connector configuration.","type":"object","properties":{"glue.access-key-id":{"type":"string","description":"Access key id used to access the Glue catalog.","nullable":true},"glue.endpoint":{"type":"string","description":"Configure an alternative endpoint of the Glue service for Glue catalog to access.\\n\\nExample: `\\"https://glue.us-east-1.amazonaws.com\\"`","nullable":true},"glue.id":{"type":"string","description":"The 12-digit ID of the Glue catalog.","nullable":true},"glue.profile-name":{"type":"string","description":"Profile used to access the Glue catalog.","nullable":true},"glue.region":{"type":"string","description":"Region of the Glue catalog.","nullable":true},"glue.secret-access-key":{"type":"string","description":"Secret access key used to access the Glue catalog.","nullable":true},"glue.session-token":{"type":"string","nullable":true},"glue.warehouse":{"type":"string","description":"Location for table metadata.\\n\\nExample: `\\"s3://my-data-warehouse/tables/\\"`","nullable":true},"rest.audience":{"type":"string","description":"Logical name of target resource or service.","nullable":true},"rest.credential":{"type":"string","description":"Credential to use for OAuth2 credential flow when initializing the catalog.\\n\\nA key and secret pair separated by \\":\\" (key is optional).","nullable":true},"rest.headers":{"type":"array","items":{"type":"array","items":{"type":"string"}},"description":"Additional HTTP request headers added to each catalog REST API call.","nullable":true},"rest.oauth2-server-uri":{"type":"string","description":"Authentication URL to use for client credentials authentication (default: uri + \'v1/oauth/tokens\')","nullable":true},"rest.prefix":{"type":"string","description":"Customize table storage paths.\\n\\nWhen combined with the `warehouse` property, the prefix determines\\nhow table data is organized within the storage.","nullable":true},"rest.resource":{"type":"string","description":"URI for the target resource or service.","nullable":true},"rest.scope":{"type":"string","nullable":true},"rest.token":{"type":"string","description":"Bearer token value to use for `Authorization` header.","nullable":true},"rest.uri":{"type":"string","description":"URI identifying the REST catalog server.","nullable":true},"rest.warehouse":{"type":"string","description":"The default location for managed tables created by the catalog.","nullable":true},"catalog_type":{"nullable":true,"type":"string","enum":["rest","glue"]},"datetime":{"type":"string","description":"Optional timestamp for the snapshot in the ISO-8601/RFC-3339 format, e.g.,\\n\\"2024-12-09T16:09:53+00:00\\".\\n\\nWhen this option is set, the connector finds and opens the snapshot of the table as of the\\nspecified point in time (based on the server time recorded in the transaction\\nlog, not the event time encoded in the data).  In `snapshot` and `snapshot_and_follow`\\nmodes, it retrieves this snapshot.  In `follow` and `snapshot_and_follow` modes, it\\nfollows transaction log records **after** this snapshot.\\n\\nNote: at most one of `snapshot_id` and `datetime` options can be specified.\\nWhen neither of the two options is specified, the latest committed version of the table\\nis used.","nullable":true},"metadata_location":{"type":"string","description":"Location of the table metadata JSON file.\\n\\nThis propery is used to access an Iceberg table without a catalog. It is mutually\\nexclusive with the `catalog_type` property.","nullable":true},"mode":{"type":"string","description":"Iceberg table read mode.\\n\\nThree options are available:\\n\\n* `snapshot` - read a snapshot of the table and stop.\\n\\n* `follow` - continuously ingest changes to the table, starting from a specified snapshot\\nor timestamp.\\n\\n* `snapshot_and_follow` - read a snapshot of the table before switching to continuous ingestion\\nmode.","enum":["snapshot","follow","snapshot_and_follow"]},"snapshot_filter":{"type":"string","description":"Optional row filter.\\n\\nThis option is only valid when `mode` is set to `snapshot` or `snapshot_and_follow`.\\n\\nWhen specified, only rows that satisfy the filter condition are included in the\\nsnapshot.  The condition must be a valid SQL Boolean expression that can be used in\\nthe `where` clause of the `select * from snapshot where ...` query.\\n\\nThis option can be used to specify the range of event times to include in the snapshot,\\ne.g.: `ts BETWEEN \'2005-01-01 00:00:00\' AND \'2010-12-31 23:59:59\'`.","nullable":true},"snapshot_id":{"type":"integer","format":"int64","description":"Optional snapshot id.\\n\\nWhen this option is set, the connector finds the specified snapshot of the table.\\nIn `snapshot` and `snapshot_and_follow` modes, it loads this snapshot.\\nIn `follow` and `snapshot_and_follow` modes, it follows table updates\\n**after** this snapshot.\\n\\nNote: at most one of `snapshot_id` and `datetime` options can be specified.\\nWhen neither of the two options is specified, the latest committed version of the table\\nis used.","nullable":true},"table_name":{"type":"string","description":"Specifies the Iceberg table name in the \\"namespace.table\\" format.\\n\\nThis option is applicable when an Iceberg catalog is configured using the `catalog_type` property.","nullable":true},"timestamp_column":{"type":"string","description":"Table column that serves as an event timestamp.\\n\\nWhen this option is specified, and `mode` is one of `snapshot` or `snapshot_and_follow`,\\ntable rows are ingested in the timestamp order, respecting the\\n[`LATENESS`](https://docs.feldera.com/sql/streaming#lateness-expressions)\\nproperty of the column: each ingested row has a timestamp no more than `LATENESS`\\ntime units earlier than the most recent timestamp of any previously ingested row.\\nThe ingestion is performed by partitioning the table into timestamp ranges of width\\n`LATENESS`. Each range is processed sequentially, in increasing timestamp order.\\n\\n# Example\\n\\nConsider a table with timestamp column of type `TIMESTAMP` and lateness attribute\\n`INTERVAL 1 DAY`. Assuming that the oldest timestamp in the table is\\n`2024-01-01T00:00:00``, the connector will fetch all records with timestamps\\nfrom `2024-01-01`, then all records for `2024-01-02`, `2024-01-03`, etc., until all records\\nin the table have been ingested.\\n\\n# Requirements\\n\\n* The timestamp column must be of a supported type: integer, `DATE`, or `TIMESTAMP`.\\n* The timestamp column must be declared with non-zero `LATENESS`.\\n* For efficient ingest, the table must be optimized for timestamp-based\\nqueries using partitioning, Z-ordering, or liquid clustering.","nullable":true}},"required":["mode"],"additionalProperties":{"type":"string","description":"Storage options for configuring backend object store.\\n\\nSee the [list of available options in PyIceberg documentation](https://py.iceberg.apache.org/configuration/#fileio)."}},"name":{"type":"string","enum":["iceberg_input"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","description":"Postgres input connector configuration.","required":["uri","query"],"properties":{"query":{"type":"string","description":"Query that specifies what data to fetch from postgres."},"uri":{"type":"string","description":"Postgres URI.\\nSee: <https://docs.rs/tokio-postgres/0.7.12/tokio_postgres/config/struct.Config.html>"}}},"name":{"type":"string","enum":["postgres_input"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","description":"Postgres output connector configuration.","required":["uri","table"],"properties":{"table":{"type":"string","description":"The table to write the output to."},"uri":{"type":"string","description":"Postgres URI.\\nSee: <https://docs.rs/tokio-postgres/0.7.12/tokio_postgres/config/struct.Config.html>"}}},"name":{"type":"string","enum":["postgres_output"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","description":"Configuration for generating random data for a table.","properties":{"plan":{"type":"array","items":{"type":"object","description":"A random generation plan for a table that generates either a limited amount of rows or runs continuously.","properties":{"fields":{"type":"object","description":"Specifies the values that the generator should produce.","default":{},"additionalProperties":{"type":"object","description":"Configuration for generating random data for a field of a table.","properties":{"e":{"type":"integer","format":"int64","description":"The frequency rank exponent for the Zipf distribution.\\n\\n- This value is only used if the strategy is set to `Zipf`.\\n- The default value is 1.0.","default":1},"fields":{"type":"object","description":"Specifies the values that the generator should produce in case the field is a struct type.","default":null,"additionalProperties":{"$ref":"#/components/schemas/RngFieldSettings"},"nullable":true},"key":{"default":null,"nullable":true,"$ref":"#/components/schemas/RngFieldSettings"},"null_percentage":{"type":"integer","description":"Percentage of records where this field should be set to NULL.\\n\\nIf not set, the generator will produce only records with non-NULL values.\\nIf set to `1..=100`, the generator will produce records with NULL values with the specified percentage.","default":null,"nullable":true,"minimum":0},"range":{"type":"object","description":"An optional, exclusive range [a, b) to limit the range of values the generator should produce.\\n\\n- For integer/floating point types specifies min/max values as an integer.\\nIf not set, the generator will produce values for the entire range of the type for number types.\\n- For string/binary types specifies min/max length as an integer, values are required to be >=0.\\nIf not set, a range of [0, 25) is used by default.\\n- For timestamp types specifies the min/max as two strings in the RFC 3339 format\\n(e.g., [\\"2021-01-01T00:00:00Z\\", \\"2022-01-02T00:00:00Z\\"]).\\nAlternatively, the range values can be specified as a number of non-leap\\nmilliseconds since January 1, 1970 0:00:00.000 UTC (aka \u201cUNIX timestamp\u201d).\\nIf not set, a range of [\\"1970-01-01T00:00:00Z\\", \\"2100-01-01T00:00:00Z\\") or [0, 4102444800000)\\nis used by default.\\n- For time types specifies the min/max as two strings in the \\"HH:MM:SS\\" format.\\nAlternatively, the range values can be specified in milliseconds as two positive integers.\\nIf not set, the range is 24h.\\n- For date types, the min/max range is specified as two strings in the \\"YYYY-MM-DD\\" format.\\nAlternatively, two integers that represent number of days since January 1, 1970 can be used.\\nIf not set, a range of [\\"1970-01-01\\", \\"2100-01-01\\") or [0, 54787) is used by default.\\n- For array types specifies the min/max number of elements as an integer.\\nIf not set, a range of [0, 5) is used by default. Range values are required to be >=0.\\n- For map types specifies the min/max number of key-value pairs as an integer.\\nIf not set, a range of [0, 5) is used by default.\\n- For struct/boolean/null types `range` is ignored."},"scale":{"type":"integer","format":"int64","description":"A scale factor to apply a multiplier to the generated value.\\n\\n- For integer/floating point types, the value is multiplied by the scale factor.\\n- For timestamp types, the generated value (milliseconds) is multiplied by the scale factor.\\n- For time types, the generated value (milliseconds) is multiplied by the scale factor.\\n- For date types, the generated value (days) is multiplied by the scale factor.\\n- For string/binary/array/map/struct/boolean/null types, the scale factor is ignored.\\n\\n- If `values` is specified, the scale factor is ignored.\\n- If `range` is specified and the range is required to be positive (struct, map, array etc.)\\nthe scale factor is required to be positive too.\\n\\nThe default scale factor is 1.","default":1},"strategy":{"default":"increment","type":"string","description":"Strategy used to generate values.","enum":["increment","uniform","zipf","word","words","sentence","sentences","paragraph","paragraphs","first_name","last_name","title","suffix","name","name_with_title","domain_suffix","email","username","password","field","position","seniority","job_title","ipv4","ipv6","ip","mac_address","user_agent","rfc_status_code","valid_status_code","company_suffix","company_name","buzzword","buzzword_middle","buzzword_tail","catch_phrase","bs_verb","bs_adj","bs_noun","bs","profession","industry","currency_code","currency_name","currency_symbol","credit_card_number","city_prefix","city_suffix","city_name","country_name","country_code","street_suffix","street_name","time_zone","state_name","state_abbr","secondary_address_type","secondary_address","zip_code","post_code","building_number","latitude","longitude","isbn","isbn13","isbn10","phone_number","cell_number","file_path","file_name","file_extension","dir_path"]},"value":{"default":null,"nullable":true,"$ref":"#/components/schemas/RngFieldSettings"},"values":{"type":"array","items":{"type":"object"},"description":"An optional set of values the generator will pick from.\\n\\nIf set, the generator will pick values from the specified set.\\nIf not set, the generator will produce values according to the specified range.\\nIf set to an empty set, the generator will produce NULL values.\\nIf set to a single value, the generator will produce only that value.\\n\\nNote that `range` is ignored if `values` is set.","default":null,"nullable":true}},"additionalProperties":false}},"limit":{"type":"integer","description":"Total number of new rows to generate.\\n\\nIf not set, the generator will produce new/unique records as long as the pipeline is running.\\nIf set to 0, the table will always remain empty.\\nIf set, the generator will produce new records until the specified limit is reached.\\n\\nNote that if the table has one or more primary keys that don\'t use the `increment` strategy to\\ngenerate the key there is a potential that an update is generated instead of an insert. In\\nthis case it\'s possible the total number of records is less than the specified limit.","default":null,"nullable":true,"minimum":0},"rate":{"type":"integer","format":"int32","description":"Non-zero number of rows to generate per second.\\n\\nIf not set, the generator will produce rows as fast as possible.","default":null,"nullable":true,"minimum":0},"worker_chunk_size":{"type":"integer","description":"When multiple workers are used, each worker will pick a consecutive \\"chunk\\" of\\nrecords to generate.\\n\\nBy default, if not specified, the generator will use the formula `min(rate, 10_000)`\\nto determine it. This works well in most situations. However, if you\'re\\nrunning tests with lateness and many workers you can e.g., reduce the\\nchunk size to make sure a smaller range of records is being ingested in parallel.\\n\\n# Example\\nAssume you generate a total of 125 records with 4 workers and a chunk size of 25.\\nIn this case, worker A will generate records 0..25, worker B will generate records 25..50,\\netc. A, B, C, and D will generate records in parallel. The first worker to finish its chunk\\nwill pick up the last chunk of records (100..125) to generate.","default":null,"nullable":true,"minimum":0}},"additionalProperties":false},"description":"The sequence of generations to perform.\\n\\nIf not set, the generator will produce a single sequence with default settings.\\nIf set, the generator will produce the specified sequences in sequential order.\\n\\nNote that if one of the sequences before the last one generates an unlimited number of rows\\nthe following sequences will not be executed.","default":[{"rate":null,"limit":null,"worker_chunk_size":null,"fields":{}}]},"seed":{"type":"integer","format":"int64","description":"Optional seed for the random generator.\\n\\nSetting this to a fixed value will make the generator produce the same sequence of records\\nevery time the pipeline is run.\\n\\n# Notes\\n- To ensure the set of generated input records is deterministic across multiple runs,\\napart from setting a seed, `workers` also needs to remain unchanged.\\n- The input will arrive in non-deterministic order if `workers > 1`.","default":null,"nullable":true,"minimum":0},"workers":{"type":"integer","description":"Number of workers to use for generating data.","default":1,"minimum":0}},"additionalProperties":false},"name":{"type":"string","enum":["datagen"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","description":"Configuration for generating Nexmark input data.\\n\\nThis connector must be used exactly three times in a pipeline if it is used\\nat all, once for each [`NexmarkTable`].","required":["table"],"properties":{"options":{"nullable":true,"type":"object","description":"Configuration for generating Nexmark input data.","properties":{"batch_size_per_thread":{"type":"integer","format":"int64","description":"Number of events to generate and submit together, per thread.\\n\\nEach thread generates this many records, which are then combined with\\nthe records generated by the other threads, to form combined input\\nbatches of size `threads \xd7 batch_size_per_thread`.","default":1000,"minimum":0},"events":{"type":"integer","format":"int64","description":"Number of events to generate.","default":100000000,"minimum":0},"max_step_size_per_thread":{"type":"integer","format":"int64","description":"Maximum number of events to submit in a single step, per thread.\\n\\nThis should really be per worker thread, not per generator thread, but\\nthe connector does not know how many worker threads there are.\\n\\nThis stands in for `max_batch_size` from the connector configuration\\nbecause it must be a constant across all three of the nexmark tables.","default":10000,"minimum":0},"threads":{"type":"integer","description":"Number of event generator threads.\\n\\nIt\'s reasonable to choose the same number of generator threads as worker\\nthreads.","default":4,"minimum":0}}},"table":{"type":"string","description":"Table in Nexmark.","enum":["bid","auction","person"]}}},"name":{"type":"string","enum":["nexmark"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","description":"Configuration for reading data via HTTP.\\n\\nHTTP input adapters cannot be usefully configured as part of pipeline\\nconfiguration.  Instead, instantiate them through the REST API as\\n`/pipelines/{pipeline_name}/ingress/{table_name}`.","required":["name"],"properties":{"name":{"type":"string","description":"Autogenerated name."}}},"name":{"type":"string","enum":["http_input"]}}},{"type":"object","required":["name"],"properties":{"name":{"type":"string","enum":["http_output"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","description":"Configuration for inserting data with ad-hoc queries\\n\\nAn ad-hoc input adapters cannot be usefully configured as part of pipeline\\nconfiguration.  Instead, use ad-hoc queries through the UI, the REST API, or\\nthe `fda` command-line tool.","required":["name"],"properties":{"name":{"type":"string","description":"Autogenerated name."}}},"name":{"type":"string","enum":["ad_hoc_input"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","required":["clock_resolution_usecs"],"properties":{"clock_resolution_usecs":{"type":"integer","format":"int64","minimum":0}}},"name":{"type":"string","enum":["clock_input"]}}}],"description":"Transport-specific endpoint configuration passed to\\n`crate::OutputTransport::new_endpoint`\\nand `crate::InputTransport::new_endpoint`.","discriminator":{"propertyName":"name"}}}}},"output_connectors":{"type":"object","description":"Output connectors derived from the schema.","additionalProperties":{"description":"Describes an output connector configuration","type":"object","required":["stream"],"properties":{"stream":{"type":"string","description":"The name of the output stream of the circuit that this endpoint is\\nconnected to."},"enable_output_buffer":{"type":"boolean","description":"Enable output buffering.\\n\\nThe output buffering mechanism allows decoupling the rate at which the pipeline\\npushes changes to the output transport from the rate of input changes.\\n\\nBy default, output updates produced by the pipeline are pushed directly to\\nthe output transport. Some destinations may prefer to receive updates in fewer\\nbigger batches. For instance, when writing Parquet files, producing\\none bigger file every few minutes is usually better than creating\\nsmall files every few milliseconds.\\n\\nTo achieve such input/output decoupling, users can enable output buffering by\\nsetting the `enable_output_buffer` flag to `true`.  When buffering is enabled, output\\nupdates produced by the pipeline are consolidated in an internal buffer and are\\npushed to the output transport when one of several conditions is satisfied:\\n\\n* data has been accumulated in the buffer for more than `max_output_buffer_time_millis`\\nmilliseconds.\\n* buffer size exceeds `max_output_buffer_size_records` records.\\n\\nThis flag is `false` by default.","default":false},"max_output_buffer_size_records":{"type":"integer","description":"Maximum number of updates to be kept in the output buffer.\\n\\nThis parameter bounds the maximal size of the buffer.\\nNote that the size of the buffer is not always equal to the\\ntotal number of updates output by the pipeline. Updates to the\\nsame record can overwrite or cancel previous updates.\\n\\nBy default, the buffer can grow indefinitely until one of\\nthe other output conditions is satisfied.\\n\\nNOTE: this configuration option requires the `enable_output_buffer` flag\\nto be set.","default":18446744073709552000,"minimum":0},"max_output_buffer_time_millis":{"type":"integer","description":"Maximum time in milliseconds data is kept in the output buffer.\\n\\nBy default, data is kept in the buffer indefinitely until one of\\nthe other output conditions is satisfied.  When this option is\\nset the buffer will be flushed at most every\\n`max_output_buffer_time_millis` milliseconds.\\n\\nNOTE: this configuration option requires the `enable_output_buffer` flag\\nto be set.","default":18446744073709552000,"minimum":0},"format":{"nullable":true,"type":"object","description":"Data format specification used to parse raw data received from the\\nendpoint or to encode data sent to the endpoint.","required":["name"],"properties":{"config":{"type":"object","description":"Format-specific parser or encoder configuration."},"name":{"type":"string","description":"Format name, e.g., \\"csv\\", \\"json\\", \\"bincode\\", etc."}}},"index":{"type":"string","description":"Name of the index that the connector is attached to.\\n\\nThis property is valid for output connectors only.  It is used with data\\ntransports and formats that expect output updates in the form of key/value\\npairs, where the key typically represents a unique id associated with the\\ntable or view.\\n\\nTo support such output formats, an output connector can be attached to an\\nindex created using the SQL CREATE INDEX statement.  An index of a table\\nor view contains the same updates as the table or view itself, indexed by\\none or more key columns.\\n\\nSee individual connector documentation for details on how they work\\nwith indexes.","nullable":true},"labels":{"type":"array","items":{"type":"string"},"description":"Arbitrary user-defined text labels associated with the connector.\\n\\nThese labels can be used in conjunction with the `start_after` property\\nto control the start order of connectors."},"max_batch_size":{"type":"integer","format":"int64","description":"Maximum batch size, in records.\\n\\nThis is the maximum number of records to process in one batch through\\nthe circuit.  The time and space cost of processing a batch is\\nasymptotically superlinear in the size of the batch, but very small\\nbatches are less efficient due to constant factors.\\n\\nThis should usually be less than `max_queued_records`, to give the\\nconnector a round-trip time to restart and refill the buffer while\\nbatches are being processed.\\n\\nSome input adapters might not honor this setting.\\n\\nThe default is 10,000.","minimum":0},"max_queued_records":{"type":"integer","format":"int64","description":"Backpressure threshold.\\n\\nMaximal number of records queued by the endpoint before the endpoint\\nis paused by the backpressure mechanism.\\n\\nFor input endpoints, this setting bounds the number of records that have\\nbeen received from the input transport but haven\'t yet been consumed by\\nthe circuit since the circuit, since the circuit is still busy processing\\nprevious inputs.\\n\\nFor output endpoints, this setting bounds the number of records that have\\nbeen produced by the circuit but not yet sent via the output transport endpoint\\nnor stored in the output buffer (see `enable_output_buffer`).\\n\\nNote that this is not a hard bound: there can be a small delay between\\nthe backpressure mechanism is triggered and the endpoint is paused, during\\nwhich more data may be queued.\\n\\nThe default is 1 million.","minimum":0},"paused":{"type":"boolean","description":"Create connector in paused state.\\n\\nThe default is `false`."},"start_after":{"type":"array","items":{"type":"string"},"description":"Start the connector after all connectors with specified labels.\\n\\nThis property is used to control the start order of connectors.\\nThe connector will not start until all connectors with the specified\\nlabels have finished processing all inputs.","nullable":true},"transport":{"oneOf":[{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","description":"Configuration for reading data from a file with `FileInputTransport`","required":["path"],"properties":{"buffer_size_bytes":{"type":"integer","description":"Read buffer size.\\n\\nDefault: when this parameter is not specified, a platform-specific\\ndefault is used.","nullable":true,"minimum":0},"follow":{"type":"boolean","description":"Enable file following.\\n\\nWhen `false`, the endpoint outputs an `InputConsumer::eoi`\\nmessage and stops upon reaching the end of file.  When `true`, the\\nendpoint will keep watching the file and outputting any new content\\nappended to it."},"path":{"type":"string","description":"File path."}}},"name":{"type":"string","enum":["file_input"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","description":"Configuration for writing data to a file with `FileOutputTransport`.","required":["path"],"properties":{"path":{"type":"string","description":"File path."}}},"name":{"type":"string","enum":["file_output"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","description":"Configuration for reading data from Kafka topics with `InputTransport`.","required":["topic"],"properties":{"group_join_timeout_secs":{"type":"integer","format":"int32","description":"Maximum timeout in seconds to wait for the endpoint to join the Kafka\\nconsumer group during initialization.","minimum":0},"log_level":{"nullable":true,"type":"string","description":"Kafka logging levels.","enum":["emerg","alert","critical","error","warning","notice","info","debug"]},"partitions":{"type":"array","items":{"type":"integer","format":"int32"},"description":"The list of Kafka partitions to read from.\\n\\nOnly the specified partitions will be consumed. If this field is not set,\\nthe connector will consume from all available partitions.\\n\\nIf `start_from` is set to `offsets` and this field is provided, the\\nnumber of partitions must exactly match the number of offsets, and the\\norder of partitions must correspond to the order of offsets.\\n\\nIf offsets are provided for all partitions, this field can be omitted.","nullable":true},"poller_threads":{"type":"integer","description":"Set to 1 or more to fix the number of threads used to poll\\n`rdkafka`. Multiple threads can increase performance with small Kafka\\nmessages; for large messages, one thread is enough. In either case, too\\nmany threads can harm performance. If unset, the default is 3, which\\nhelps with small messages but will not harm performance with large\\nmessagee","nullable":true,"minimum":0},"region":{"type":"string","description":"The AWS region to use while connecting to AWS Managed Streaming for Kafka (MSK).","nullable":true},"start_from":{"oneOf":[{"type":"string","description":"Start from the beginning of the topic.","enum":["earliest"]},{"type":"string","description":"Start from the current end of the topic.\\n\\nThis will only read any data that is added to the topic after the\\nconnector initializes.","enum":["latest"]},{"type":"object","required":["offsets"],"properties":{"offsets":{"type":"array","items":{"type":"integer","format":"int64"},"description":"Start from particular offsets in the topic.\\n\\nThe number of offsets must match the number of partitions in the topic."}}}],"description":"Where to begin reading a Kafka topic."},"topic":{"type":"string","description":"Topic to subscribe to."}},"additionalProperties":{"type":"string","description":"Options passed directly to `rdkafka`.\\n\\n[`librdkafka` options](https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md)\\nused to configure the Kafka consumer.\\n\\nThis input connector does not use consumer groups, so options related to\\nconsumer groups are rejected, including:\\n\\n* `group.id`, if present, is ignored.\\n* `auto.offset.reset` (use `start_from` instead).\\n* \\"enable.auto.commit\\", if present, must be set to \\"false\\".\\n* \\"enable.auto.offset.store\\", if present, must be set to \\"false\\"."}},"name":{"type":"string","enum":["kafka_input"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","description":"Configuration for writing data to a Kafka topic with `OutputTransport`.","required":["topic"],"properties":{"fault_tolerance":{"nullable":true,"type":"object","description":"Fault tolerance configuration for Kafka output connector.","properties":{"consumer_options":{"type":"object","description":"Options passed to `rdkafka` for consumers only, as documented at\\n[`librdkafka`\\noptions](https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md).\\n\\nThese options override `kafka_options` for consumers, and may be empty.","default":{},"additionalProperties":{"type":"string"}},"producer_options":{"type":"object","description":"Options passed to `rdkafka` for producers only, as documented at\\n[`librdkafka`\\noptions](https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md).\\n\\nThese options override `kafka_options` for producers, and may be empty.","default":{},"additionalProperties":{"type":"string"}}}},"headers":{"type":"array","items":{"type":"object","description":"Kafka message header.","required":["key"],"properties":{"key":{"type":"string"},"value":{"nullable":true,"type":"string","format":"binary","description":"Kafka header value encoded as a UTF-8 string or a byte array."}}},"description":"Kafka headers to be added to each message produced by this connector."},"initialization_timeout_secs":{"type":"integer","format":"int32","description":"Maximum timeout in seconds to wait for the endpoint to connect to\\na Kafka broker.\\n\\nDefaults to 60.","minimum":0},"kafka_service":{"type":"string","description":"If specified, this service is used to provide defaults for the Kafka options.","nullable":true},"log_level":{"nullable":true,"type":"string","description":"Kafka logging levels.","enum":["emerg","alert","critical","error","warning","notice","info","debug"]},"region":{"type":"string","description":"The AWS region to use while connecting to AWS Managed Streaming for Kafka (MSK).","nullable":true},"topic":{"type":"string","description":"Topic to write to."}},"additionalProperties":{"type":"string","description":"Options passed directly to `rdkafka`.\\n\\nSee [`librdkafka` options](https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md)\\nused to configure the Kafka producer."}},"name":{"type":"string","enum":["kafka_output"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","description":"Google Pub/Sub input connector configuration.","required":["subscription"],"properties":{"connect_timeout_seconds":{"type":"integer","format":"int32","description":"gRPC connection timeout.","nullable":true,"minimum":0},"credentials":{"type":"string","description":"The content of a Google Cloud credentials JSON file.\\n\\nWhen this option is specified, the connector will use the provided credentials for\\nauthentication.  Otherwise, it will use Application Default Credentials (ADC) configured\\nin the environment where the Feldera service is running.  See\\n[Google Cloud documentation](https://cloud.google.com/docs/authentication/provide-credentials-adc)\\nfor information on configuring application default credentials.\\n\\nWhen running Feldera in an environment where ADC are not configured,\\ne.g., a Docker container, use this option to ship Google Cloud credentials from another environment.\\nFor example, if you use the\\n[`gcloud auth application-default login`](https://cloud.google.com/pubsub/docs/authentication#client-libs)\\ncommand for authentication in your local development environment, ADC are stored in the\\n`.config/gcloud/application_default_credentials.json` file in your home directory.","nullable":true},"emulator":{"type":"string","description":"Set in order to use a Pub/Sub [emulator](https://cloud.google.com/pubsub/docs/emulator)\\ninstead of the production service, e.g., \'localhost:8681\'.","nullable":true},"endpoint":{"type":"string","description":"Override the default service endpoint \'pubsub.googleapis.com\'","nullable":true},"pool_size":{"type":"integer","format":"int32","description":"gRPC channel pool size.","nullable":true,"minimum":0},"project_id":{"type":"string","description":"Google Cloud project_id.\\n\\nWhen not specified, the connector will use the project id associated\\nwith the authenticated account.","nullable":true},"snapshot":{"type":"string","description":"Reset subscription\'s backlog to a given snapshot on startup,\\nusing the Pub/Sub `Seek` API.\\n\\nThis option is mutually exclusive with the `timestamp` option.","nullable":true},"subscription":{"type":"string","description":"Subscription name."},"timeout_seconds":{"type":"integer","format":"int32","description":"gRPC request timeout.","nullable":true,"minimum":0},"timestamp":{"type":"string","description":"Reset subscription\'s backlog to a given timestamp on startup,\\nusing the Pub/Sub `Seek` API.\\n\\nThe value of this option is an ISO 8601-encoded UTC time, e.g., \\"2024-08-17T16:39:57-08:00\\".\\n\\nThis option is mutually exclusive with the `snapshot` option.","nullable":true}}},"name":{"type":"string","enum":["pub_sub_input"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","description":"Configuration for reading data from an HTTP or HTTPS URL with\\n`UrlInputTransport`.","required":["path"],"properties":{"path":{"type":"string","description":"URL."},"pause_timeout":{"type":"integer","format":"int32","description":"Timeout before disconnection when paused, in seconds.\\n\\nIf the pipeline is paused, or if the input adapter reads data faster\\nthan the pipeline can process it, then the controller will pause the\\ninput adapter. If the input adapter stays paused longer than this\\ntimeout, it will drop the network connection to the server. It will\\nautomatically reconnect when the input adapter starts running again.","minimum":0}}},"name":{"type":"string","enum":["url_input"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","description":"Configuration for reading data from AWS S3.","required":["region","bucket_name"],"properties":{"aws_access_key_id":{"type":"string","description":"AWS Access Key id. This property must be specified unless `no_sign_request` is set to `true`.","nullable":true},"aws_secret_access_key":{"type":"string","description":"Secret Access Key. This property must be specified unless `no_sign_request` is set to `true`.","nullable":true},"bucket_name":{"type":"string","description":"S3 bucket name to access."},"endpoint_url":{"type":"string","description":"The endpoint URL used to communicate with this service. Can be used to make this connector\\ntalk to non-AWS services with an S3 API.","nullable":true},"key":{"type":"string","description":"Read a single object specified by a key.","nullable":true},"max_concurrent_fetches":{"type":"integer","format":"int32","description":"Controls the number of S3 objects fetched in parallel.\\n\\nIncreasing this value can improve throughput by enabling greater concurrency.\\nHowever, higher concurrency may lead to timeouts or increased memory usage due to in-memory buffering.\\n\\nRecommended range: 1\u201310. Default: 8.","minimum":0},"no_sign_request":{"type":"boolean","description":"Do not sign requests. This is equivalent to the `--no-sign-request` flag in the AWS CLI."},"prefix":{"type":"string","description":"Read all objects whose keys match a prefix. Set to an empty string to read all objects in the bucket.","nullable":true},"region":{"type":"string","description":"AWS region."}}},"name":{"type":"string","enum":["s3_input"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","description":"Delta table input connector configuration.","required":["uri","mode"],"properties":{"cdc_delete_filter":{"type":"string","description":"A predicate that determines whether the record represents a deletion.\\n\\nThis setting is only valid in the `cdc` mode. It specifies a predicate applied to\\neach row in the Delta table to determine whether the row represents a deletion event.\\nIts value must be a valid Boolean SQL expression that can be used in a query of the\\nform `SELECT * from <table> WHERE <cdc_delete_filter>`.","nullable":true},"cdc_order_by":{"type":"string","description":"An expression that determines the ordering of updates in the Delta table.\\n\\nThis setting is only valid in the `cdc` mode. It specifies a predicate applied to\\neach row in the Delta table to determine the order in which updates in the table should\\nbe applied. Its value must be a valid SQL expression that can be used in a query of the\\nform `SELECT * from <table> ORDER BY <cdc_order_by>`.","nullable":true},"datetime":{"type":"string","description":"Optional timestamp for the snapshot in the ISO-8601/RFC-3339 format, e.g.,\\n\\"2024-12-09T16:09:53+00:00\\".\\n\\nWhen this option is set, the connector finds and opens the version of the table as of the\\nspecified point in time (based on the server time recorded in the transaction log, not the\\nevent time encoded in the data).  In `snapshot` and `snapshot_and_follow` modes, it\\nretrieves the snapshot of this version of the table.  In `follow`, `snapshot_and_follow`, and\\n`cdc` modes, it follows transaction log records **after** this version.\\n\\nNote: at most one of `version` and `datetime` options can be specified.\\nWhen neither of the two options is specified, the latest committed version of the table\\nis used.","nullable":true},"end_version":{"type":"integer","format":"int64","description":"Optional final table version.\\n\\nValid only when the connector is configured in `follow`, `snapshot_and_follow`, or `cdc` mode.\\n\\nWhen set, the connector will stop scanning the table\u2019s transaction log after reaching this version or any greater version.\\nThis bound is inclusive: if the specified version appears in the log, it will be processed before signaling end-of-input.","nullable":true},"filter":{"type":"string","description":"Optional row filter.\\n\\nWhen specified, only rows that satisfy the filter condition are read from the delta table.\\nThe condition must be a valid SQL Boolean expression that can be used in\\nthe `where` clause of the `select * from my_table where ...` query.","nullable":true},"max_concurrent_readers":{"type":"integer","format":"int32","description":"Maximum number of concurrent object store reads performed by all Delta Lake connectors.\\n\\nThis setting is used to limit the number of concurrent reads of the object store in a\\npipeline with a large number of Delta Lake connectors. When multiple connectors are simultaneously\\nreading from the object store, this can lead to transport timeouts.\\n\\nWhen enabled, this setting limits the number of concurrent reads across all connectors.\\nThis is a global setting that affects all Delta Lake connectors, and not just the connector\\nwhere it is specified. It should therefore be used at most once in a pipeline.  If multiple\\nconnectors specify this setting, they must all use the same value.\\n\\nThe default value is 6.","nullable":true,"minimum":0},"mode":{"type":"string","description":"Delta table read mode.\\n\\nThree options are available:\\n\\n* `snapshot` - read a snapshot of the table and stop.\\n\\n* `follow` - continuously ingest changes to the table, starting from a specified version\\nor timestamp.\\n\\n* `snapshot_and_follow` - read a snapshot of the table before switching to continuous ingestion\\nmode.","enum":["snapshot","follow","snapshot_and_follow","cdc"]},"num_parsers":{"type":"integer","format":"int32","description":"The number of parallel parsing tasks the connector uses to process data read from the\\ntable. Increasing this value can enhance performance by allowing more concurrent processing.\\nRecommended range: 1\u201310. The default is 4.","minimum":0},"skip_unused_columns":{"type":"boolean","description":"Don\'t read unused columns from the Delta table.\\n\\nWhen set to `true`, this option instructs the connector to avoid reading\\ncolumns from the Delta table that are not used in any view definitions.\\nTo be skipped, the columns must be either nullable or have default\\nvalues. This can improve ingestion performance, especially for wide\\ntables.\\n\\nNote: The simplest way to exclude unused columns is to omit them from the Feldera SQL table\\ndeclaration. The connector never reads columns that aren\'t declared in the SQL schema.\\nAdditionally, the SQL compiler emits warnings for declared but unused columns\u2014use these as\\na guide to optimize your schema."},"snapshot_filter":{"type":"string","description":"Optional snapshot filter.\\n\\nThis option is only valid when `mode` is set to `snapshot` or `snapshot_and_follow`.\\n\\nWhen specified, only rows that satisfy the filter condition are included in the\\nsnapshot.  The condition must be a valid SQL Boolean expression that can be used in\\nthe `where` clause of the `select * from snapshot where ...` query.\\n\\nUnlike the `filter` option, which applies to all records retrieved from the table, this\\nfilter only applies to rows in the initial snapshot of the table.\\nFor instance, it can be used to specify the range of event times to include in the snapshot,\\ne.g.: `ts BETWEEN TIMESTAMP \'2005-01-01 00:00:00\' AND TIMESTAMP \'2010-12-31 23:59:59\'`.\\n\\nThis option can be used together with the `filter` option. During the initial snapshot,\\nonly rows that satisfy both `filter` and `snapshot_filter` are retrieved from the Delta table.\\nWhen subsequently following changes in the the transaction log (`mode = snapshot_and_follow`),\\nall rows that meet the `filter` condition are ingested, regardless of `snapshot_filter`.","nullable":true},"timestamp_column":{"type":"string","description":"Table column that serves as an event timestamp.\\n\\nWhen this option is specified, and `mode` is one of `snapshot` or `snapshot_and_follow`,\\ntable rows are ingested in the timestamp order, respecting the\\n[`LATENESS`](https://docs.feldera.com/sql/streaming#lateness-expressions)\\nproperty of the column: each ingested row has a timestamp no more than `LATENESS`\\ntime units earlier than the most recent timestamp of any previously ingested row.\\nThe ingestion is performed by partitioning the table into timestamp ranges of width\\n`LATENESS`. Each range is processed sequentially, in increasing timestamp order.\\n\\n# Example\\n\\nConsider a table with timestamp column of type `TIMESTAMP` and lateness attribute\\n`INTERVAL 1 DAY`. Assuming that the oldest timestamp in the table is\\n`2024-01-01T00:00:00``, the connector will fetch all records with timestamps\\nfrom `2024-01-01`, then all records for `2024-01-02`, `2024-01-03`, etc., until all records\\nin the table have been ingested.\\n\\n# Requirements\\n\\n* The timestamp column must be of a supported type: integer, `DATE`, or `TIMESTAMP`.\\n* The timestamp column must be declared with non-zero `LATENESS`.\\n* For efficient ingest, the table must be optimized for timestamp-based\\nqueries using partitioning, Z-ordering, or liquid clustering.","nullable":true},"uri":{"type":"string","description":"Table URI.\\n\\nExample: \\"s3://feldera-fraud-detection-data/demographics_train\\""},"version":{"type":"integer","format":"int64","description":"Optional table version.\\n\\nWhen this option is set, the connector finds and opens the specified version of the table.\\nIn `snapshot` and `snapshot_and_follow` modes, it retrieves the snapshot of this version of\\nthe table.  In `follow`, `snapshot_and_follow`, and `cdc` modes, it follows transaction log records\\n**after** this version.\\n\\nNote: at most one of `version` and `datetime` options can be specified.\\nWhen neither of the two options is specified, the latest committed version of the table\\nis used.","nullable":true}},"additionalProperties":{"type":"string","description":"Storage options for configuring backend object store.\\n\\nFor specific options available for different storage backends, see:\\n* [Azure options](https://docs.rs/object_store/latest/object_store/azure/enum.AzureConfigKey.html)\\n* [Amazon S3 options](https://docs.rs/object_store/latest/object_store/aws/enum.AmazonS3ConfigKey.html)\\n* [Google Cloud Storage options](https://docs.rs/object_store/latest/object_store/gcp/enum.GoogleConfigKey.html)"}},"name":{"type":"string","enum":["delta_table_input"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","description":"Delta table output connector configuration.","required":["uri"],"properties":{"mode":{"type":"string","description":"Delta table write mode.\\n\\nDetermines how the Delta table connector handles an existing table at the target location.","enum":["append","truncate","error_if_exists"]},"uri":{"type":"string","description":"Table URI."}},"additionalProperties":{"type":"string","description":"Storage options for configuring backend object store.\\n\\nFor specific options available for different storage backends, see:\\n* [Azure options](https://docs.rs/object_store/latest/object_store/azure/enum.AzureConfigKey.html)\\n* [Amazon S3 options](https://docs.rs/object_store/latest/object_store/aws/enum.AmazonS3ConfigKey.html)\\n* [Google Cloud Storage options](https://docs.rs/object_store/latest/object_store/gcp/enum.GoogleConfigKey.html)"}},"name":{"type":"string","enum":["delta_table_output"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","description":"Redis output connector configuration.","required":["connection_string"],"properties":{"connection_string":{"type":"string","description":"The URL format: `redis://[<username>][:<password>@]<hostname>[:port][/[<db>][?protocol=<protocol>]]`\\nThis is parsed by the [redis](https://docs.rs/redis/latest/redis/#connection-parameters) crate."},"key_separator":{"type":"string","description":"Separator used to join multiple components into a single key.\\n\\":\\" by default."}}},"name":{"type":"string","enum":["redis_output"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"description":"Iceberg input connector configuration.","type":"object","properties":{"glue.access-key-id":{"type":"string","description":"Access key id used to access the Glue catalog.","nullable":true},"glue.endpoint":{"type":"string","description":"Configure an alternative endpoint of the Glue service for Glue catalog to access.\\n\\nExample: `\\"https://glue.us-east-1.amazonaws.com\\"`","nullable":true},"glue.id":{"type":"string","description":"The 12-digit ID of the Glue catalog.","nullable":true},"glue.profile-name":{"type":"string","description":"Profile used to access the Glue catalog.","nullable":true},"glue.region":{"type":"string","description":"Region of the Glue catalog.","nullable":true},"glue.secret-access-key":{"type":"string","description":"Secret access key used to access the Glue catalog.","nullable":true},"glue.session-token":{"type":"string","nullable":true},"glue.warehouse":{"type":"string","description":"Location for table metadata.\\n\\nExample: `\\"s3://my-data-warehouse/tables/\\"`","nullable":true},"rest.audience":{"type":"string","description":"Logical name of target resource or service.","nullable":true},"rest.credential":{"type":"string","description":"Credential to use for OAuth2 credential flow when initializing the catalog.\\n\\nA key and secret pair separated by \\":\\" (key is optional).","nullable":true},"rest.headers":{"type":"array","items":{"type":"array","items":{"type":"string"}},"description":"Additional HTTP request headers added to each catalog REST API call.","nullable":true},"rest.oauth2-server-uri":{"type":"string","description":"Authentication URL to use for client credentials authentication (default: uri + \'v1/oauth/tokens\')","nullable":true},"rest.prefix":{"type":"string","description":"Customize table storage paths.\\n\\nWhen combined with the `warehouse` property, the prefix determines\\nhow table data is organized within the storage.","nullable":true},"rest.resource":{"type":"string","description":"URI for the target resource or service.","nullable":true},"rest.scope":{"type":"string","nullable":true},"rest.token":{"type":"string","description":"Bearer token value to use for `Authorization` header.","nullable":true},"rest.uri":{"type":"string","description":"URI identifying the REST catalog server.","nullable":true},"rest.warehouse":{"type":"string","description":"The default location for managed tables created by the catalog.","nullable":true},"catalog_type":{"nullable":true,"type":"string","enum":["rest","glue"]},"datetime":{"type":"string","description":"Optional timestamp for the snapshot in the ISO-8601/RFC-3339 format, e.g.,\\n\\"2024-12-09T16:09:53+00:00\\".\\n\\nWhen this option is set, the connector finds and opens the snapshot of the table as of the\\nspecified point in time (based on the server time recorded in the transaction\\nlog, not the event time encoded in the data).  In `snapshot` and `snapshot_and_follow`\\nmodes, it retrieves this snapshot.  In `follow` and `snapshot_and_follow` modes, it\\nfollows transaction log records **after** this snapshot.\\n\\nNote: at most one of `snapshot_id` and `datetime` options can be specified.\\nWhen neither of the two options is specified, the latest committed version of the table\\nis used.","nullable":true},"metadata_location":{"type":"string","description":"Location of the table metadata JSON file.\\n\\nThis propery is used to access an Iceberg table without a catalog. It is mutually\\nexclusive with the `catalog_type` property.","nullable":true},"mode":{"type":"string","description":"Iceberg table read mode.\\n\\nThree options are available:\\n\\n* `snapshot` - read a snapshot of the table and stop.\\n\\n* `follow` - continuously ingest changes to the table, starting from a specified snapshot\\nor timestamp.\\n\\n* `snapshot_and_follow` - read a snapshot of the table before switching to continuous ingestion\\nmode.","enum":["snapshot","follow","snapshot_and_follow"]},"snapshot_filter":{"type":"string","description":"Optional row filter.\\n\\nThis option is only valid when `mode` is set to `snapshot` or `snapshot_and_follow`.\\n\\nWhen specified, only rows that satisfy the filter condition are included in the\\nsnapshot.  The condition must be a valid SQL Boolean expression that can be used in\\nthe `where` clause of the `select * from snapshot where ...` query.\\n\\nThis option can be used to specify the range of event times to include in the snapshot,\\ne.g.: `ts BETWEEN \'2005-01-01 00:00:00\' AND \'2010-12-31 23:59:59\'`.","nullable":true},"snapshot_id":{"type":"integer","format":"int64","description":"Optional snapshot id.\\n\\nWhen this option is set, the connector finds the specified snapshot of the table.\\nIn `snapshot` and `snapshot_and_follow` modes, it loads this snapshot.\\nIn `follow` and `snapshot_and_follow` modes, it follows table updates\\n**after** this snapshot.\\n\\nNote: at most one of `snapshot_id` and `datetime` options can be specified.\\nWhen neither of the two options is specified, the latest committed version of the table\\nis used.","nullable":true},"table_name":{"type":"string","description":"Specifies the Iceberg table name in the \\"namespace.table\\" format.\\n\\nThis option is applicable when an Iceberg catalog is configured using the `catalog_type` property.","nullable":true},"timestamp_column":{"type":"string","description":"Table column that serves as an event timestamp.\\n\\nWhen this option is specified, and `mode` is one of `snapshot` or `snapshot_and_follow`,\\ntable rows are ingested in the timestamp order, respecting the\\n[`LATENESS`](https://docs.feldera.com/sql/streaming#lateness-expressions)\\nproperty of the column: each ingested row has a timestamp no more than `LATENESS`\\ntime units earlier than the most recent timestamp of any previously ingested row.\\nThe ingestion is performed by partitioning the table into timestamp ranges of width\\n`LATENESS`. Each range is processed sequentially, in increasing timestamp order.\\n\\n# Example\\n\\nConsider a table with timestamp column of type `TIMESTAMP` and lateness attribute\\n`INTERVAL 1 DAY`. Assuming that the oldest timestamp in the table is\\n`2024-01-01T00:00:00``, the connector will fetch all records with timestamps\\nfrom `2024-01-01`, then all records for `2024-01-02`, `2024-01-03`, etc., until all records\\nin the table have been ingested.\\n\\n# Requirements\\n\\n* The timestamp column must be of a supported type: integer, `DATE`, or `TIMESTAMP`.\\n* The timestamp column must be declared with non-zero `LATENESS`.\\n* For efficient ingest, the table must be optimized for timestamp-based\\nqueries using partitioning, Z-ordering, or liquid clustering.","nullable":true}},"required":["mode"],"additionalProperties":{"type":"string","description":"Storage options for configuring backend object store.\\n\\nSee the [list of available options in PyIceberg documentation](https://py.iceberg.apache.org/configuration/#fileio)."}},"name":{"type":"string","enum":["iceberg_input"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","description":"Postgres input connector configuration.","required":["uri","query"],"properties":{"query":{"type":"string","description":"Query that specifies what data to fetch from postgres."},"uri":{"type":"string","description":"Postgres URI.\\nSee: <https://docs.rs/tokio-postgres/0.7.12/tokio_postgres/config/struct.Config.html>"}}},"name":{"type":"string","enum":["postgres_input"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","description":"Postgres output connector configuration.","required":["uri","table"],"properties":{"table":{"type":"string","description":"The table to write the output to."},"uri":{"type":"string","description":"Postgres URI.\\nSee: <https://docs.rs/tokio-postgres/0.7.12/tokio_postgres/config/struct.Config.html>"}}},"name":{"type":"string","enum":["postgres_output"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","description":"Configuration for generating random data for a table.","properties":{"plan":{"type":"array","items":{"type":"object","description":"A random generation plan for a table that generates either a limited amount of rows or runs continuously.","properties":{"fields":{"type":"object","description":"Specifies the values that the generator should produce.","default":{},"additionalProperties":{"type":"object","description":"Configuration for generating random data for a field of a table.","properties":{"e":{"type":"integer","format":"int64","description":"The frequency rank exponent for the Zipf distribution.\\n\\n- This value is only used if the strategy is set to `Zipf`.\\n- The default value is 1.0.","default":1},"fields":{"type":"object","description":"Specifies the values that the generator should produce in case the field is a struct type.","default":null,"additionalProperties":{"$ref":"#/components/schemas/RngFieldSettings"},"nullable":true},"key":{"default":null,"nullable":true,"$ref":"#/components/schemas/RngFieldSettings"},"null_percentage":{"type":"integer","description":"Percentage of records where this field should be set to NULL.\\n\\nIf not set, the generator will produce only records with non-NULL values.\\nIf set to `1..=100`, the generator will produce records with NULL values with the specified percentage.","default":null,"nullable":true,"minimum":0},"range":{"type":"object","description":"An optional, exclusive range [a, b) to limit the range of values the generator should produce.\\n\\n- For integer/floating point types specifies min/max values as an integer.\\nIf not set, the generator will produce values for the entire range of the type for number types.\\n- For string/binary types specifies min/max length as an integer, values are required to be >=0.\\nIf not set, a range of [0, 25) is used by default.\\n- For timestamp types specifies the min/max as two strings in the RFC 3339 format\\n(e.g., [\\"2021-01-01T00:00:00Z\\", \\"2022-01-02T00:00:00Z\\"]).\\nAlternatively, the range values can be specified as a number of non-leap\\nmilliseconds since January 1, 1970 0:00:00.000 UTC (aka \u201cUNIX timestamp\u201d).\\nIf not set, a range of [\\"1970-01-01T00:00:00Z\\", \\"2100-01-01T00:00:00Z\\") or [0, 4102444800000)\\nis used by default.\\n- For time types specifies the min/max as two strings in the \\"HH:MM:SS\\" format.\\nAlternatively, the range values can be specified in milliseconds as two positive integers.\\nIf not set, the range is 24h.\\n- For date types, the min/max range is specified as two strings in the \\"YYYY-MM-DD\\" format.\\nAlternatively, two integers that represent number of days since January 1, 1970 can be used.\\nIf not set, a range of [\\"1970-01-01\\", \\"2100-01-01\\") or [0, 54787) is used by default.\\n- For array types specifies the min/max number of elements as an integer.\\nIf not set, a range of [0, 5) is used by default. Range values are required to be >=0.\\n- For map types specifies the min/max number of key-value pairs as an integer.\\nIf not set, a range of [0, 5) is used by default.\\n- For struct/boolean/null types `range` is ignored."},"scale":{"type":"integer","format":"int64","description":"A scale factor to apply a multiplier to the generated value.\\n\\n- For integer/floating point types, the value is multiplied by the scale factor.\\n- For timestamp types, the generated value (milliseconds) is multiplied by the scale factor.\\n- For time types, the generated value (milliseconds) is multiplied by the scale factor.\\n- For date types, the generated value (days) is multiplied by the scale factor.\\n- For string/binary/array/map/struct/boolean/null types, the scale factor is ignored.\\n\\n- If `values` is specified, the scale factor is ignored.\\n- If `range` is specified and the range is required to be positive (struct, map, array etc.)\\nthe scale factor is required to be positive too.\\n\\nThe default scale factor is 1.","default":1},"strategy":{"default":"increment","type":"string","description":"Strategy used to generate values.","enum":["increment","uniform","zipf","word","words","sentence","sentences","paragraph","paragraphs","first_name","last_name","title","suffix","name","name_with_title","domain_suffix","email","username","password","field","position","seniority","job_title","ipv4","ipv6","ip","mac_address","user_agent","rfc_status_code","valid_status_code","company_suffix","company_name","buzzword","buzzword_middle","buzzword_tail","catch_phrase","bs_verb","bs_adj","bs_noun","bs","profession","industry","currency_code","currency_name","currency_symbol","credit_card_number","city_prefix","city_suffix","city_name","country_name","country_code","street_suffix","street_name","time_zone","state_name","state_abbr","secondary_address_type","secondary_address","zip_code","post_code","building_number","latitude","longitude","isbn","isbn13","isbn10","phone_number","cell_number","file_path","file_name","file_extension","dir_path"]},"value":{"default":null,"nullable":true,"$ref":"#/components/schemas/RngFieldSettings"},"values":{"type":"array","items":{"type":"object"},"description":"An optional set of values the generator will pick from.\\n\\nIf set, the generator will pick values from the specified set.\\nIf not set, the generator will produce values according to the specified range.\\nIf set to an empty set, the generator will produce NULL values.\\nIf set to a single value, the generator will produce only that value.\\n\\nNote that `range` is ignored if `values` is set.","default":null,"nullable":true}},"additionalProperties":false}},"limit":{"type":"integer","description":"Total number of new rows to generate.\\n\\nIf not set, the generator will produce new/unique records as long as the pipeline is running.\\nIf set to 0, the table will always remain empty.\\nIf set, the generator will produce new records until the specified limit is reached.\\n\\nNote that if the table has one or more primary keys that don\'t use the `increment` strategy to\\ngenerate the key there is a potential that an update is generated instead of an insert. In\\nthis case it\'s possible the total number of records is less than the specified limit.","default":null,"nullable":true,"minimum":0},"rate":{"type":"integer","format":"int32","description":"Non-zero number of rows to generate per second.\\n\\nIf not set, the generator will produce rows as fast as possible.","default":null,"nullable":true,"minimum":0},"worker_chunk_size":{"type":"integer","description":"When multiple workers are used, each worker will pick a consecutive \\"chunk\\" of\\nrecords to generate.\\n\\nBy default, if not specified, the generator will use the formula `min(rate, 10_000)`\\nto determine it. This works well in most situations. However, if you\'re\\nrunning tests with lateness and many workers you can e.g., reduce the\\nchunk size to make sure a smaller range of records is being ingested in parallel.\\n\\n# Example\\nAssume you generate a total of 125 records with 4 workers and a chunk size of 25.\\nIn this case, worker A will generate records 0..25, worker B will generate records 25..50,\\netc. A, B, C, and D will generate records in parallel. The first worker to finish its chunk\\nwill pick up the last chunk of records (100..125) to generate.","default":null,"nullable":true,"minimum":0}},"additionalProperties":false},"description":"The sequence of generations to perform.\\n\\nIf not set, the generator will produce a single sequence with default settings.\\nIf set, the generator will produce the specified sequences in sequential order.\\n\\nNote that if one of the sequences before the last one generates an unlimited number of rows\\nthe following sequences will not be executed.","default":[{"rate":null,"limit":null,"worker_chunk_size":null,"fields":{}}]},"seed":{"type":"integer","format":"int64","description":"Optional seed for the random generator.\\n\\nSetting this to a fixed value will make the generator produce the same sequence of records\\nevery time the pipeline is run.\\n\\n# Notes\\n- To ensure the set of generated input records is deterministic across multiple runs,\\napart from setting a seed, `workers` also needs to remain unchanged.\\n- The input will arrive in non-deterministic order if `workers > 1`.","default":null,"nullable":true,"minimum":0},"workers":{"type":"integer","description":"Number of workers to use for generating data.","default":1,"minimum":0}},"additionalProperties":false},"name":{"type":"string","enum":["datagen"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","description":"Configuration for generating Nexmark input data.\\n\\nThis connector must be used exactly three times in a pipeline if it is used\\nat all, once for each [`NexmarkTable`].","required":["table"],"properties":{"options":{"nullable":true,"type":"object","description":"Configuration for generating Nexmark input data.","properties":{"batch_size_per_thread":{"type":"integer","format":"int64","description":"Number of events to generate and submit together, per thread.\\n\\nEach thread generates this many records, which are then combined with\\nthe records generated by the other threads, to form combined input\\nbatches of size `threads \xd7 batch_size_per_thread`.","default":1000,"minimum":0},"events":{"type":"integer","format":"int64","description":"Number of events to generate.","default":100000000,"minimum":0},"max_step_size_per_thread":{"type":"integer","format":"int64","description":"Maximum number of events to submit in a single step, per thread.\\n\\nThis should really be per worker thread, not per generator thread, but\\nthe connector does not know how many worker threads there are.\\n\\nThis stands in for `max_batch_size` from the connector configuration\\nbecause it must be a constant across all three of the nexmark tables.","default":10000,"minimum":0},"threads":{"type":"integer","description":"Number of event generator threads.\\n\\nIt\'s reasonable to choose the same number of generator threads as worker\\nthreads.","default":4,"minimum":0}}},"table":{"type":"string","description":"Table in Nexmark.","enum":["bid","auction","person"]}}},"name":{"type":"string","enum":["nexmark"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","description":"Configuration for reading data via HTTP.\\n\\nHTTP input adapters cannot be usefully configured as part of pipeline\\nconfiguration.  Instead, instantiate them through the REST API as\\n`/pipelines/{pipeline_name}/ingress/{table_name}`.","required":["name"],"properties":{"name":{"type":"string","description":"Autogenerated name."}}},"name":{"type":"string","enum":["http_input"]}}},{"type":"object","required":["name"],"properties":{"name":{"type":"string","enum":["http_output"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","description":"Configuration for inserting data with ad-hoc queries\\n\\nAn ad-hoc input adapters cannot be usefully configured as part of pipeline\\nconfiguration.  Instead, use ad-hoc queries through the UI, the REST API, or\\nthe `fda` command-line tool.","required":["name"],"properties":{"name":{"type":"string","description":"Autogenerated name."}}},"name":{"type":"string","enum":["ad_hoc_input"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","required":["clock_resolution_usecs"],"properties":{"clock_resolution_usecs":{"type":"integer","format":"int64","minimum":0}}},"name":{"type":"string","enum":["clock_input"]}}}],"description":"Transport-specific endpoint configuration passed to\\n`crate::OutputTransport::new_endpoint`\\nand `crate::InputTransport::new_endpoint`.","discriminator":{"propertyName":"name"}}}}},"schema":{"type":"object","description":"A struct containing the tables (inputs) and views for a program.\\n\\nParse from the JSON data-type of the DDL generated by the SQL compiler.","required":["inputs","outputs"],"properties":{"inputs":{"type":"array","items":{"description":"A SQL table or view. It has a name and a list of fields.\\n\\nMatches the Calcite JSON format.","type":"object","required":["name","case_sensitive","fields"],"properties":{"case_sensitive":{"type":"boolean"},"name":{"type":"string"},"fields":{"type":"array","items":{"description":"A SQL field.\\n\\nMatches the SQL compiler JSON format.","type":"object","required":["name","case_sensitive","columntype","unused"],"properties":{"case_sensitive":{"type":"boolean"},"name":{"type":"string"},"columntype":{"$ref":"#/components/schemas/ColumnType"},"default":{"type":"string","nullable":true},"lateness":{"type":"string","nullable":true},"unused":{"type":"boolean"},"watermark":{"type":"string","nullable":true}}}},"materialized":{"type":"boolean"},"properties":{"type":"object","additionalProperties":{"type":"object","required":["value","key_position","value_position"],"properties":{"key_position":{"type":"object","required":["start_line_number","start_column","end_line_number","end_column"],"properties":{"end_column":{"type":"integer","minimum":0},"end_line_number":{"type":"integer","minimum":0},"start_column":{"type":"integer","minimum":0},"start_line_number":{"type":"integer","minimum":0}}},"value":{"type":"string"},"value_position":{"type":"object","required":["start_line_number","start_column","end_line_number","end_column"],"properties":{"end_column":{"type":"integer","minimum":0},"end_line_number":{"type":"integer","minimum":0},"start_column":{"type":"integer","minimum":0},"start_line_number":{"type":"integer","minimum":0}}}}}}}}},"outputs":{"type":"array","items":{"description":"A SQL table or view. It has a name and a list of fields.\\n\\nMatches the Calcite JSON format.","type":"object","required":["name","case_sensitive","fields"],"properties":{"case_sensitive":{"type":"boolean"},"name":{"type":"string"},"fields":{"type":"array","items":{"description":"A SQL field.\\n\\nMatches the SQL compiler JSON format.","type":"object","required":["name","case_sensitive","columntype","unused"],"properties":{"case_sensitive":{"type":"boolean"},"name":{"type":"string"},"columntype":{"$ref":"#/components/schemas/ColumnType"},"default":{"type":"string","nullable":true},"lateness":{"type":"string","nullable":true},"unused":{"type":"boolean"},"watermark":{"type":"string","nullable":true}}}},"materialized":{"type":"boolean"},"properties":{"type":"object","additionalProperties":{"type":"object","required":["value","key_position","value_position"],"properties":{"key_position":{"type":"object","required":["start_line_number","start_column","end_line_number","end_column"],"properties":{"end_column":{"type":"integer","minimum":0},"end_line_number":{"type":"integer","minimum":0},"start_column":{"type":"integer","minimum":0},"start_line_number":{"type":"integer","minimum":0}}},"value":{"type":"string"},"value_position":{"type":"object","required":["start_line_number","start_column","end_line_number","end_column"],"properties":{"end_column":{"type":"integer","minimum":0},"end_line_number":{"type":"integer","minimum":0},"start_column":{"type":"integer","minimum":0},"start_line_number":{"type":"integer","minimum":0}}}}}}}}}}},"udf_stubs":{"type":"string","description":"Generated user defined function (UDF) stubs Rust code: stubs.rs"}}},"program_status":{"type":"string","description":"Program compilation status.","enum":["Pending","CompilingSql","SqlCompiled","CompilingRust","Success","SqlError","RustError","SystemError"]},"program_status_since":{"type":"string","format":"date-time"},"program_version":{"type":"integer","format":"int64","description":"Version number."},"refresh_version":{"type":"integer","format":"int64","description":"Version number."},"runtime_config":{"type":"object","description":"Global pipeline configuration settings. This is the publicly\\nexposed type for users to configure pipelines.","properties":{"checkpoint_during_suspend":{"type":"boolean","description":"Deprecated: setting this true or false does not have an effect anymore.","default":true},"clock_resolution_usecs":{"type":"integer","format":"int64","description":"Real-time clock resolution in microseconds.\\n\\nThis parameter controls the execution of queries that use the `NOW()` function.  The output of such\\nqueries depends on the real-time clock and can change over time without any external\\ninputs.  The pipeline will update the clock value and trigger incremental recomputation\\nat most each `clock_resolution_usecs` microseconds.\\n\\nIt is set to 1 second (1,000,000 microseconds) by default.\\n\\nSet to `null` to disable periodic clock updates.","default":1000000,"nullable":true,"minimum":0},"cpu_profiler":{"type":"boolean","description":"Enable CPU profiler.\\n\\nThe default value is `true`.","default":true},"dev_tweaks":{"type":"object","description":"Optional settings for tweaking Feldera internals.\\n\\nThe available key-value pairs change from one version of Feldera to\\nanother, so users should not depend on particular settings being\\navailable, or on their behavior.","default":{},"additionalProperties":{}},"fault_tolerance":{"default":{"model":"none","checkpoint_interval_secs":60},"type":"object","description":"Fault-tolerance configuration.\\n\\nThe default [FtConfig] (via [FtConfig::default]) disables fault tolerance,\\nwhich is the configuration that one gets if [RuntimeConfig] omits fault\\ntolerance configuration.\\n\\nThe default value for [FtConfig::model] enables fault tolerance, as\\n`Some(FtModel::default())`.  This is the configuration that one gets if\\n[RuntimeConfig] includes a fault tolerance configuration but does not\\nspecify a particular model.","properties":{"checkpoint_interval_secs":{"type":"integer","format":"int64","description":"Interval between automatic checkpoints, in seconds.\\n\\nThe default is 60 seconds.  Values less than 1 or greater than 3600 will\\nbe forced into that range.","nullable":true,"minimum":0},"model":{"oneOf":[{"type":"string","description":"Fault tolerance model.\\n\\nThe ordering is significant: we consider [Self::ExactlyOnce] to be a \\"higher\\nlevel\\" of fault tolerance than [Self::AtLeastOnce].","enum":["at_least_once","exactly_once"]},{"type":"string","enum":["none"]}],"default":"exactly_once"}}},"http_workers":{"type":"integer","format":"int64","description":"Sets the number of available runtime threads for the http server.\\n\\nIn most cases, this does not need to be set explicitly and\\nthe default is sufficient. Can be increased in case the\\npipeline HTTP API operations are a bottleneck.\\n\\nIf not specified, the default is set to `workers`.","default":null,"nullable":true,"minimum":0},"init_containers":{"description":"Specification of additional (sidecar) containers.","nullable":true},"io_workers":{"type":"integer","format":"int64","description":"Sets the number of available runtime threads for async IO tasks.\\n\\nThis affects some networking and file I/O operations\\nespecially adapters and ad-hoc queries.\\n\\nIn most cases, this does not need to be set explicitly and\\nthe default is sufficient. Can be increased in case\\ningress, egress or ad-hoc queries are a bottleneck.\\n\\nIf not specified, the default is set to `workers`.","default":null,"nullable":true,"minimum":0},"logging":{"type":"string","description":"Log filtering directives.\\n\\nIf set to a valid [tracing-subscriber] filter, this controls the log\\nmessages emitted by the pipeline process.  Otherwise, or if the filter\\nhas invalid syntax, messages at \\"info\\" severity and higher are written\\nto the log and all others are discarded.\\n\\n[tracing-subscriber]: https://docs.rs/tracing-subscriber/latest/tracing_subscriber/filter/struct.EnvFilter.html#directives","default":null,"nullable":true},"max_buffering_delay_usecs":{"type":"integer","format":"int64","description":"Maximal delay in microseconds to wait for `min_batch_size_records` to\\nget buffered by the controller, defaults to 0.","default":0,"minimum":0},"max_parallel_connector_init":{"type":"integer","format":"int64","description":"The maximum number of connectors initialized in parallel during pipeline\\nstartup.\\n\\nAt startup, the pipeline must initialize all of its input and output connectors.\\nDepending on the number and types of connectors, this can take a long time.\\nTo accelerate the process, multiple connectors are initialized concurrently.\\nThis option controls the maximum number of connectors that can be initialized\\nin parallel.\\n\\nThe default is 10.","default":null,"nullable":true,"minimum":0},"min_batch_size_records":{"type":"integer","format":"int64","description":"Minimal input batch size.\\n\\nThe controller delays pushing input records to the circuit until at\\nleast `min_batch_size_records` records have been received (total\\nacross all endpoints) or `max_buffering_delay_usecs` microseconds\\nhave passed since at least one input records has been buffered.\\nDefaults to 0.","default":0,"minimum":0},"pin_cpus":{"type":"array","items":{"type":"integer","minimum":0},"description":"Optionally, a list of CPU numbers for CPUs to which the pipeline may pin\\nits worker threads.  Specify at least twice as many CPU numbers as\\nworkers.  CPUs are generally numbered starting from 0.  The pipeline\\nmight not be able to honor CPU pinning requests.\\n\\nCPU pinning can make pipelines run faster and perform more consistently,\\nas long as different pipelines running on the same machine are pinned to\\ndifferent CPUs.","default":[]},"provisioning_timeout_secs":{"type":"integer","format":"int64","description":"Timeout in seconds for the `Provisioning` phase of the pipeline.\\nSetting this value will override the default of the runner.","default":null,"nullable":true,"minimum":0},"resources":{"default":{"cpu_cores_min":null,"cpu_cores_max":null,"memory_mb_min":null,"memory_mb_max":null,"storage_mb_max":null,"storage_class":null},"type":"object","properties":{"cpu_cores_max":{"type":"integer","format":"int64","description":"The maximum number of CPU cores to reserve\\nfor an instance of this pipeline","default":null,"nullable":true,"minimum":0},"cpu_cores_min":{"type":"integer","format":"int64","description":"The minimum number of CPU cores to reserve\\nfor an instance of this pipeline","default":null,"nullable":true,"minimum":0},"memory_mb_max":{"type":"integer","format":"int64","description":"The maximum memory in Megabytes to reserve\\nfor an instance of this pipeline","default":null,"nullable":true,"minimum":0},"memory_mb_min":{"type":"integer","format":"int64","description":"The minimum memory in Megabytes to reserve\\nfor an instance of this pipeline","default":null,"nullable":true,"minimum":0},"storage_class":{"type":"string","description":"Storage class to use for an instance of this pipeline.\\nThe class determines storage performance such as IOPS and throughput.","default":null,"nullable":true},"storage_mb_max":{"type":"integer","format":"int64","description":"The total storage in Megabytes to reserve\\nfor an instance of this pipeline","default":null,"nullable":true,"minimum":0}}},"storage":{"default":{"backend":{"name":"default"},"min_storage_bytes":null,"min_step_storage_bytes":null,"compression":"default","cache_mib":null},"nullable":true,"type":"object","description":"Storage configuration for a pipeline.","properties":{"backend":{"default":{"name":"default"},"oneOf":[{"type":"object","required":["name"],"properties":{"name":{"type":"string","enum":["default"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","description":"Configuration for local file system access.","properties":{"async_threads":{"type":"boolean","description":"Whether to use background threads for file I/O.\\n\\nBackground threads should improve performance, but they can reduce\\nperformance if too few cores are available. This is provided for\\ndebugging and fine-tuning and should ordinarily be left unset.","default":null,"nullable":true},"ioop_delay":{"type":"integer","format":"int64","description":"Per-I/O operation sleep duration, in milliseconds.\\n\\nThis is for simulating slow storage devices.  Do not use this in\\nproduction.","default":null,"nullable":true,"minimum":0},"sync":{"default":null,"nullable":true,"type":"object","required":["bucket","start_from_checkpoint"],"properties":{"access_key":{"type":"string","description":"The access key used to authenticate with the storage provider.\\n\\nIf not provided, rclone will fall back to environment-based credentials, such as\\n`RCLONE_S3_ACCESS_KEY_ID`. In Kubernetes environments using IRSA (IAM Roles for Service Accounts),\\nthis can be left empty to allow automatic authentication via the pod\'s service account.","nullable":true},"bucket":{"type":"string","description":"The name of the storage bucket.\\n\\nThis may include a path to a folder inside the bucket (e.g., `my-bucket/data`)."},"endpoint":{"type":"string","description":"The endpoint URL for the storage service.\\n\\nThis is typically required for custom or local S3-compatible storage providers like MinIO.\\nExample: `http://localhost:9000`\\n\\nRelevant rclone config key: [`endpoint`](https://rclone.org/s3/#s3-endpoint)","nullable":true},"provider":{"type":"string","description":"The name of the cloud storage provider (e.g., `\\"AWS\\"`, `\\"Minio\\"`).\\n\\nUsed for provider-specific behavior in rclone.\\nIf omitted, defaults to `\\"Other\\"`.\\n\\nSee [rclone S3 provider documentation](https://rclone.org/s3/#s3-provider)","nullable":true},"region":{"type":"string","description":"The region that this bucket is in.\\n\\nLeave empty for Minio or the default region (`us-east-1` for AWS).","nullable":true},"secret_key":{"type":"string","description":"The secret key used together with the access key for authentication.\\n\\nIf not provided, rclone will fall back to environment-based credentials, such as\\n`RCLONE_S3_SECRET_ACCESS_KEY`. In Kubernetes environments using IRSA (IAM Roles for Service Accounts),\\nthis can be left empty to allow automatic authentication via the pod\'s service account.","nullable":true},"start_from_checkpoint":{"type":"boolean","description":"If `true`, will try to pull the latest checkpoint from the configured\\nobject store and resume from that point."}}}}},"name":{"type":"string","enum":["file"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","required":["url"],"properties":{"url":{"type":"string","description":"URL.\\n\\nThe following URL schemes are supported:\\n\\n* S3:\\n- `s3://<bucket>/<path>`\\n- `s3a://<bucket>/<path>`\\n- `https://s3.<region>.amazonaws.com/<bucket>`\\n- `https://<bucket>.s3.<region>.amazonaws.com`\\n- `https://ACCOUNT_ID.r2.cloudflarestorage.com/bucket`\\n* Google Cloud Storage:\\n- `gs://<bucket>/<path>`\\n* Microsoft Azure Blob Storage:\\n- `abfs[s]://<container>/<path>` (according to [fsspec](https://github.com/fsspec/adlfs))\\n- `abfs[s]://<file_system>@<account_name>.dfs.core.windows.net/<path>`\\n- `abfs[s]://<file_system>@<account_name>.dfs.fabric.microsoft.com/<path>`\\n- `az://<container>/<path>` (according to [fsspec](https://github.com/fsspec/adlfs))\\n- `adl://<container>/<path>` (according to [fsspec](https://github.com/fsspec/adlfs))\\n- `azure://<container>/<path>` (custom)\\n- `https://<account>.dfs.core.windows.net`\\n- `https://<account>.blob.core.windows.net`\\n- `https://<account>.blob.core.windows.net/<container>`\\n- `https://<account>.dfs.fabric.microsoft.com`\\n- `https://<account>.dfs.fabric.microsoft.com/<container>`\\n- `https://<account>.blob.fabric.microsoft.com`\\n- `https://<account>.blob.fabric.microsoft.com/<container>`\\n\\nSettings derived from the URL will override other settings."}},"additionalProperties":{"type":"string","description":"Additional options as key-value pairs.\\n\\nThe following keys are supported:\\n\\n* S3:\\n- `access_key_id`: AWS Access Key.\\n- `secret_access_key`: AWS Secret Access Key.\\n- `region`: Region.\\n- `default_region`: Default region.\\n- `endpoint`: Custom endpoint for communicating with S3,\\ne.g. `https://localhost:4566` for testing against a localstack\\ninstance.\\n- `token`: Token to use for requests (passed to underlying provider).\\n- [Other keys](https://docs.rs/object_store/latest/object_store/aws/enum.AmazonS3ConfigKey.html#variants).\\n* Google Cloud Storage:\\n- `service_account`: Path to the service account file.\\n- `service_account_key`: The serialized service account key.\\n- `google_application_credentials`: Application credentials path.\\n- [Other keys](https://docs.rs/object_store/latest/object_store/gcp/enum.GoogleConfigKey.html).\\n* Microsoft Azure Blob Storage:\\n- `access_key`: Azure Access Key.\\n- `container_name`: Azure Container Name.\\n- `account`: Azure Account.\\n- `bearer_token_authorization`: Static bearer token for authorizing requests.\\n- `client_id`: Client ID for use in client secret or Kubernetes federated credential flow.\\n- `client_secret`: Client secret for use in client secret flow.\\n- `tenant_id`: Tenant ID for use in client secret or Kubernetes federated credential flow.\\n- `endpoint`: Override the endpoint for communicating with blob storage.\\n- [Other keys](https://docs.rs/object_store/latest/object_store/azure/enum.AzureConfigKey.html#variants).\\n\\nOptions set through the URL take precedence over those set with these\\noptions."}},"name":{"type":"string","enum":["object"]}}}],"description":"Backend storage configuration.","discriminator":{"propertyName":"name"}},"cache_mib":{"type":"integer","description":"The maximum size of the in-memory storage cache, in MiB.\\n\\nIf set, the specified cache size is spread across all the foreground and\\nbackground threads. If unset, each foreground or background thread cache\\nis limited to 256 MiB.","default":null,"nullable":true,"minimum":0},"compression":{"default":"default","type":"string","description":"Storage compression algorithm.","enum":["default","none","snappy"]},"min_step_storage_bytes":{"type":"integer","description":"For a batch of data passed through the pipeline during a single step,\\nthe minimum estimated number of bytes to write it to storage.\\n\\nThis is provided for debugging and fine-tuning and should ordinarily be\\nleft unset.  A value of 0 will write even empty batches to storage, and\\nnonzero values provide a threshold.  `usize::MAX`, the default,\\neffectively disables storage for such batches.  If it is set to another\\nvalue, it should ordinarily be greater than or equal to\\n`min_storage_bytes`.","default":null,"nullable":true,"minimum":0},"min_storage_bytes":{"type":"integer","description":"For a batch of data maintained as part of a persistent index during a\\npipeline run, the minimum estimated number of bytes to write it to\\nstorage.\\n\\nThis is provided for debugging and fine-tuning and should ordinarily be\\nleft unset.\\n\\nA value of 0 will write even empty batches to storage, and nonzero\\nvalues provide a threshold.  `usize::MAX` would effectively disable\\nstorage for such batches.  The default is 1,048,576 (1 MiB).","default":null,"nullable":true,"minimum":0}}},"tracing":{"type":"boolean","description":"Enable pipeline tracing.","default":false},"tracing_endpoint_jaeger":{"type":"string","description":"Jaeger tracing endpoint to send tracing information to.","default":"127.0.0.1:6831"},"workers":{"type":"integer","format":"int32","description":"Number of DBSP worker threads.\\n\\nEach DBSP \\"foreground\\" worker thread is paired with a \\"background\\"\\nthread for LSM merging, making the total number of threads twice the\\nspecified number.\\n\\nThe typical sweet spot for the number of workers is between 4 and 16.\\nEach worker increases overall memory consumption for data structures\\nused during a step.","default":8,"minimum":0}}},"storage_status":{"type":"string","description":"Storage status.\\n\\nThe storage status can only transition when the pipeline status is `Stopped`.\\n\\n```text\\nCleared \u2500\u2500\u2500\u2510\\n\u25b2       \u2502\\n/clear \u2502       \u2502\\n\u2502       \u2502\\nClearing   \u2502\\n\u25b2       \u2502\\n\u2502       \u2502\\nInUse \u25c4\u2500\u2500\u2500\u2518\\n```","enum":["Cleared","InUse","Clearing"]},"udf_rust":{"type":"string"},"udf_toml":{"type":"string"},"version":{"type":"integer","format":"int64","description":"Version number."}}},"example":{"id":"67e55044-10b1-426f-9247-bb680e5fe0c8","name":"example1","description":"Description of the pipeline example1","created_at":"1970-01-01T00:00:00Z","version":4,"platform_version":"v0","runtime_config":{"workers":16,"storage":{"backend":{"name":"default"},"min_storage_bytes":null,"min_step_storage_bytes":null,"compression":"default","cache_mib":null},"fault_tolerance":{"model":"none","checkpoint_interval_secs":60},"cpu_profiler":true,"tracing":false,"tracing_endpoint_jaeger":"","min_batch_size_records":0,"max_buffering_delay_usecs":0,"resources":{"cpu_cores_min":null,"cpu_cores_max":null,"memory_mb_min":null,"memory_mb_max":null,"storage_mb_max":null,"storage_class":null},"clock_resolution_usecs":1000000,"pin_cpus":[],"provisioning_timeout_secs":null,"max_parallel_connector_init":null,"init_containers":null,"checkpoint_during_suspend":true,"http_workers":null,"io_workers":null,"dev_tweaks":{},"logging":null},"program_code":"CREATE TABLE table1 ( col1 INT );","udf_rust":"","udf_toml":"","program_config":{"profile":"optimized","cache":true,"runtime_version":null},"program_version":2,"program_status":"Pending","program_status_since":"1970-01-01T00:00:00Z","program_error":{"sql_compilation":null,"rust_compilation":null,"system_error":null},"program_info":null,"deployment_status":"Stopped","deployment_status_since":"1970-01-01T00:00:00Z","deployment_desired_status":"Stopped","deployment_error":null,"refresh_version":4,"storage_status":"Cleared"}}}},"201":{"description":"Pipeline successfully created","content":{"application/json":{"schema":{"type":"object","description":"Pipeline information.\\nIt both includes fields which are user-provided and system-generated.","required":["id","name","description","created_at","version","platform_version","runtime_config","program_code","udf_rust","udf_toml","program_config","program_version","program_status","program_status_since","program_error","deployment_status","deployment_status_since","deployment_desired_status","refresh_version","storage_status"],"properties":{"created_at":{"type":"string","format":"date-time"},"deployment_desired_status":{"type":"string","enum":["Stopped","Paused","Running","Suspended"]},"deployment_error":{"nullable":true,"type":"object","description":"Information returned by REST API endpoints on error.","required":["message","error_code","details"],"properties":{"details":{"type":"object","description":"Detailed error metadata.\\nThe contents of this field is determined by `error_code`."},"error_code":{"type":"string","description":"Error code is a string that specifies this error type.","example":"CodeSpecifyingErrorType"},"message":{"type":"string","description":"Human-readable error message.","example":"Explanation of the error that occurred."}}},"deployment_status":{"type":"string","description":"Pipeline status.\\n\\nThis type represents the state of the pipeline tracked by the pipeline\\nrunner and observed by the API client via the `GET /v0/pipelines/{name}` endpoint.\\n\\n### The lifecycle of a pipeline\\n\\nThe following automaton captures the lifecycle of the pipeline.\\nIndividual states and transitions of the automaton are described below.\\n\\n* States labeled with the hourglass symbol (\u231b) are **timed** states. The\\nautomaton stays in timed state until the corresponding operation completes\\nor until it transitions to become failed after the pre-defined timeout\\nperiod expires.\\n\\n* State transitions labeled with API endpoint names (`/start`, `/pause`,\\n`/stop`) are triggered by invoking corresponding endpoint,\\ne.g., `POST /v0/pipelines/{name}/start`. Note that these only express\\ndesired state, and are applied asynchronously by the automata.\\n\\n```text\\nStopped \u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Stopping \u25c4\u2500\u2500\u2500\u2500\u2500 All states can transition\\n\u2502                    \u25b2            to Stopping by either:\\n/start or /pause \u2502                    \u2502            (1) user calling /stop?force=true, or;\\n\u25bc                    \u2502            (2) pipeline encountering a fatal\\n\u231bProvisioning          Suspending            resource or runtime error,\\n\u2502                    \u25b2                having the system call /stop?force=true\\n\u25bc                    \u2502 /stop          effectively\\n\u231bInitializing \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524  ?force=false\\n\u2502                    \u2502\\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2510\\n\u2502         \u25bc                          \u2502\\n\u2502       Paused  \u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u25ba Unavailable \u2502\\n\u2502        \u2502   \u25b2                \u25b2      \u2502\\n\u2502 /start \u2502   \u2502  /pause        \u2502      \u2502\\n\u2502        \u25bc   \u2502                \u2502      \u2502\\n\u2502       Running \u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2502\\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\\n```\\n\\n### Desired and actual status\\n\\nWe use the desired state model to manage the lifecycle of a pipeline.\\nIn this model, the pipeline has two status attributes associated with\\nit at runtime: the **desired** status, which represents what the user\\nwould like the pipeline to do, and the **current** status, which\\nrepresents the actual state of the pipeline.  The pipeline runner\\nservice continuously monitors both fields and steers the pipeline\\ntowards the desired state specified by the user.\\n\\nOnly four of the states in the pipeline automaton above can be\\nused as desired statuses: `Paused`, `Running`, `Suspended` and\\n`Stopped`. These statuses are selected by invoking REST endpoints\\nshown in the diagram (respectively, `/pause`, `/start`, and `/stop`).\\n\\nThe user can monitor the current state of the pipeline via the\\n`GET /v0/pipelines/{name}` endpoint. In a typical scenario,\\nthe user first sets the desired state, e.g., by invoking the\\n`/start` endpoint, and then polls the `GET /v0/pipelines/{name}`\\nendpoint to monitor the actual status of the pipeline until its\\n`deployment_status` attribute changes to `Running` indicating\\nthat the pipeline has been successfully initialized and is\\nprocessing data, or `Stopped` with `deployment_error` being set.","enum":["Stopped","Provisioning","Initializing","Paused","Running","Unavailable","Suspending","Stopping"]},"deployment_status_since":{"type":"string","format":"date-time"},"description":{"type":"string"},"id":{"type":"string","format":"uuid","description":"Pipeline identifier."},"name":{"type":"string"},"platform_version":{"type":"string"},"program_code":{"type":"string"},"program_config":{"type":"object","description":"Program configuration.","properties":{"cache":{"type":"boolean","description":"If `true` (default), when a prior compilation with the same checksum\\nalready exists, the output of that (i.e., binary) is used.\\nSet `false` to always trigger a new compilation, which might take longer\\nand as well can result in overriding an existing binary.","default":true},"profile":{"default":null,"nullable":true,"type":"string","description":"Enumeration of possible compilation profiles that can be passed to the Rust compiler\\nas an argument via `cargo build --profile <>`. A compilation profile affects among\\nother things the compilation speed (how long till the program is ready to be run)\\nand runtime speed (the performance while running).","enum":["dev","unoptimized","optimized"]},"runtime_version":{"type":"string","description":"Override runtime version of the pipeline being executed.\\n\\nWarning: This option is experimental and may change in the future.\\nShould only be used for CI/testing purposes, and requires network access.\\n\\nA runtime version can be specified in the form of a version\\nor SHA taken from the `feldera/feldera` repository main branch.\\n\\nExamples: `v0.96.0` or `f4dcac0989ca0fda7d2eb93602a49d007cb3b0ae`\\n\\nA platform of version `0.x.y` may be capable of running future and past\\nruntimes with versions `>=0.x.y` and `<=0.x.y` until breaking API changes happen,\\nthe exact bounds for each platform version are unspecified until we reach a\\nstable version. Compatibility is only guaranteed if platform and runtime version\\nare exact matches.\\n\\nNote that any enterprise features are currently considered to be part of\\nthe platform.\\n\\nIf not set (null), the runtime version will be the same as the platform version.","default":null,"nullable":true}}},"program_error":{"type":"object","description":"Log, warning and error information about the program compilation.","properties":{"rust_compilation":{"nullable":true,"type":"object","description":"Rust compilation information.","required":["exit_code","stdout","stderr"],"properties":{"exit_code":{"type":"integer","format":"int32","description":"Exit code of the `cargo` compilation command."},"stderr":{"type":"string","description":"Output printed to stderr by the `cargo` compilation command."},"stdout":{"type":"string","description":"Output printed to stdout by the `cargo` compilation command."}}},"sql_compilation":{"nullable":true,"type":"object","description":"SQL compilation information.","required":["exit_code","messages"],"properties":{"exit_code":{"type":"integer","format":"int32","description":"Exit code of the SQL compiler."},"messages":{"type":"array","items":{"type":"object","description":"A SQL compiler error.\\n\\nThe SQL compiler returns a list of errors in the following JSON format if\\nit\'s invoked with the `-je` option.\\n\\n```ignore\\n[ {\\n\\"start_line_number\\" : 2,\\n\\"start_column\\" : 4,\\n\\"end_line_number\\" : 2,\\n\\"end_column\\" : 8,\\n\\"warning\\" : false,\\n\\"error_type\\" : \\"PRIMARY KEY cannot be nullable\\",\\n\\"message\\" : \\"PRIMARY KEY column \'C\' has type INTEGER, which is nullable\\",\\n\\"snippet\\" : \\"    2|   c INT PRIMARY KEY\\\\n         ^^^^^\\\\n    3|);\\\\n\\"\\n} ]\\n```","required":["start_line_number","start_column","end_line_number","end_column","warning","error_type","message"],"properties":{"end_column":{"type":"integer","minimum":0},"end_line_number":{"type":"integer","minimum":0},"error_type":{"type":"string"},"message":{"type":"string"},"snippet":{"type":"string","nullable":true},"start_column":{"type":"integer","minimum":0},"start_line_number":{"type":"integer","minimum":0},"warning":{"type":"boolean"}}},"description":"Messages (warnings and errors) generated by the SQL compiler."}}},"system_error":{"type":"string","description":"System error that occurred.\\n- Set `Some(...)` upon transition to `SystemError`\\n- Set `None` upon transition to `Pending`","nullable":true}}},"program_info":{"nullable":true,"type":"object","description":"Program information is the result of the SQL compilation.","required":["schema","udf_stubs","input_connectors","output_connectors"],"properties":{"input_connectors":{"type":"object","description":"Input connectors derived from the schema.","additionalProperties":{"description":"Describes an input connector configuration","type":"object","required":["stream"],"properties":{"stream":{"type":"string","description":"The name of the input stream of the circuit that this endpoint is\\nconnected to."},"enable_output_buffer":{"type":"boolean","description":"Enable output buffering.\\n\\nThe output buffering mechanism allows decoupling the rate at which the pipeline\\npushes changes to the output transport from the rate of input changes.\\n\\nBy default, output updates produced by the pipeline are pushed directly to\\nthe output transport. Some destinations may prefer to receive updates in fewer\\nbigger batches. For instance, when writing Parquet files, producing\\none bigger file every few minutes is usually better than creating\\nsmall files every few milliseconds.\\n\\nTo achieve such input/output decoupling, users can enable output buffering by\\nsetting the `enable_output_buffer` flag to `true`.  When buffering is enabled, output\\nupdates produced by the pipeline are consolidated in an internal buffer and are\\npushed to the output transport when one of several conditions is satisfied:\\n\\n* data has been accumulated in the buffer for more than `max_output_buffer_time_millis`\\nmilliseconds.\\n* buffer size exceeds `max_output_buffer_size_records` records.\\n\\nThis flag is `false` by default.","default":false},"max_output_buffer_size_records":{"type":"integer","description":"Maximum number of updates to be kept in the output buffer.\\n\\nThis parameter bounds the maximal size of the buffer.\\nNote that the size of the buffer is not always equal to the\\ntotal number of updates output by the pipeline. Updates to the\\nsame record can overwrite or cancel previous updates.\\n\\nBy default, the buffer can grow indefinitely until one of\\nthe other output conditions is satisfied.\\n\\nNOTE: this configuration option requires the `enable_output_buffer` flag\\nto be set.","default":18446744073709552000,"minimum":0},"max_output_buffer_time_millis":{"type":"integer","description":"Maximum time in milliseconds data is kept in the output buffer.\\n\\nBy default, data is kept in the buffer indefinitely until one of\\nthe other output conditions is satisfied.  When this option is\\nset the buffer will be flushed at most every\\n`max_output_buffer_time_millis` milliseconds.\\n\\nNOTE: this configuration option requires the `enable_output_buffer` flag\\nto be set.","default":18446744073709552000,"minimum":0},"format":{"nullable":true,"type":"object","description":"Data format specification used to parse raw data received from the\\nendpoint or to encode data sent to the endpoint.","required":["name"],"properties":{"config":{"type":"object","description":"Format-specific parser or encoder configuration."},"name":{"type":"string","description":"Format name, e.g., \\"csv\\", \\"json\\", \\"bincode\\", etc."}}},"index":{"type":"string","description":"Name of the index that the connector is attached to.\\n\\nThis property is valid for output connectors only.  It is used with data\\ntransports and formats that expect output updates in the form of key/value\\npairs, where the key typically represents a unique id associated with the\\ntable or view.\\n\\nTo support such output formats, an output connector can be attached to an\\nindex created using the SQL CREATE INDEX statement.  An index of a table\\nor view contains the same updates as the table or view itself, indexed by\\none or more key columns.\\n\\nSee individual connector documentation for details on how they work\\nwith indexes.","nullable":true},"labels":{"type":"array","items":{"type":"string"},"description":"Arbitrary user-defined text labels associated with the connector.\\n\\nThese labels can be used in conjunction with the `start_after` property\\nto control the start order of connectors."},"max_batch_size":{"type":"integer","format":"int64","description":"Maximum batch size, in records.\\n\\nThis is the maximum number of records to process in one batch through\\nthe circuit.  The time and space cost of processing a batch is\\nasymptotically superlinear in the size of the batch, but very small\\nbatches are less efficient due to constant factors.\\n\\nThis should usually be less than `max_queued_records`, to give the\\nconnector a round-trip time to restart and refill the buffer while\\nbatches are being processed.\\n\\nSome input adapters might not honor this setting.\\n\\nThe default is 10,000.","minimum":0},"max_queued_records":{"type":"integer","format":"int64","description":"Backpressure threshold.\\n\\nMaximal number of records queued by the endpoint before the endpoint\\nis paused by the backpressure mechanism.\\n\\nFor input endpoints, this setting bounds the number of records that have\\nbeen received from the input transport but haven\'t yet been consumed by\\nthe circuit since the circuit, since the circuit is still busy processing\\nprevious inputs.\\n\\nFor output endpoints, this setting bounds the number of records that have\\nbeen produced by the circuit but not yet sent via the output transport endpoint\\nnor stored in the output buffer (see `enable_output_buffer`).\\n\\nNote that this is not a hard bound: there can be a small delay between\\nthe backpressure mechanism is triggered and the endpoint is paused, during\\nwhich more data may be queued.\\n\\nThe default is 1 million.","minimum":0},"paused":{"type":"boolean","description":"Create connector in paused state.\\n\\nThe default is `false`."},"start_after":{"type":"array","items":{"type":"string"},"description":"Start the connector after all connectors with specified labels.\\n\\nThis property is used to control the start order of connectors.\\nThe connector will not start until all connectors with the specified\\nlabels have finished processing all inputs.","nullable":true},"transport":{"oneOf":[{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","description":"Configuration for reading data from a file with `FileInputTransport`","required":["path"],"properties":{"buffer_size_bytes":{"type":"integer","description":"Read buffer size.\\n\\nDefault: when this parameter is not specified, a platform-specific\\ndefault is used.","nullable":true,"minimum":0},"follow":{"type":"boolean","description":"Enable file following.\\n\\nWhen `false`, the endpoint outputs an `InputConsumer::eoi`\\nmessage and stops upon reaching the end of file.  When `true`, the\\nendpoint will keep watching the file and outputting any new content\\nappended to it."},"path":{"type":"string","description":"File path."}}},"name":{"type":"string","enum":["file_input"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","description":"Configuration for writing data to a file with `FileOutputTransport`.","required":["path"],"properties":{"path":{"type":"string","description":"File path."}}},"name":{"type":"string","enum":["file_output"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","description":"Configuration for reading data from Kafka topics with `InputTransport`.","required":["topic"],"properties":{"group_join_timeout_secs":{"type":"integer","format":"int32","description":"Maximum timeout in seconds to wait for the endpoint to join the Kafka\\nconsumer group during initialization.","minimum":0},"log_level":{"nullable":true,"type":"string","description":"Kafka logging levels.","enum":["emerg","alert","critical","error","warning","notice","info","debug"]},"partitions":{"type":"array","items":{"type":"integer","format":"int32"},"description":"The list of Kafka partitions to read from.\\n\\nOnly the specified partitions will be consumed. If this field is not set,\\nthe connector will consume from all available partitions.\\n\\nIf `start_from` is set to `offsets` and this field is provided, the\\nnumber of partitions must exactly match the number of offsets, and the\\norder of partitions must correspond to the order of offsets.\\n\\nIf offsets are provided for all partitions, this field can be omitted.","nullable":true},"poller_threads":{"type":"integer","description":"Set to 1 or more to fix the number of threads used to poll\\n`rdkafka`. Multiple threads can increase performance with small Kafka\\nmessages; for large messages, one thread is enough. In either case, too\\nmany threads can harm performance. If unset, the default is 3, which\\nhelps with small messages but will not harm performance with large\\nmessagee","nullable":true,"minimum":0},"region":{"type":"string","description":"The AWS region to use while connecting to AWS Managed Streaming for Kafka (MSK).","nullable":true},"start_from":{"oneOf":[{"type":"string","description":"Start from the beginning of the topic.","enum":["earliest"]},{"type":"string","description":"Start from the current end of the topic.\\n\\nThis will only read any data that is added to the topic after the\\nconnector initializes.","enum":["latest"]},{"type":"object","required":["offsets"],"properties":{"offsets":{"type":"array","items":{"type":"integer","format":"int64"},"description":"Start from particular offsets in the topic.\\n\\nThe number of offsets must match the number of partitions in the topic."}}}],"description":"Where to begin reading a Kafka topic."},"topic":{"type":"string","description":"Topic to subscribe to."}},"additionalProperties":{"type":"string","description":"Options passed directly to `rdkafka`.\\n\\n[`librdkafka` options](https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md)\\nused to configure the Kafka consumer.\\n\\nThis input connector does not use consumer groups, so options related to\\nconsumer groups are rejected, including:\\n\\n* `group.id`, if present, is ignored.\\n* `auto.offset.reset` (use `start_from` instead).\\n* \\"enable.auto.commit\\", if present, must be set to \\"false\\".\\n* \\"enable.auto.offset.store\\", if present, must be set to \\"false\\"."}},"name":{"type":"string","enum":["kafka_input"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","description":"Configuration for writing data to a Kafka topic with `OutputTransport`.","required":["topic"],"properties":{"fault_tolerance":{"nullable":true,"type":"object","description":"Fault tolerance configuration for Kafka output connector.","properties":{"consumer_options":{"type":"object","description":"Options passed to `rdkafka` for consumers only, as documented at\\n[`librdkafka`\\noptions](https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md).\\n\\nThese options override `kafka_options` for consumers, and may be empty.","default":{},"additionalProperties":{"type":"string"}},"producer_options":{"type":"object","description":"Options passed to `rdkafka` for producers only, as documented at\\n[`librdkafka`\\noptions](https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md).\\n\\nThese options override `kafka_options` for producers, and may be empty.","default":{},"additionalProperties":{"type":"string"}}}},"headers":{"type":"array","items":{"type":"object","description":"Kafka message header.","required":["key"],"properties":{"key":{"type":"string"},"value":{"nullable":true,"type":"string","format":"binary","description":"Kafka header value encoded as a UTF-8 string or a byte array."}}},"description":"Kafka headers to be added to each message produced by this connector."},"initialization_timeout_secs":{"type":"integer","format":"int32","description":"Maximum timeout in seconds to wait for the endpoint to connect to\\na Kafka broker.\\n\\nDefaults to 60.","minimum":0},"kafka_service":{"type":"string","description":"If specified, this service is used to provide defaults for the Kafka options.","nullable":true},"log_level":{"nullable":true,"type":"string","description":"Kafka logging levels.","enum":["emerg","alert","critical","error","warning","notice","info","debug"]},"region":{"type":"string","description":"The AWS region to use while connecting to AWS Managed Streaming for Kafka (MSK).","nullable":true},"topic":{"type":"string","description":"Topic to write to."}},"additionalProperties":{"type":"string","description":"Options passed directly to `rdkafka`.\\n\\nSee [`librdkafka` options](https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md)\\nused to configure the Kafka producer."}},"name":{"type":"string","enum":["kafka_output"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","description":"Google Pub/Sub input connector configuration.","required":["subscription"],"properties":{"connect_timeout_seconds":{"type":"integer","format":"int32","description":"gRPC connection timeout.","nullable":true,"minimum":0},"credentials":{"type":"string","description":"The content of a Google Cloud credentials JSON file.\\n\\nWhen this option is specified, the connector will use the provided credentials for\\nauthentication.  Otherwise, it will use Application Default Credentials (ADC) configured\\nin the environment where the Feldera service is running.  See\\n[Google Cloud documentation](https://cloud.google.com/docs/authentication/provide-credentials-adc)\\nfor information on configuring application default credentials.\\n\\nWhen running Feldera in an environment where ADC are not configured,\\ne.g., a Docker container, use this option to ship Google Cloud credentials from another environment.\\nFor example, if you use the\\n[`gcloud auth application-default login`](https://cloud.google.com/pubsub/docs/authentication#client-libs)\\ncommand for authentication in your local development environment, ADC are stored in the\\n`.config/gcloud/application_default_credentials.json` file in your home directory.","nullable":true},"emulator":{"type":"string","description":"Set in order to use a Pub/Sub [emulator](https://cloud.google.com/pubsub/docs/emulator)\\ninstead of the production service, e.g., \'localhost:8681\'.","nullable":true},"endpoint":{"type":"string","description":"Override the default service endpoint \'pubsub.googleapis.com\'","nullable":true},"pool_size":{"type":"integer","format":"int32","description":"gRPC channel pool size.","nullable":true,"minimum":0},"project_id":{"type":"string","description":"Google Cloud project_id.\\n\\nWhen not specified, the connector will use the project id associated\\nwith the authenticated account.","nullable":true},"snapshot":{"type":"string","description":"Reset subscription\'s backlog to a given snapshot on startup,\\nusing the Pub/Sub `Seek` API.\\n\\nThis option is mutually exclusive with the `timestamp` option.","nullable":true},"subscription":{"type":"string","description":"Subscription name."},"timeout_seconds":{"type":"integer","format":"int32","description":"gRPC request timeout.","nullable":true,"minimum":0},"timestamp":{"type":"string","description":"Reset subscription\'s backlog to a given timestamp on startup,\\nusing the Pub/Sub `Seek` API.\\n\\nThe value of this option is an ISO 8601-encoded UTC time, e.g., \\"2024-08-17T16:39:57-08:00\\".\\n\\nThis option is mutually exclusive with the `snapshot` option.","nullable":true}}},"name":{"type":"string","enum":["pub_sub_input"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","description":"Configuration for reading data from an HTTP or HTTPS URL with\\n`UrlInputTransport`.","required":["path"],"properties":{"path":{"type":"string","description":"URL."},"pause_timeout":{"type":"integer","format":"int32","description":"Timeout before disconnection when paused, in seconds.\\n\\nIf the pipeline is paused, or if the input adapter reads data faster\\nthan the pipeline can process it, then the controller will pause the\\ninput adapter. If the input adapter stays paused longer than this\\ntimeout, it will drop the network connection to the server. It will\\nautomatically reconnect when the input adapter starts running again.","minimum":0}}},"name":{"type":"string","enum":["url_input"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","description":"Configuration for reading data from AWS S3.","required":["region","bucket_name"],"properties":{"aws_access_key_id":{"type":"string","description":"AWS Access Key id. This property must be specified unless `no_sign_request` is set to `true`.","nullable":true},"aws_secret_access_key":{"type":"string","description":"Secret Access Key. This property must be specified unless `no_sign_request` is set to `true`.","nullable":true},"bucket_name":{"type":"string","description":"S3 bucket name to access."},"endpoint_url":{"type":"string","description":"The endpoint URL used to communicate with this service. Can be used to make this connector\\ntalk to non-AWS services with an S3 API.","nullable":true},"key":{"type":"string","description":"Read a single object specified by a key.","nullable":true},"max_concurrent_fetches":{"type":"integer","format":"int32","description":"Controls the number of S3 objects fetched in parallel.\\n\\nIncreasing this value can improve throughput by enabling greater concurrency.\\nHowever, higher concurrency may lead to timeouts or increased memory usage due to in-memory buffering.\\n\\nRecommended range: 1\u201310. Default: 8.","minimum":0},"no_sign_request":{"type":"boolean","description":"Do not sign requests. This is equivalent to the `--no-sign-request` flag in the AWS CLI."},"prefix":{"type":"string","description":"Read all objects whose keys match a prefix. Set to an empty string to read all objects in the bucket.","nullable":true},"region":{"type":"string","description":"AWS region."}}},"name":{"type":"string","enum":["s3_input"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","description":"Delta table input connector configuration.","required":["uri","mode"],"properties":{"cdc_delete_filter":{"type":"string","description":"A predicate that determines whether the record represents a deletion.\\n\\nThis setting is only valid in the `cdc` mode. It specifies a predicate applied to\\neach row in the Delta table to determine whether the row represents a deletion event.\\nIts value must be a valid Boolean SQL expression that can be used in a query of the\\nform `SELECT * from <table> WHERE <cdc_delete_filter>`.","nullable":true},"cdc_order_by":{"type":"string","description":"An expression that determines the ordering of updates in the Delta table.\\n\\nThis setting is only valid in the `cdc` mode. It specifies a predicate applied to\\neach row in the Delta table to determine the order in which updates in the table should\\nbe applied. Its value must be a valid SQL expression that can be used in a query of the\\nform `SELECT * from <table> ORDER BY <cdc_order_by>`.","nullable":true},"datetime":{"type":"string","description":"Optional timestamp for the snapshot in the ISO-8601/RFC-3339 format, e.g.,\\n\\"2024-12-09T16:09:53+00:00\\".\\n\\nWhen this option is set, the connector finds and opens the version of the table as of the\\nspecified point in time (based on the server time recorded in the transaction log, not the\\nevent time encoded in the data).  In `snapshot` and `snapshot_and_follow` modes, it\\nretrieves the snapshot of this version of the table.  In `follow`, `snapshot_and_follow`, and\\n`cdc` modes, it follows transaction log records **after** this version.\\n\\nNote: at most one of `version` and `datetime` options can be specified.\\nWhen neither of the two options is specified, the latest committed version of the table\\nis used.","nullable":true},"end_version":{"type":"integer","format":"int64","description":"Optional final table version.\\n\\nValid only when the connector is configured in `follow`, `snapshot_and_follow`, or `cdc` mode.\\n\\nWhen set, the connector will stop scanning the table\u2019s transaction log after reaching this version or any greater version.\\nThis bound is inclusive: if the specified version appears in the log, it will be processed before signaling end-of-input.","nullable":true},"filter":{"type":"string","description":"Optional row filter.\\n\\nWhen specified, only rows that satisfy the filter condition are read from the delta table.\\nThe condition must be a valid SQL Boolean expression that can be used in\\nthe `where` clause of the `select * from my_table where ...` query.","nullable":true},"max_concurrent_readers":{"type":"integer","format":"int32","description":"Maximum number of concurrent object store reads performed by all Delta Lake connectors.\\n\\nThis setting is used to limit the number of concurrent reads of the object store in a\\npipeline with a large number of Delta Lake connectors. When multiple connectors are simultaneously\\nreading from the object store, this can lead to transport timeouts.\\n\\nWhen enabled, this setting limits the number of concurrent reads across all connectors.\\nThis is a global setting that affects all Delta Lake connectors, and not just the connector\\nwhere it is specified. It should therefore be used at most once in a pipeline.  If multiple\\nconnectors specify this setting, they must all use the same value.\\n\\nThe default value is 6.","nullable":true,"minimum":0},"mode":{"type":"string","description":"Delta table read mode.\\n\\nThree options are available:\\n\\n* `snapshot` - read a snapshot of the table and stop.\\n\\n* `follow` - continuously ingest changes to the table, starting from a specified version\\nor timestamp.\\n\\n* `snapshot_and_follow` - read a snapshot of the table before switching to continuous ingestion\\nmode.","enum":["snapshot","follow","snapshot_and_follow","cdc"]},"num_parsers":{"type":"integer","format":"int32","description":"The number of parallel parsing tasks the connector uses to process data read from the\\ntable. Increasing this value can enhance performance by allowing more concurrent processing.\\nRecommended range: 1\u201310. The default is 4.","minimum":0},"skip_unused_columns":{"type":"boolean","description":"Don\'t read unused columns from the Delta table.\\n\\nWhen set to `true`, this option instructs the connector to avoid reading\\ncolumns from the Delta table that are not used in any view definitions.\\nTo be skipped, the columns must be either nullable or have default\\nvalues. This can improve ingestion performance, especially for wide\\ntables.\\n\\nNote: The simplest way to exclude unused columns is to omit them from the Feldera SQL table\\ndeclaration. The connector never reads columns that aren\'t declared in the SQL schema.\\nAdditionally, the SQL compiler emits warnings for declared but unused columns\u2014use these as\\na guide to optimize your schema."},"snapshot_filter":{"type":"string","description":"Optional snapshot filter.\\n\\nThis option is only valid when `mode` is set to `snapshot` or `snapshot_and_follow`.\\n\\nWhen specified, only rows that satisfy the filter condition are included in the\\nsnapshot.  The condition must be a valid SQL Boolean expression that can be used in\\nthe `where` clause of the `select * from snapshot where ...` query.\\n\\nUnlike the `filter` option, which applies to all records retrieved from the table, this\\nfilter only applies to rows in the initial snapshot of the table.\\nFor instance, it can be used to specify the range of event times to include in the snapshot,\\ne.g.: `ts BETWEEN TIMESTAMP \'2005-01-01 00:00:00\' AND TIMESTAMP \'2010-12-31 23:59:59\'`.\\n\\nThis option can be used together with the `filter` option. During the initial snapshot,\\nonly rows that satisfy both `filter` and `snapshot_filter` are retrieved from the Delta table.\\nWhen subsequently following changes in the the transaction log (`mode = snapshot_and_follow`),\\nall rows that meet the `filter` condition are ingested, regardless of `snapshot_filter`.","nullable":true},"timestamp_column":{"type":"string","description":"Table column that serves as an event timestamp.\\n\\nWhen this option is specified, and `mode` is one of `snapshot` or `snapshot_and_follow`,\\ntable rows are ingested in the timestamp order, respecting the\\n[`LATENESS`](https://docs.feldera.com/sql/streaming#lateness-expressions)\\nproperty of the column: each ingested row has a timestamp no more than `LATENESS`\\ntime units earlier than the most recent timestamp of any previously ingested row.\\nThe ingestion is performed by partitioning the table into timestamp ranges of width\\n`LATENESS`. Each range is processed sequentially, in increasing timestamp order.\\n\\n# Example\\n\\nConsider a table with timestamp column of type `TIMESTAMP` and lateness attribute\\n`INTERVAL 1 DAY`. Assuming that the oldest timestamp in the table is\\n`2024-01-01T00:00:00``, the connector will fetch all records with timestamps\\nfrom `2024-01-01`, then all records for `2024-01-02`, `2024-01-03`, etc., until all records\\nin the table have been ingested.\\n\\n# Requirements\\n\\n* The timestamp column must be of a supported type: integer, `DATE`, or `TIMESTAMP`.\\n* The timestamp column must be declared with non-zero `LATENESS`.\\n* For efficient ingest, the table must be optimized for timestamp-based\\nqueries using partitioning, Z-ordering, or liquid clustering.","nullable":true},"uri":{"type":"string","description":"Table URI.\\n\\nExample: \\"s3://feldera-fraud-detection-data/demographics_train\\""},"version":{"type":"integer","format":"int64","description":"Optional table version.\\n\\nWhen this option is set, the connector finds and opens the specified version of the table.\\nIn `snapshot` and `snapshot_and_follow` modes, it retrieves the snapshot of this version of\\nthe table.  In `follow`, `snapshot_and_follow`, and `cdc` modes, it follows transaction log records\\n**after** this version.\\n\\nNote: at most one of `version` and `datetime` options can be specified.\\nWhen neither of the two options is specified, the latest committed version of the table\\nis used.","nullable":true}},"additionalProperties":{"type":"string","description":"Storage options for configuring backend object store.\\n\\nFor specific options available for different storage backends, see:\\n* [Azure options](https://docs.rs/object_store/latest/object_store/azure/enum.AzureConfigKey.html)\\n* [Amazon S3 options](https://docs.rs/object_store/latest/object_store/aws/enum.AmazonS3ConfigKey.html)\\n* [Google Cloud Storage options](https://docs.rs/object_store/latest/object_store/gcp/enum.GoogleConfigKey.html)"}},"name":{"type":"string","enum":["delta_table_input"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","description":"Delta table output connector configuration.","required":["uri"],"properties":{"mode":{"type":"string","description":"Delta table write mode.\\n\\nDetermines how the Delta table connector handles an existing table at the target location.","enum":["append","truncate","error_if_exists"]},"uri":{"type":"string","description":"Table URI."}},"additionalProperties":{"type":"string","description":"Storage options for configuring backend object store.\\n\\nFor specific options available for different storage backends, see:\\n* [Azure options](https://docs.rs/object_store/latest/object_store/azure/enum.AzureConfigKey.html)\\n* [Amazon S3 options](https://docs.rs/object_store/latest/object_store/aws/enum.AmazonS3ConfigKey.html)\\n* [Google Cloud Storage options](https://docs.rs/object_store/latest/object_store/gcp/enum.GoogleConfigKey.html)"}},"name":{"type":"string","enum":["delta_table_output"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","description":"Redis output connector configuration.","required":["connection_string"],"properties":{"connection_string":{"type":"string","description":"The URL format: `redis://[<username>][:<password>@]<hostname>[:port][/[<db>][?protocol=<protocol>]]`\\nThis is parsed by the [redis](https://docs.rs/redis/latest/redis/#connection-parameters) crate."},"key_separator":{"type":"string","description":"Separator used to join multiple components into a single key.\\n\\":\\" by default."}}},"name":{"type":"string","enum":["redis_output"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"description":"Iceberg input connector configuration.","type":"object","properties":{"glue.access-key-id":{"type":"string","description":"Access key id used to access the Glue catalog.","nullable":true},"glue.endpoint":{"type":"string","description":"Configure an alternative endpoint of the Glue service for Glue catalog to access.\\n\\nExample: `\\"https://glue.us-east-1.amazonaws.com\\"`","nullable":true},"glue.id":{"type":"string","description":"The 12-digit ID of the Glue catalog.","nullable":true},"glue.profile-name":{"type":"string","description":"Profile used to access the Glue catalog.","nullable":true},"glue.region":{"type":"string","description":"Region of the Glue catalog.","nullable":true},"glue.secret-access-key":{"type":"string","description":"Secret access key used to access the Glue catalog.","nullable":true},"glue.session-token":{"type":"string","nullable":true},"glue.warehouse":{"type":"string","description":"Location for table metadata.\\n\\nExample: `\\"s3://my-data-warehouse/tables/\\"`","nullable":true},"rest.audience":{"type":"string","description":"Logical name of target resource or service.","nullable":true},"rest.credential":{"type":"string","description":"Credential to use for OAuth2 credential flow when initializing the catalog.\\n\\nA key and secret pair separated by \\":\\" (key is optional).","nullable":true},"rest.headers":{"type":"array","items":{"type":"array","items":{"type":"string"}},"description":"Additional HTTP request headers added to each catalog REST API call.","nullable":true},"rest.oauth2-server-uri":{"type":"string","description":"Authentication URL to use for client credentials authentication (default: uri + \'v1/oauth/tokens\')","nullable":true},"rest.prefix":{"type":"string","description":"Customize table storage paths.\\n\\nWhen combined with the `warehouse` property, the prefix determines\\nhow table data is organized within the storage.","nullable":true},"rest.resource":{"type":"string","description":"URI for the target resource or service.","nullable":true},"rest.scope":{"type":"string","nullable":true},"rest.token":{"type":"string","description":"Bearer token value to use for `Authorization` header.","nullable":true},"rest.uri":{"type":"string","description":"URI identifying the REST catalog server.","nullable":true},"rest.warehouse":{"type":"string","description":"The default location for managed tables created by the catalog.","nullable":true},"catalog_type":{"nullable":true,"type":"string","enum":["rest","glue"]},"datetime":{"type":"string","description":"Optional timestamp for the snapshot in the ISO-8601/RFC-3339 format, e.g.,\\n\\"2024-12-09T16:09:53+00:00\\".\\n\\nWhen this option is set, the connector finds and opens the snapshot of the table as of the\\nspecified point in time (based on the server time recorded in the transaction\\nlog, not the event time encoded in the data).  In `snapshot` and `snapshot_and_follow`\\nmodes, it retrieves this snapshot.  In `follow` and `snapshot_and_follow` modes, it\\nfollows transaction log records **after** this snapshot.\\n\\nNote: at most one of `snapshot_id` and `datetime` options can be specified.\\nWhen neither of the two options is specified, the latest committed version of the table\\nis used.","nullable":true},"metadata_location":{"type":"string","description":"Location of the table metadata JSON file.\\n\\nThis propery is used to access an Iceberg table without a catalog. It is mutually\\nexclusive with the `catalog_type` property.","nullable":true},"mode":{"type":"string","description":"Iceberg table read mode.\\n\\nThree options are available:\\n\\n* `snapshot` - read a snapshot of the table and stop.\\n\\n* `follow` - continuously ingest changes to the table, starting from a specified snapshot\\nor timestamp.\\n\\n* `snapshot_and_follow` - read a snapshot of the table before switching to continuous ingestion\\nmode.","enum":["snapshot","follow","snapshot_and_follow"]},"snapshot_filter":{"type":"string","description":"Optional row filter.\\n\\nThis option is only valid when `mode` is set to `snapshot` or `snapshot_and_follow`.\\n\\nWhen specified, only rows that satisfy the filter condition are included in the\\nsnapshot.  The condition must be a valid SQL Boolean expression that can be used in\\nthe `where` clause of the `select * from snapshot where ...` query.\\n\\nThis option can be used to specify the range of event times to include in the snapshot,\\ne.g.: `ts BETWEEN \'2005-01-01 00:00:00\' AND \'2010-12-31 23:59:59\'`.","nullable":true},"snapshot_id":{"type":"integer","format":"int64","description":"Optional snapshot id.\\n\\nWhen this option is set, the connector finds the specified snapshot of the table.\\nIn `snapshot` and `snapshot_and_follow` modes, it loads this snapshot.\\nIn `follow` and `snapshot_and_follow` modes, it follows table updates\\n**after** this snapshot.\\n\\nNote: at most one of `snapshot_id` and `datetime` options can be specified.\\nWhen neither of the two options is specified, the latest committed version of the table\\nis used.","nullable":true},"table_name":{"type":"string","description":"Specifies the Iceberg table name in the \\"namespace.table\\" format.\\n\\nThis option is applicable when an Iceberg catalog is configured using the `catalog_type` property.","nullable":true},"timestamp_column":{"type":"string","description":"Table column that serves as an event timestamp.\\n\\nWhen this option is specified, and `mode` is one of `snapshot` or `snapshot_and_follow`,\\ntable rows are ingested in the timestamp order, respecting the\\n[`LATENESS`](https://docs.feldera.com/sql/streaming#lateness-expressions)\\nproperty of the column: each ingested row has a timestamp no more than `LATENESS`\\ntime units earlier than the most recent timestamp of any previously ingested row.\\nThe ingestion is performed by partitioning the table into timestamp ranges of width\\n`LATENESS`. Each range is processed sequentially, in increasing timestamp order.\\n\\n# Example\\n\\nConsider a table with timestamp column of type `TIMESTAMP` and lateness attribute\\n`INTERVAL 1 DAY`. Assuming that the oldest timestamp in the table is\\n`2024-01-01T00:00:00``, the connector will fetch all records with timestamps\\nfrom `2024-01-01`, then all records for `2024-01-02`, `2024-01-03`, etc., until all records\\nin the table have been ingested.\\n\\n# Requirements\\n\\n* The timestamp column must be of a supported type: integer, `DATE`, or `TIMESTAMP`.\\n* The timestamp column must be declared with non-zero `LATENESS`.\\n* For efficient ingest, the table must be optimized for timestamp-based\\nqueries using partitioning, Z-ordering, or liquid clustering.","nullable":true}},"required":["mode"],"additionalProperties":{"type":"string","description":"Storage options for configuring backend object store.\\n\\nSee the [list of available options in PyIceberg documentation](https://py.iceberg.apache.org/configuration/#fileio)."}},"name":{"type":"string","enum":["iceberg_input"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","description":"Postgres input connector configuration.","required":["uri","query"],"properties":{"query":{"type":"string","description":"Query that specifies what data to fetch from postgres."},"uri":{"type":"string","description":"Postgres URI.\\nSee: <https://docs.rs/tokio-postgres/0.7.12/tokio_postgres/config/struct.Config.html>"}}},"name":{"type":"string","enum":["postgres_input"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","description":"Postgres output connector configuration.","required":["uri","table"],"properties":{"table":{"type":"string","description":"The table to write the output to."},"uri":{"type":"string","description":"Postgres URI.\\nSee: <https://docs.rs/tokio-postgres/0.7.12/tokio_postgres/config/struct.Config.html>"}}},"name":{"type":"string","enum":["postgres_output"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","description":"Configuration for generating random data for a table.","properties":{"plan":{"type":"array","items":{"type":"object","description":"A random generation plan for a table that generates either a limited amount of rows or runs continuously.","properties":{"fields":{"type":"object","description":"Specifies the values that the generator should produce.","default":{},"additionalProperties":{"type":"object","description":"Configuration for generating random data for a field of a table.","properties":{"e":{"type":"integer","format":"int64","description":"The frequency rank exponent for the Zipf distribution.\\n\\n- This value is only used if the strategy is set to `Zipf`.\\n- The default value is 1.0.","default":1},"fields":{"type":"object","description":"Specifies the values that the generator should produce in case the field is a struct type.","default":null,"additionalProperties":{"$ref":"#/components/schemas/RngFieldSettings"},"nullable":true},"key":{"default":null,"nullable":true,"$ref":"#/components/schemas/RngFieldSettings"},"null_percentage":{"type":"integer","description":"Percentage of records where this field should be set to NULL.\\n\\nIf not set, the generator will produce only records with non-NULL values.\\nIf set to `1..=100`, the generator will produce records with NULL values with the specified percentage.","default":null,"nullable":true,"minimum":0},"range":{"type":"object","description":"An optional, exclusive range [a, b) to limit the range of values the generator should produce.\\n\\n- For integer/floating point types specifies min/max values as an integer.\\nIf not set, the generator will produce values for the entire range of the type for number types.\\n- For string/binary types specifies min/max length as an integer, values are required to be >=0.\\nIf not set, a range of [0, 25) is used by default.\\n- For timestamp types specifies the min/max as two strings in the RFC 3339 format\\n(e.g., [\\"2021-01-01T00:00:00Z\\", \\"2022-01-02T00:00:00Z\\"]).\\nAlternatively, the range values can be specified as a number of non-leap\\nmilliseconds since January 1, 1970 0:00:00.000 UTC (aka \u201cUNIX timestamp\u201d).\\nIf not set, a range of [\\"1970-01-01T00:00:00Z\\", \\"2100-01-01T00:00:00Z\\") or [0, 4102444800000)\\nis used by default.\\n- For time types specifies the min/max as two strings in the \\"HH:MM:SS\\" format.\\nAlternatively, the range values can be specified in milliseconds as two positive integers.\\nIf not set, the range is 24h.\\n- For date types, the min/max range is specified as two strings in the \\"YYYY-MM-DD\\" format.\\nAlternatively, two integers that represent number of days since January 1, 1970 can be used.\\nIf not set, a range of [\\"1970-01-01\\", \\"2100-01-01\\") or [0, 54787) is used by default.\\n- For array types specifies the min/max number of elements as an integer.\\nIf not set, a range of [0, 5) is used by default. Range values are required to be >=0.\\n- For map types specifies the min/max number of key-value pairs as an integer.\\nIf not set, a range of [0, 5) is used by default.\\n- For struct/boolean/null types `range` is ignored."},"scale":{"type":"integer","format":"int64","description":"A scale factor to apply a multiplier to the generated value.\\n\\n- For integer/floating point types, the value is multiplied by the scale factor.\\n- For timestamp types, the generated value (milliseconds) is multiplied by the scale factor.\\n- For time types, the generated value (milliseconds) is multiplied by the scale factor.\\n- For date types, the generated value (days) is multiplied by the scale factor.\\n- For string/binary/array/map/struct/boolean/null types, the scale factor is ignored.\\n\\n- If `values` is specified, the scale factor is ignored.\\n- If `range` is specified and the range is required to be positive (struct, map, array etc.)\\nthe scale factor is required to be positive too.\\n\\nThe default scale factor is 1.","default":1},"strategy":{"default":"increment","type":"string","description":"Strategy used to generate values.","enum":["increment","uniform","zipf","word","words","sentence","sentences","paragraph","paragraphs","first_name","last_name","title","suffix","name","name_with_title","domain_suffix","email","username","password","field","position","seniority","job_title","ipv4","ipv6","ip","mac_address","user_agent","rfc_status_code","valid_status_code","company_suffix","company_name","buzzword","buzzword_middle","buzzword_tail","catch_phrase","bs_verb","bs_adj","bs_noun","bs","profession","industry","currency_code","currency_name","currency_symbol","credit_card_number","city_prefix","city_suffix","city_name","country_name","country_code","street_suffix","street_name","time_zone","state_name","state_abbr","secondary_address_type","secondary_address","zip_code","post_code","building_number","latitude","longitude","isbn","isbn13","isbn10","phone_number","cell_number","file_path","file_name","file_extension","dir_path"]},"value":{"default":null,"nullable":true,"$ref":"#/components/schemas/RngFieldSettings"},"values":{"type":"array","items":{"type":"object"},"description":"An optional set of values the generator will pick from.\\n\\nIf set, the generator will pick values from the specified set.\\nIf not set, the generator will produce values according to the specified range.\\nIf set to an empty set, the generator will produce NULL values.\\nIf set to a single value, the generator will produce only that value.\\n\\nNote that `range` is ignored if `values` is set.","default":null,"nullable":true}},"additionalProperties":false}},"limit":{"type":"integer","description":"Total number of new rows to generate.\\n\\nIf not set, the generator will produce new/unique records as long as the pipeline is running.\\nIf set to 0, the table will always remain empty.\\nIf set, the generator will produce new records until the specified limit is reached.\\n\\nNote that if the table has one or more primary keys that don\'t use the `increment` strategy to\\ngenerate the key there is a potential that an update is generated instead of an insert. In\\nthis case it\'s possible the total number of records is less than the specified limit.","default":null,"nullable":true,"minimum":0},"rate":{"type":"integer","format":"int32","description":"Non-zero number of rows to generate per second.\\n\\nIf not set, the generator will produce rows as fast as possible.","default":null,"nullable":true,"minimum":0},"worker_chunk_size":{"type":"integer","description":"When multiple workers are used, each worker will pick a consecutive \\"chunk\\" of\\nrecords to generate.\\n\\nBy default, if not specified, the generator will use the formula `min(rate, 10_000)`\\nto determine it. This works well in most situations. However, if you\'re\\nrunning tests with lateness and many workers you can e.g., reduce the\\nchunk size to make sure a smaller range of records is being ingested in parallel.\\n\\n# Example\\nAssume you generate a total of 125 records with 4 workers and a chunk size of 25.\\nIn this case, worker A will generate records 0..25, worker B will generate records 25..50,\\netc. A, B, C, and D will generate records in parallel. The first worker to finish its chunk\\nwill pick up the last chunk of records (100..125) to generate.","default":null,"nullable":true,"minimum":0}},"additionalProperties":false},"description":"The sequence of generations to perform.\\n\\nIf not set, the generator will produce a single sequence with default settings.\\nIf set, the generator will produce the specified sequences in sequential order.\\n\\nNote that if one of the sequences before the last one generates an unlimited number of rows\\nthe following sequences will not be executed.","default":[{"rate":null,"limit":null,"worker_chunk_size":null,"fields":{}}]},"seed":{"type":"integer","format":"int64","description":"Optional seed for the random generator.\\n\\nSetting this to a fixed value will make the generator produce the same sequence of records\\nevery time the pipeline is run.\\n\\n# Notes\\n- To ensure the set of generated input records is deterministic across multiple runs,\\napart from setting a seed, `workers` also needs to remain unchanged.\\n- The input will arrive in non-deterministic order if `workers > 1`.","default":null,"nullable":true,"minimum":0},"workers":{"type":"integer","description":"Number of workers to use for generating data.","default":1,"minimum":0}},"additionalProperties":false},"name":{"type":"string","enum":["datagen"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","description":"Configuration for generating Nexmark input data.\\n\\nThis connector must be used exactly three times in a pipeline if it is used\\nat all, once for each [`NexmarkTable`].","required":["table"],"properties":{"options":{"nullable":true,"type":"object","description":"Configuration for generating Nexmark input data.","properties":{"batch_size_per_thread":{"type":"integer","format":"int64","description":"Number of events to generate and submit together, per thread.\\n\\nEach thread generates this many records, which are then combined with\\nthe records generated by the other threads, to form combined input\\nbatches of size `threads \xd7 batch_size_per_thread`.","default":1000,"minimum":0},"events":{"type":"integer","format":"int64","description":"Number of events to generate.","default":100000000,"minimum":0},"max_step_size_per_thread":{"type":"integer","format":"int64","description":"Maximum number of events to submit in a single step, per thread.\\n\\nThis should really be per worker thread, not per generator thread, but\\nthe connector does not know how many worker threads there are.\\n\\nThis stands in for `max_batch_size` from the connector configuration\\nbecause it must be a constant across all three of the nexmark tables.","default":10000,"minimum":0},"threads":{"type":"integer","description":"Number of event generator threads.\\n\\nIt\'s reasonable to choose the same number of generator threads as worker\\nthreads.","default":4,"minimum":0}}},"table":{"type":"string","description":"Table in Nexmark.","enum":["bid","auction","person"]}}},"name":{"type":"string","enum":["nexmark"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","description":"Configuration for reading data via HTTP.\\n\\nHTTP input adapters cannot be usefully configured as part of pipeline\\nconfiguration.  Instead, instantiate them through the REST API as\\n`/pipelines/{pipeline_name}/ingress/{table_name}`.","required":["name"],"properties":{"name":{"type":"string","description":"Autogenerated name."}}},"name":{"type":"string","enum":["http_input"]}}},{"type":"object","required":["name"],"properties":{"name":{"type":"string","enum":["http_output"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","description":"Configuration for inserting data with ad-hoc queries\\n\\nAn ad-hoc input adapters cannot be usefully configured as part of pipeline\\nconfiguration.  Instead, use ad-hoc queries through the UI, the REST API, or\\nthe `fda` command-line tool.","required":["name"],"properties":{"name":{"type":"string","description":"Autogenerated name."}}},"name":{"type":"string","enum":["ad_hoc_input"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","required":["clock_resolution_usecs"],"properties":{"clock_resolution_usecs":{"type":"integer","format":"int64","minimum":0}}},"name":{"type":"string","enum":["clock_input"]}}}],"description":"Transport-specific endpoint configuration passed to\\n`crate::OutputTransport::new_endpoint`\\nand `crate::InputTransport::new_endpoint`.","discriminator":{"propertyName":"name"}}}}},"output_connectors":{"type":"object","description":"Output connectors derived from the schema.","additionalProperties":{"description":"Describes an output connector configuration","type":"object","required":["stream"],"properties":{"stream":{"type":"string","description":"The name of the output stream of the circuit that this endpoint is\\nconnected to."},"enable_output_buffer":{"type":"boolean","description":"Enable output buffering.\\n\\nThe output buffering mechanism allows decoupling the rate at which the pipeline\\npushes changes to the output transport from the rate of input changes.\\n\\nBy default, output updates produced by the pipeline are pushed directly to\\nthe output transport. Some destinations may prefer to receive updates in fewer\\nbigger batches. For instance, when writing Parquet files, producing\\none bigger file every few minutes is usually better than creating\\nsmall files every few milliseconds.\\n\\nTo achieve such input/output decoupling, users can enable output buffering by\\nsetting the `enable_output_buffer` flag to `true`.  When buffering is enabled, output\\nupdates produced by the pipeline are consolidated in an internal buffer and are\\npushed to the output transport when one of several conditions is satisfied:\\n\\n* data has been accumulated in the buffer for more than `max_output_buffer_time_millis`\\nmilliseconds.\\n* buffer size exceeds `max_output_buffer_size_records` records.\\n\\nThis flag is `false` by default.","default":false},"max_output_buffer_size_records":{"type":"integer","description":"Maximum number of updates to be kept in the output buffer.\\n\\nThis parameter bounds the maximal size of the buffer.\\nNote that the size of the buffer is not always equal to the\\ntotal number of updates output by the pipeline. Updates to the\\nsame record can overwrite or cancel previous updates.\\n\\nBy default, the buffer can grow indefinitely until one of\\nthe other output conditions is satisfied.\\n\\nNOTE: this configuration option requires the `enable_output_buffer` flag\\nto be set.","default":18446744073709552000,"minimum":0},"max_output_buffer_time_millis":{"type":"integer","description":"Maximum time in milliseconds data is kept in the output buffer.\\n\\nBy default, data is kept in the buffer indefinitely until one of\\nthe other output conditions is satisfied.  When this option is\\nset the buffer will be flushed at most every\\n`max_output_buffer_time_millis` milliseconds.\\n\\nNOTE: this configuration option requires the `enable_output_buffer` flag\\nto be set.","default":18446744073709552000,"minimum":0},"format":{"nullable":true,"type":"object","description":"Data format specification used to parse raw data received from the\\nendpoint or to encode data sent to the endpoint.","required":["name"],"properties":{"config":{"type":"object","description":"Format-specific parser or encoder configuration."},"name":{"type":"string","description":"Format name, e.g., \\"csv\\", \\"json\\", \\"bincode\\", etc."}}},"index":{"type":"string","description":"Name of the index that the connector is attached to.\\n\\nThis property is valid for output connectors only.  It is used with data\\ntransports and formats that expect output updates in the form of key/value\\npairs, where the key typically represents a unique id associated with the\\ntable or view.\\n\\nTo support such output formats, an output connector can be attached to an\\nindex created using the SQL CREATE INDEX statement.  An index of a table\\nor view contains the same updates as the table or view itself, indexed by\\none or more key columns.\\n\\nSee individual connector documentation for details on how they work\\nwith indexes.","nullable":true},"labels":{"type":"array","items":{"type":"string"},"description":"Arbitrary user-defined text labels associated with the connector.\\n\\nThese labels can be used in conjunction with the `start_after` property\\nto control the start order of connectors."},"max_batch_size":{"type":"integer","format":"int64","description":"Maximum batch size, in records.\\n\\nThis is the maximum number of records to process in one batch through\\nthe circuit.  The time and space cost of processing a batch is\\nasymptotically superlinear in the size of the batch, but very small\\nbatches are less efficient due to constant factors.\\n\\nThis should usually be less than `max_queued_records`, to give the\\nconnector a round-trip time to restart and refill the buffer while\\nbatches are being processed.\\n\\nSome input adapters might not honor this setting.\\n\\nThe default is 10,000.","minimum":0},"max_queued_records":{"type":"integer","format":"int64","description":"Backpressure threshold.\\n\\nMaximal number of records queued by the endpoint before the endpoint\\nis paused by the backpressure mechanism.\\n\\nFor input endpoints, this setting bounds the number of records that have\\nbeen received from the input transport but haven\'t yet been consumed by\\nthe circuit since the circuit, since the circuit is still busy processing\\nprevious inputs.\\n\\nFor output endpoints, this setting bounds the number of records that have\\nbeen produced by the circuit but not yet sent via the output transport endpoint\\nnor stored in the output buffer (see `enable_output_buffer`).\\n\\nNote that this is not a hard bound: there can be a small delay between\\nthe backpressure mechanism is triggered and the endpoint is paused, during\\nwhich more data may be queued.\\n\\nThe default is 1 million.","minimum":0},"paused":{"type":"boolean","description":"Create connector in paused state.\\n\\nThe default is `false`."},"start_after":{"type":"array","items":{"type":"string"},"description":"Start the connector after all connectors with specified labels.\\n\\nThis property is used to control the start order of connectors.\\nThe connector will not start until all connectors with the specified\\nlabels have finished processing all inputs.","nullable":true},"transport":{"oneOf":[{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","description":"Configuration for reading data from a file with `FileInputTransport`","required":["path"],"properties":{"buffer_size_bytes":{"type":"integer","description":"Read buffer size.\\n\\nDefault: when this parameter is not specified, a platform-specific\\ndefault is used.","nullable":true,"minimum":0},"follow":{"type":"boolean","description":"Enable file following.\\n\\nWhen `false`, the endpoint outputs an `InputConsumer::eoi`\\nmessage and stops upon reaching the end of file.  When `true`, the\\nendpoint will keep watching the file and outputting any new content\\nappended to it."},"path":{"type":"string","description":"File path."}}},"name":{"type":"string","enum":["file_input"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","description":"Configuration for writing data to a file with `FileOutputTransport`.","required":["path"],"properties":{"path":{"type":"string","description":"File path."}}},"name":{"type":"string","enum":["file_output"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","description":"Configuration for reading data from Kafka topics with `InputTransport`.","required":["topic"],"properties":{"group_join_timeout_secs":{"type":"integer","format":"int32","description":"Maximum timeout in seconds to wait for the endpoint to join the Kafka\\nconsumer group during initialization.","minimum":0},"log_level":{"nullable":true,"type":"string","description":"Kafka logging levels.","enum":["emerg","alert","critical","error","warning","notice","info","debug"]},"partitions":{"type":"array","items":{"type":"integer","format":"int32"},"description":"The list of Kafka partitions to read from.\\n\\nOnly the specified partitions will be consumed. If this field is not set,\\nthe connector will consume from all available partitions.\\n\\nIf `start_from` is set to `offsets` and this field is provided, the\\nnumber of partitions must exactly match the number of offsets, and the\\norder of partitions must correspond to the order of offsets.\\n\\nIf offsets are provided for all partitions, this field can be omitted.","nullable":true},"poller_threads":{"type":"integer","description":"Set to 1 or more to fix the number of threads used to poll\\n`rdkafka`. Multiple threads can increase performance with small Kafka\\nmessages; for large messages, one thread is enough. In either case, too\\nmany threads can harm performance. If unset, the default is 3, which\\nhelps with small messages but will not harm performance with large\\nmessagee","nullable":true,"minimum":0},"region":{"type":"string","description":"The AWS region to use while connecting to AWS Managed Streaming for Kafka (MSK).","nullable":true},"start_from":{"oneOf":[{"type":"string","description":"Start from the beginning of the topic.","enum":["earliest"]},{"type":"string","description":"Start from the current end of the topic.\\n\\nThis will only read any data that is added to the topic after the\\nconnector initializes.","enum":["latest"]},{"type":"object","required":["offsets"],"properties":{"offsets":{"type":"array","items":{"type":"integer","format":"int64"},"description":"Start from particular offsets in the topic.\\n\\nThe number of offsets must match the number of partitions in the topic."}}}],"description":"Where to begin reading a Kafka topic."},"topic":{"type":"string","description":"Topic to subscribe to."}},"additionalProperties":{"type":"string","description":"Options passed directly to `rdkafka`.\\n\\n[`librdkafka` options](https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md)\\nused to configure the Kafka consumer.\\n\\nThis input connector does not use consumer groups, so options related to\\nconsumer groups are rejected, including:\\n\\n* `group.id`, if present, is ignored.\\n* `auto.offset.reset` (use `start_from` instead).\\n* \\"enable.auto.commit\\", if present, must be set to \\"false\\".\\n* \\"enable.auto.offset.store\\", if present, must be set to \\"false\\"."}},"name":{"type":"string","enum":["kafka_input"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","description":"Configuration for writing data to a Kafka topic with `OutputTransport`.","required":["topic"],"properties":{"fault_tolerance":{"nullable":true,"type":"object","description":"Fault tolerance configuration for Kafka output connector.","properties":{"consumer_options":{"type":"object","description":"Options passed to `rdkafka` for consumers only, as documented at\\n[`librdkafka`\\noptions](https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md).\\n\\nThese options override `kafka_options` for consumers, and may be empty.","default":{},"additionalProperties":{"type":"string"}},"producer_options":{"type":"object","description":"Options passed to `rdkafka` for producers only, as documented at\\n[`librdkafka`\\noptions](https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md).\\n\\nThese options override `kafka_options` for producers, and may be empty.","default":{},"additionalProperties":{"type":"string"}}}},"headers":{"type":"array","items":{"type":"object","description":"Kafka message header.","required":["key"],"properties":{"key":{"type":"string"},"value":{"nullable":true,"type":"string","format":"binary","description":"Kafka header value encoded as a UTF-8 string or a byte array."}}},"description":"Kafka headers to be added to each message produced by this connector."},"initialization_timeout_secs":{"type":"integer","format":"int32","description":"Maximum timeout in seconds to wait for the endpoint to connect to\\na Kafka broker.\\n\\nDefaults to 60.","minimum":0},"kafka_service":{"type":"string","description":"If specified, this service is used to provide defaults for the Kafka options.","nullable":true},"log_level":{"nullable":true,"type":"string","description":"Kafka logging levels.","enum":["emerg","alert","critical","error","warning","notice","info","debug"]},"region":{"type":"string","description":"The AWS region to use while connecting to AWS Managed Streaming for Kafka (MSK).","nullable":true},"topic":{"type":"string","description":"Topic to write to."}},"additionalProperties":{"type":"string","description":"Options passed directly to `rdkafka`.\\n\\nSee [`librdkafka` options](https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md)\\nused to configure the Kafka producer."}},"name":{"type":"string","enum":["kafka_output"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","description":"Google Pub/Sub input connector configuration.","required":["subscription"],"properties":{"connect_timeout_seconds":{"type":"integer","format":"int32","description":"gRPC connection timeout.","nullable":true,"minimum":0},"credentials":{"type":"string","description":"The content of a Google Cloud credentials JSON file.\\n\\nWhen this option is specified, the connector will use the provided credentials for\\nauthentication.  Otherwise, it will use Application Default Credentials (ADC) configured\\nin the environment where the Feldera service is running.  See\\n[Google Cloud documentation](https://cloud.google.com/docs/authentication/provide-credentials-adc)\\nfor information on configuring application default credentials.\\n\\nWhen running Feldera in an environment where ADC are not configured,\\ne.g., a Docker container, use this option to ship Google Cloud credentials from another environment.\\nFor example, if you use the\\n[`gcloud auth application-default login`](https://cloud.google.com/pubsub/docs/authentication#client-libs)\\ncommand for authentication in your local development environment, ADC are stored in the\\n`.config/gcloud/application_default_credentials.json` file in your home directory.","nullable":true},"emulator":{"type":"string","description":"Set in order to use a Pub/Sub [emulator](https://cloud.google.com/pubsub/docs/emulator)\\ninstead of the production service, e.g., \'localhost:8681\'.","nullable":true},"endpoint":{"type":"string","description":"Override the default service endpoint \'pubsub.googleapis.com\'","nullable":true},"pool_size":{"type":"integer","format":"int32","description":"gRPC channel pool size.","nullable":true,"minimum":0},"project_id":{"type":"string","description":"Google Cloud project_id.\\n\\nWhen not specified, the connector will use the project id associated\\nwith the authenticated account.","nullable":true},"snapshot":{"type":"string","description":"Reset subscription\'s backlog to a given snapshot on startup,\\nusing the Pub/Sub `Seek` API.\\n\\nThis option is mutually exclusive with the `timestamp` option.","nullable":true},"subscription":{"type":"string","description":"Subscription name."},"timeout_seconds":{"type":"integer","format":"int32","description":"gRPC request timeout.","nullable":true,"minimum":0},"timestamp":{"type":"string","description":"Reset subscription\'s backlog to a given timestamp on startup,\\nusing the Pub/Sub `Seek` API.\\n\\nThe value of this option is an ISO 8601-encoded UTC time, e.g., \\"2024-08-17T16:39:57-08:00\\".\\n\\nThis option is mutually exclusive with the `snapshot` option.","nullable":true}}},"name":{"type":"string","enum":["pub_sub_input"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","description":"Configuration for reading data from an HTTP or HTTPS URL with\\n`UrlInputTransport`.","required":["path"],"properties":{"path":{"type":"string","description":"URL."},"pause_timeout":{"type":"integer","format":"int32","description":"Timeout before disconnection when paused, in seconds.\\n\\nIf the pipeline is paused, or if the input adapter reads data faster\\nthan the pipeline can process it, then the controller will pause the\\ninput adapter. If the input adapter stays paused longer than this\\ntimeout, it will drop the network connection to the server. It will\\nautomatically reconnect when the input adapter starts running again.","minimum":0}}},"name":{"type":"string","enum":["url_input"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","description":"Configuration for reading data from AWS S3.","required":["region","bucket_name"],"properties":{"aws_access_key_id":{"type":"string","description":"AWS Access Key id. This property must be specified unless `no_sign_request` is set to `true`.","nullable":true},"aws_secret_access_key":{"type":"string","description":"Secret Access Key. This property must be specified unless `no_sign_request` is set to `true`.","nullable":true},"bucket_name":{"type":"string","description":"S3 bucket name to access."},"endpoint_url":{"type":"string","description":"The endpoint URL used to communicate with this service. Can be used to make this connector\\ntalk to non-AWS services with an S3 API.","nullable":true},"key":{"type":"string","description":"Read a single object specified by a key.","nullable":true},"max_concurrent_fetches":{"type":"integer","format":"int32","description":"Controls the number of S3 objects fetched in parallel.\\n\\nIncreasing this value can improve throughput by enabling greater concurrency.\\nHowever, higher concurrency may lead to timeouts or increased memory usage due to in-memory buffering.\\n\\nRecommended range: 1\u201310. Default: 8.","minimum":0},"no_sign_request":{"type":"boolean","description":"Do not sign requests. This is equivalent to the `--no-sign-request` flag in the AWS CLI."},"prefix":{"type":"string","description":"Read all objects whose keys match a prefix. Set to an empty string to read all objects in the bucket.","nullable":true},"region":{"type":"string","description":"AWS region."}}},"name":{"type":"string","enum":["s3_input"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","description":"Delta table input connector configuration.","required":["uri","mode"],"properties":{"cdc_delete_filter":{"type":"string","description":"A predicate that determines whether the record represents a deletion.\\n\\nThis setting is only valid in the `cdc` mode. It specifies a predicate applied to\\neach row in the Delta table to determine whether the row represents a deletion event.\\nIts value must be a valid Boolean SQL expression that can be used in a query of the\\nform `SELECT * from <table> WHERE <cdc_delete_filter>`.","nullable":true},"cdc_order_by":{"type":"string","description":"An expression that determines the ordering of updates in the Delta table.\\n\\nThis setting is only valid in the `cdc` mode. It specifies a predicate applied to\\neach row in the Delta table to determine the order in which updates in the table should\\nbe applied. Its value must be a valid SQL expression that can be used in a query of the\\nform `SELECT * from <table> ORDER BY <cdc_order_by>`.","nullable":true},"datetime":{"type":"string","description":"Optional timestamp for the snapshot in the ISO-8601/RFC-3339 format, e.g.,\\n\\"2024-12-09T16:09:53+00:00\\".\\n\\nWhen this option is set, the connector finds and opens the version of the table as of the\\nspecified point in time (based on the server time recorded in the transaction log, not the\\nevent time encoded in the data).  In `snapshot` and `snapshot_and_follow` modes, it\\nretrieves the snapshot of this version of the table.  In `follow`, `snapshot_and_follow`, and\\n`cdc` modes, it follows transaction log records **after** this version.\\n\\nNote: at most one of `version` and `datetime` options can be specified.\\nWhen neither of the two options is specified, the latest committed version of the table\\nis used.","nullable":true},"end_version":{"type":"integer","format":"int64","description":"Optional final table version.\\n\\nValid only when the connector is configured in `follow`, `snapshot_and_follow`, or `cdc` mode.\\n\\nWhen set, the connector will stop scanning the table\u2019s transaction log after reaching this version or any greater version.\\nThis bound is inclusive: if the specified version appears in the log, it will be processed before signaling end-of-input.","nullable":true},"filter":{"type":"string","description":"Optional row filter.\\n\\nWhen specified, only rows that satisfy the filter condition are read from the delta table.\\nThe condition must be a valid SQL Boolean expression that can be used in\\nthe `where` clause of the `select * from my_table where ...` query.","nullable":true},"max_concurrent_readers":{"type":"integer","format":"int32","description":"Maximum number of concurrent object store reads performed by all Delta Lake connectors.\\n\\nThis setting is used to limit the number of concurrent reads of the object store in a\\npipeline with a large number of Delta Lake connectors. When multiple connectors are simultaneously\\nreading from the object store, this can lead to transport timeouts.\\n\\nWhen enabled, this setting limits the number of concurrent reads across all connectors.\\nThis is a global setting that affects all Delta Lake connectors, and not just the connector\\nwhere it is specified. It should therefore be used at most once in a pipeline.  If multiple\\nconnectors specify this setting, they must all use the same value.\\n\\nThe default value is 6.","nullable":true,"minimum":0},"mode":{"type":"string","description":"Delta table read mode.\\n\\nThree options are available:\\n\\n* `snapshot` - read a snapshot of the table and stop.\\n\\n* `follow` - continuously ingest changes to the table, starting from a specified version\\nor timestamp.\\n\\n* `snapshot_and_follow` - read a snapshot of the table before switching to continuous ingestion\\nmode.","enum":["snapshot","follow","snapshot_and_follow","cdc"]},"num_parsers":{"type":"integer","format":"int32","description":"The number of parallel parsing tasks the connector uses to process data read from the\\ntable. Increasing this value can enhance performance by allowing more concurrent processing.\\nRecommended range: 1\u201310. The default is 4.","minimum":0},"skip_unused_columns":{"type":"boolean","description":"Don\'t read unused columns from the Delta table.\\n\\nWhen set to `true`, this option instructs the connector to avoid reading\\ncolumns from the Delta table that are not used in any view definitions.\\nTo be skipped, the columns must be either nullable or have default\\nvalues. This can improve ingestion performance, especially for wide\\ntables.\\n\\nNote: The simplest way to exclude unused columns is to omit them from the Feldera SQL table\\ndeclaration. The connector never reads columns that aren\'t declared in the SQL schema.\\nAdditionally, the SQL compiler emits warnings for declared but unused columns\u2014use these as\\na guide to optimize your schema."},"snapshot_filter":{"type":"string","description":"Optional snapshot filter.\\n\\nThis option is only valid when `mode` is set to `snapshot` or `snapshot_and_follow`.\\n\\nWhen specified, only rows that satisfy the filter condition are included in the\\nsnapshot.  The condition must be a valid SQL Boolean expression that can be used in\\nthe `where` clause of the `select * from snapshot where ...` query.\\n\\nUnlike the `filter` option, which applies to all records retrieved from the table, this\\nfilter only applies to rows in the initial snapshot of the table.\\nFor instance, it can be used to specify the range of event times to include in the snapshot,\\ne.g.: `ts BETWEEN TIMESTAMP \'2005-01-01 00:00:00\' AND TIMESTAMP \'2010-12-31 23:59:59\'`.\\n\\nThis option can be used together with the `filter` option. During the initial snapshot,\\nonly rows that satisfy both `filter` and `snapshot_filter` are retrieved from the Delta table.\\nWhen subsequently following changes in the the transaction log (`mode = snapshot_and_follow`),\\nall rows that meet the `filter` condition are ingested, regardless of `snapshot_filter`.","nullable":true},"timestamp_column":{"type":"string","description":"Table column that serves as an event timestamp.\\n\\nWhen this option is specified, and `mode` is one of `snapshot` or `snapshot_and_follow`,\\ntable rows are ingested in the timestamp order, respecting the\\n[`LATENESS`](https://docs.feldera.com/sql/streaming#lateness-expressions)\\nproperty of the column: each ingested row has a timestamp no more than `LATENESS`\\ntime units earlier than the most recent timestamp of any previously ingested row.\\nThe ingestion is performed by partitioning the table into timestamp ranges of width\\n`LATENESS`. Each range is processed sequentially, in increasing timestamp order.\\n\\n# Example\\n\\nConsider a table with timestamp column of type `TIMESTAMP` and lateness attribute\\n`INTERVAL 1 DAY`. Assuming that the oldest timestamp in the table is\\n`2024-01-01T00:00:00``, the connector will fetch all records with timestamps\\nfrom `2024-01-01`, then all records for `2024-01-02`, `2024-01-03`, etc., until all records\\nin the table have been ingested.\\n\\n# Requirements\\n\\n* The timestamp column must be of a supported type: integer, `DATE`, or `TIMESTAMP`.\\n* The timestamp column must be declared with non-zero `LATENESS`.\\n* For efficient ingest, the table must be optimized for timestamp-based\\nqueries using partitioning, Z-ordering, or liquid clustering.","nullable":true},"uri":{"type":"string","description":"Table URI.\\n\\nExample: \\"s3://feldera-fraud-detection-data/demographics_train\\""},"version":{"type":"integer","format":"int64","description":"Optional table version.\\n\\nWhen this option is set, the connector finds and opens the specified version of the table.\\nIn `snapshot` and `snapshot_and_follow` modes, it retrieves the snapshot of this version of\\nthe table.  In `follow`, `snapshot_and_follow`, and `cdc` modes, it follows transaction log records\\n**after** this version.\\n\\nNote: at most one of `version` and `datetime` options can be specified.\\nWhen neither of the two options is specified, the latest committed version of the table\\nis used.","nullable":true}},"additionalProperties":{"type":"string","description":"Storage options for configuring backend object store.\\n\\nFor specific options available for different storage backends, see:\\n* [Azure options](https://docs.rs/object_store/latest/object_store/azure/enum.AzureConfigKey.html)\\n* [Amazon S3 options](https://docs.rs/object_store/latest/object_store/aws/enum.AmazonS3ConfigKey.html)\\n* [Google Cloud Storage options](https://docs.rs/object_store/latest/object_store/gcp/enum.GoogleConfigKey.html)"}},"name":{"type":"string","enum":["delta_table_input"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","description":"Delta table output connector configuration.","required":["uri"],"properties":{"mode":{"type":"string","description":"Delta table write mode.\\n\\nDetermines how the Delta table connector handles an existing table at the target location.","enum":["append","truncate","error_if_exists"]},"uri":{"type":"string","description":"Table URI."}},"additionalProperties":{"type":"string","description":"Storage options for configuring backend object store.\\n\\nFor specific options available for different storage backends, see:\\n* [Azure options](https://docs.rs/object_store/latest/object_store/azure/enum.AzureConfigKey.html)\\n* [Amazon S3 options](https://docs.rs/object_store/latest/object_store/aws/enum.AmazonS3ConfigKey.html)\\n* [Google Cloud Storage options](https://docs.rs/object_store/latest/object_store/gcp/enum.GoogleConfigKey.html)"}},"name":{"type":"string","enum":["delta_table_output"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","description":"Redis output connector configuration.","required":["connection_string"],"properties":{"connection_string":{"type":"string","description":"The URL format: `redis://[<username>][:<password>@]<hostname>[:port][/[<db>][?protocol=<protocol>]]`\\nThis is parsed by the [redis](https://docs.rs/redis/latest/redis/#connection-parameters) crate."},"key_separator":{"type":"string","description":"Separator used to join multiple components into a single key.\\n\\":\\" by default."}}},"name":{"type":"string","enum":["redis_output"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"description":"Iceberg input connector configuration.","type":"object","properties":{"glue.access-key-id":{"type":"string","description":"Access key id used to access the Glue catalog.","nullable":true},"glue.endpoint":{"type":"string","description":"Configure an alternative endpoint of the Glue service for Glue catalog to access.\\n\\nExample: `\\"https://glue.us-east-1.amazonaws.com\\"`","nullable":true},"glue.id":{"type":"string","description":"The 12-digit ID of the Glue catalog.","nullable":true},"glue.profile-name":{"type":"string","description":"Profile used to access the Glue catalog.","nullable":true},"glue.region":{"type":"string","description":"Region of the Glue catalog.","nullable":true},"glue.secret-access-key":{"type":"string","description":"Secret access key used to access the Glue catalog.","nullable":true},"glue.session-token":{"type":"string","nullable":true},"glue.warehouse":{"type":"string","description":"Location for table metadata.\\n\\nExample: `\\"s3://my-data-warehouse/tables/\\"`","nullable":true},"rest.audience":{"type":"string","description":"Logical name of target resource or service.","nullable":true},"rest.credential":{"type":"string","description":"Credential to use for OAuth2 credential flow when initializing the catalog.\\n\\nA key and secret pair separated by \\":\\" (key is optional).","nullable":true},"rest.headers":{"type":"array","items":{"type":"array","items":{"type":"string"}},"description":"Additional HTTP request headers added to each catalog REST API call.","nullable":true},"rest.oauth2-server-uri":{"type":"string","description":"Authentication URL to use for client credentials authentication (default: uri + \'v1/oauth/tokens\')","nullable":true},"rest.prefix":{"type":"string","description":"Customize table storage paths.\\n\\nWhen combined with the `warehouse` property, the prefix determines\\nhow table data is organized within the storage.","nullable":true},"rest.resource":{"type":"string","description":"URI for the target resource or service.","nullable":true},"rest.scope":{"type":"string","nullable":true},"rest.token":{"type":"string","description":"Bearer token value to use for `Authorization` header.","nullable":true},"rest.uri":{"type":"string","description":"URI identifying the REST catalog server.","nullable":true},"rest.warehouse":{"type":"string","description":"The default location for managed tables created by the catalog.","nullable":true},"catalog_type":{"nullable":true,"type":"string","enum":["rest","glue"]},"datetime":{"type":"string","description":"Optional timestamp for the snapshot in the ISO-8601/RFC-3339 format, e.g.,\\n\\"2024-12-09T16:09:53+00:00\\".\\n\\nWhen this option is set, the connector finds and opens the snapshot of the table as of the\\nspecified point in time (based on the server time recorded in the transaction\\nlog, not the event time encoded in the data).  In `snapshot` and `snapshot_and_follow`\\nmodes, it retrieves this snapshot.  In `follow` and `snapshot_and_follow` modes, it\\nfollows transaction log records **after** this snapshot.\\n\\nNote: at most one of `snapshot_id` and `datetime` options can be specified.\\nWhen neither of the two options is specified, the latest committed version of the table\\nis used.","nullable":true},"metadata_location":{"type":"string","description":"Location of the table metadata JSON file.\\n\\nThis propery is used to access an Iceberg table without a catalog. It is mutually\\nexclusive with the `catalog_type` property.","nullable":true},"mode":{"type":"string","description":"Iceberg table read mode.\\n\\nThree options are available:\\n\\n* `snapshot` - read a snapshot of the table and stop.\\n\\n* `follow` - continuously ingest changes to the table, starting from a specified snapshot\\nor timestamp.\\n\\n* `snapshot_and_follow` - read a snapshot of the table before switching to continuous ingestion\\nmode.","enum":["snapshot","follow","snapshot_and_follow"]},"snapshot_filter":{"type":"string","description":"Optional row filter.\\n\\nThis option is only valid when `mode` is set to `snapshot` or `snapshot_and_follow`.\\n\\nWhen specified, only rows that satisfy the filter condition are included in the\\nsnapshot.  The condition must be a valid SQL Boolean expression that can be used in\\nthe `where` clause of the `select * from snapshot where ...` query.\\n\\nThis option can be used to specify the range of event times to include in the snapshot,\\ne.g.: `ts BETWEEN \'2005-01-01 00:00:00\' AND \'2010-12-31 23:59:59\'`.","nullable":true},"snapshot_id":{"type":"integer","format":"int64","description":"Optional snapshot id.\\n\\nWhen this option is set, the connector finds the specified snapshot of the table.\\nIn `snapshot` and `snapshot_and_follow` modes, it loads this snapshot.\\nIn `follow` and `snapshot_and_follow` modes, it follows table updates\\n**after** this snapshot.\\n\\nNote: at most one of `snapshot_id` and `datetime` options can be specified.\\nWhen neither of the two options is specified, the latest committed version of the table\\nis used.","nullable":true},"table_name":{"type":"string","description":"Specifies the Iceberg table name in the \\"namespace.table\\" format.\\n\\nThis option is applicable when an Iceberg catalog is configured using the `catalog_type` property.","nullable":true},"timestamp_column":{"type":"string","description":"Table column that serves as an event timestamp.\\n\\nWhen this option is specified, and `mode` is one of `snapshot` or `snapshot_and_follow`,\\ntable rows are ingested in the timestamp order, respecting the\\n[`LATENESS`](https://docs.feldera.com/sql/streaming#lateness-expressions)\\nproperty of the column: each ingested row has a timestamp no more than `LATENESS`\\ntime units earlier than the most recent timestamp of any previously ingested row.\\nThe ingestion is performed by partitioning the table into timestamp ranges of width\\n`LATENESS`. Each range is processed sequentially, in increasing timestamp order.\\n\\n# Example\\n\\nConsider a table with timestamp column of type `TIMESTAMP` and lateness attribute\\n`INTERVAL 1 DAY`. Assuming that the oldest timestamp in the table is\\n`2024-01-01T00:00:00``, the connector will fetch all records with timestamps\\nfrom `2024-01-01`, then all records for `2024-01-02`, `2024-01-03`, etc., until all records\\nin the table have been ingested.\\n\\n# Requirements\\n\\n* The timestamp column must be of a supported type: integer, `DATE`, or `TIMESTAMP`.\\n* The timestamp column must be declared with non-zero `LATENESS`.\\n* For efficient ingest, the table must be optimized for timestamp-based\\nqueries using partitioning, Z-ordering, or liquid clustering.","nullable":true}},"required":["mode"],"additionalProperties":{"type":"string","description":"Storage options for configuring backend object store.\\n\\nSee the [list of available options in PyIceberg documentation](https://py.iceberg.apache.org/configuration/#fileio)."}},"name":{"type":"string","enum":["iceberg_input"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","description":"Postgres input connector configuration.","required":["uri","query"],"properties":{"query":{"type":"string","description":"Query that specifies what data to fetch from postgres."},"uri":{"type":"string","description":"Postgres URI.\\nSee: <https://docs.rs/tokio-postgres/0.7.12/tokio_postgres/config/struct.Config.html>"}}},"name":{"type":"string","enum":["postgres_input"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","description":"Postgres output connector configuration.","required":["uri","table"],"properties":{"table":{"type":"string","description":"The table to write the output to."},"uri":{"type":"string","description":"Postgres URI.\\nSee: <https://docs.rs/tokio-postgres/0.7.12/tokio_postgres/config/struct.Config.html>"}}},"name":{"type":"string","enum":["postgres_output"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","description":"Configuration for generating random data for a table.","properties":{"plan":{"type":"array","items":{"type":"object","description":"A random generation plan for a table that generates either a limited amount of rows or runs continuously.","properties":{"fields":{"type":"object","description":"Specifies the values that the generator should produce.","default":{},"additionalProperties":{"type":"object","description":"Configuration for generating random data for a field of a table.","properties":{"e":{"type":"integer","format":"int64","description":"The frequency rank exponent for the Zipf distribution.\\n\\n- This value is only used if the strategy is set to `Zipf`.\\n- The default value is 1.0.","default":1},"fields":{"type":"object","description":"Specifies the values that the generator should produce in case the field is a struct type.","default":null,"additionalProperties":{"$ref":"#/components/schemas/RngFieldSettings"},"nullable":true},"key":{"default":null,"nullable":true,"$ref":"#/components/schemas/RngFieldSettings"},"null_percentage":{"type":"integer","description":"Percentage of records where this field should be set to NULL.\\n\\nIf not set, the generator will produce only records with non-NULL values.\\nIf set to `1..=100`, the generator will produce records with NULL values with the specified percentage.","default":null,"nullable":true,"minimum":0},"range":{"type":"object","description":"An optional, exclusive range [a, b) to limit the range of values the generator should produce.\\n\\n- For integer/floating point types specifies min/max values as an integer.\\nIf not set, the generator will produce values for the entire range of the type for number types.\\n- For string/binary types specifies min/max length as an integer, values are required to be >=0.\\nIf not set, a range of [0, 25) is used by default.\\n- For timestamp types specifies the min/max as two strings in the RFC 3339 format\\n(e.g., [\\"2021-01-01T00:00:00Z\\", \\"2022-01-02T00:00:00Z\\"]).\\nAlternatively, the range values can be specified as a number of non-leap\\nmilliseconds since January 1, 1970 0:00:00.000 UTC (aka \u201cUNIX timestamp\u201d).\\nIf not set, a range of [\\"1970-01-01T00:00:00Z\\", \\"2100-01-01T00:00:00Z\\") or [0, 4102444800000)\\nis used by default.\\n- For time types specifies the min/max as two strings in the \\"HH:MM:SS\\" format.\\nAlternatively, the range values can be specified in milliseconds as two positive integers.\\nIf not set, the range is 24h.\\n- For date types, the min/max range is specified as two strings in the \\"YYYY-MM-DD\\" format.\\nAlternatively, two integers that represent number of days since January 1, 1970 can be used.\\nIf not set, a range of [\\"1970-01-01\\", \\"2100-01-01\\") or [0, 54787) is used by default.\\n- For array types specifies the min/max number of elements as an integer.\\nIf not set, a range of [0, 5) is used by default. Range values are required to be >=0.\\n- For map types specifies the min/max number of key-value pairs as an integer.\\nIf not set, a range of [0, 5) is used by default.\\n- For struct/boolean/null types `range` is ignored."},"scale":{"type":"integer","format":"int64","description":"A scale factor to apply a multiplier to the generated value.\\n\\n- For integer/floating point types, the value is multiplied by the scale factor.\\n- For timestamp types, the generated value (milliseconds) is multiplied by the scale factor.\\n- For time types, the generated value (milliseconds) is multiplied by the scale factor.\\n- For date types, the generated value (days) is multiplied by the scale factor.\\n- For string/binary/array/map/struct/boolean/null types, the scale factor is ignored.\\n\\n- If `values` is specified, the scale factor is ignored.\\n- If `range` is specified and the range is required to be positive (struct, map, array etc.)\\nthe scale factor is required to be positive too.\\n\\nThe default scale factor is 1.","default":1},"strategy":{"default":"increment","type":"string","description":"Strategy used to generate values.","enum":["increment","uniform","zipf","word","words","sentence","sentences","paragraph","paragraphs","first_name","last_name","title","suffix","name","name_with_title","domain_suffix","email","username","password","field","position","seniority","job_title","ipv4","ipv6","ip","mac_address","user_agent","rfc_status_code","valid_status_code","company_suffix","company_name","buzzword","buzzword_middle","buzzword_tail","catch_phrase","bs_verb","bs_adj","bs_noun","bs","profession","industry","currency_code","currency_name","currency_symbol","credit_card_number","city_prefix","city_suffix","city_name","country_name","country_code","street_suffix","street_name","time_zone","state_name","state_abbr","secondary_address_type","secondary_address","zip_code","post_code","building_number","latitude","longitude","isbn","isbn13","isbn10","phone_number","cell_number","file_path","file_name","file_extension","dir_path"]},"value":{"default":null,"nullable":true,"$ref":"#/components/schemas/RngFieldSettings"},"values":{"type":"array","items":{"type":"object"},"description":"An optional set of values the generator will pick from.\\n\\nIf set, the generator will pick values from the specified set.\\nIf not set, the generator will produce values according to the specified range.\\nIf set to an empty set, the generator will produce NULL values.\\nIf set to a single value, the generator will produce only that value.\\n\\nNote that `range` is ignored if `values` is set.","default":null,"nullable":true}},"additionalProperties":false}},"limit":{"type":"integer","description":"Total number of new rows to generate.\\n\\nIf not set, the generator will produce new/unique records as long as the pipeline is running.\\nIf set to 0, the table will always remain empty.\\nIf set, the generator will produce new records until the specified limit is reached.\\n\\nNote that if the table has one or more primary keys that don\'t use the `increment` strategy to\\ngenerate the key there is a potential that an update is generated instead of an insert. In\\nthis case it\'s possible the total number of records is less than the specified limit.","default":null,"nullable":true,"minimum":0},"rate":{"type":"integer","format":"int32","description":"Non-zero number of rows to generate per second.\\n\\nIf not set, the generator will produce rows as fast as possible.","default":null,"nullable":true,"minimum":0},"worker_chunk_size":{"type":"integer","description":"When multiple workers are used, each worker will pick a consecutive \\"chunk\\" of\\nrecords to generate.\\n\\nBy default, if not specified, the generator will use the formula `min(rate, 10_000)`\\nto determine it. This works well in most situations. However, if you\'re\\nrunning tests with lateness and many workers you can e.g., reduce the\\nchunk size to make sure a smaller range of records is being ingested in parallel.\\n\\n# Example\\nAssume you generate a total of 125 records with 4 workers and a chunk size of 25.\\nIn this case, worker A will generate records 0..25, worker B will generate records 25..50,\\netc. A, B, C, and D will generate records in parallel. The first worker to finish its chunk\\nwill pick up the last chunk of records (100..125) to generate.","default":null,"nullable":true,"minimum":0}},"additionalProperties":false},"description":"The sequence of generations to perform.\\n\\nIf not set, the generator will produce a single sequence with default settings.\\nIf set, the generator will produce the specified sequences in sequential order.\\n\\nNote that if one of the sequences before the last one generates an unlimited number of rows\\nthe following sequences will not be executed.","default":[{"rate":null,"limit":null,"worker_chunk_size":null,"fields":{}}]},"seed":{"type":"integer","format":"int64","description":"Optional seed for the random generator.\\n\\nSetting this to a fixed value will make the generator produce the same sequence of records\\nevery time the pipeline is run.\\n\\n# Notes\\n- To ensure the set of generated input records is deterministic across multiple runs,\\napart from setting a seed, `workers` also needs to remain unchanged.\\n- The input will arrive in non-deterministic order if `workers > 1`.","default":null,"nullable":true,"minimum":0},"workers":{"type":"integer","description":"Number of workers to use for generating data.","default":1,"minimum":0}},"additionalProperties":false},"name":{"type":"string","enum":["datagen"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","description":"Configuration for generating Nexmark input data.\\n\\nThis connector must be used exactly three times in a pipeline if it is used\\nat all, once for each [`NexmarkTable`].","required":["table"],"properties":{"options":{"nullable":true,"type":"object","description":"Configuration for generating Nexmark input data.","properties":{"batch_size_per_thread":{"type":"integer","format":"int64","description":"Number of events to generate and submit together, per thread.\\n\\nEach thread generates this many records, which are then combined with\\nthe records generated by the other threads, to form combined input\\nbatches of size `threads \xd7 batch_size_per_thread`.","default":1000,"minimum":0},"events":{"type":"integer","format":"int64","description":"Number of events to generate.","default":100000000,"minimum":0},"max_step_size_per_thread":{"type":"integer","format":"int64","description":"Maximum number of events to submit in a single step, per thread.\\n\\nThis should really be per worker thread, not per generator thread, but\\nthe connector does not know how many worker threads there are.\\n\\nThis stands in for `max_batch_size` from the connector configuration\\nbecause it must be a constant across all three of the nexmark tables.","default":10000,"minimum":0},"threads":{"type":"integer","description":"Number of event generator threads.\\n\\nIt\'s reasonable to choose the same number of generator threads as worker\\nthreads.","default":4,"minimum":0}}},"table":{"type":"string","description":"Table in Nexmark.","enum":["bid","auction","person"]}}},"name":{"type":"string","enum":["nexmark"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","description":"Configuration for reading data via HTTP.\\n\\nHTTP input adapters cannot be usefully configured as part of pipeline\\nconfiguration.  Instead, instantiate them through the REST API as\\n`/pipelines/{pipeline_name}/ingress/{table_name}`.","required":["name"],"properties":{"name":{"type":"string","description":"Autogenerated name."}}},"name":{"type":"string","enum":["http_input"]}}},{"type":"object","required":["name"],"properties":{"name":{"type":"string","enum":["http_output"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","description":"Configuration for inserting data with ad-hoc queries\\n\\nAn ad-hoc input adapters cannot be usefully configured as part of pipeline\\nconfiguration.  Instead, use ad-hoc queries through the UI, the REST API, or\\nthe `fda` command-line tool.","required":["name"],"properties":{"name":{"type":"string","description":"Autogenerated name."}}},"name":{"type":"string","enum":["ad_hoc_input"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","required":["clock_resolution_usecs"],"properties":{"clock_resolution_usecs":{"type":"integer","format":"int64","minimum":0}}},"name":{"type":"string","enum":["clock_input"]}}}],"description":"Transport-specific endpoint configuration passed to\\n`crate::OutputTransport::new_endpoint`\\nand `crate::InputTransport::new_endpoint`.","discriminator":{"propertyName":"name"}}}}},"schema":{"type":"object","description":"A struct containing the tables (inputs) and views for a program.\\n\\nParse from the JSON data-type of the DDL generated by the SQL compiler.","required":["inputs","outputs"],"properties":{"inputs":{"type":"array","items":{"description":"A SQL table or view. It has a name and a list of fields.\\n\\nMatches the Calcite JSON format.","type":"object","required":["name","case_sensitive","fields"],"properties":{"case_sensitive":{"type":"boolean"},"name":{"type":"string"},"fields":{"type":"array","items":{"description":"A SQL field.\\n\\nMatches the SQL compiler JSON format.","type":"object","required":["name","case_sensitive","columntype","unused"],"properties":{"case_sensitive":{"type":"boolean"},"name":{"type":"string"},"columntype":{"$ref":"#/components/schemas/ColumnType"},"default":{"type":"string","nullable":true},"lateness":{"type":"string","nullable":true},"unused":{"type":"boolean"},"watermark":{"type":"string","nullable":true}}}},"materialized":{"type":"boolean"},"properties":{"type":"object","additionalProperties":{"type":"object","required":["value","key_position","value_position"],"properties":{"key_position":{"type":"object","required":["start_line_number","start_column","end_line_number","end_column"],"properties":{"end_column":{"type":"integer","minimum":0},"end_line_number":{"type":"integer","minimum":0},"start_column":{"type":"integer","minimum":0},"start_line_number":{"type":"integer","minimum":0}}},"value":{"type":"string"},"value_position":{"type":"object","required":["start_line_number","start_column","end_line_number","end_column"],"properties":{"end_column":{"type":"integer","minimum":0},"end_line_number":{"type":"integer","minimum":0},"start_column":{"type":"integer","minimum":0},"start_line_number":{"type":"integer","minimum":0}}}}}}}}},"outputs":{"type":"array","items":{"description":"A SQL table or view. It has a name and a list of fields.\\n\\nMatches the Calcite JSON format.","type":"object","required":["name","case_sensitive","fields"],"properties":{"case_sensitive":{"type":"boolean"},"name":{"type":"string"},"fields":{"type":"array","items":{"description":"A SQL field.\\n\\nMatches the SQL compiler JSON format.","type":"object","required":["name","case_sensitive","columntype","unused"],"properties":{"case_sensitive":{"type":"boolean"},"name":{"type":"string"},"columntype":{"$ref":"#/components/schemas/ColumnType"},"default":{"type":"string","nullable":true},"lateness":{"type":"string","nullable":true},"unused":{"type":"boolean"},"watermark":{"type":"string","nullable":true}}}},"materialized":{"type":"boolean"},"properties":{"type":"object","additionalProperties":{"type":"object","required":["value","key_position","value_position"],"properties":{"key_position":{"type":"object","required":["start_line_number","start_column","end_line_number","end_column"],"properties":{"end_column":{"type":"integer","minimum":0},"end_line_number":{"type":"integer","minimum":0},"start_column":{"type":"integer","minimum":0},"start_line_number":{"type":"integer","minimum":0}}},"value":{"type":"string"},"value_position":{"type":"object","required":["start_line_number","start_column","end_line_number","end_column"],"properties":{"end_column":{"type":"integer","minimum":0},"end_line_number":{"type":"integer","minimum":0},"start_column":{"type":"integer","minimum":0},"start_line_number":{"type":"integer","minimum":0}}}}}}}}}}},"udf_stubs":{"type":"string","description":"Generated user defined function (UDF) stubs Rust code: stubs.rs"}}},"program_status":{"type":"string","description":"Program compilation status.","enum":["Pending","CompilingSql","SqlCompiled","CompilingRust","Success","SqlError","RustError","SystemError"]},"program_status_since":{"type":"string","format":"date-time"},"program_version":{"type":"integer","format":"int64","description":"Version number."},"refresh_version":{"type":"integer","format":"int64","description":"Version number."},"runtime_config":{"type":"object","description":"Global pipeline configuration settings. This is the publicly\\nexposed type for users to configure pipelines.","properties":{"checkpoint_during_suspend":{"type":"boolean","description":"Deprecated: setting this true or false does not have an effect anymore.","default":true},"clock_resolution_usecs":{"type":"integer","format":"int64","description":"Real-time clock resolution in microseconds.\\n\\nThis parameter controls the execution of queries that use the `NOW()` function.  The output of such\\nqueries depends on the real-time clock and can change over time without any external\\ninputs.  The pipeline will update the clock value and trigger incremental recomputation\\nat most each `clock_resolution_usecs` microseconds.\\n\\nIt is set to 1 second (1,000,000 microseconds) by default.\\n\\nSet to `null` to disable periodic clock updates.","default":1000000,"nullable":true,"minimum":0},"cpu_profiler":{"type":"boolean","description":"Enable CPU profiler.\\n\\nThe default value is `true`.","default":true},"dev_tweaks":{"type":"object","description":"Optional settings for tweaking Feldera internals.\\n\\nThe available key-value pairs change from one version of Feldera to\\nanother, so users should not depend on particular settings being\\navailable, or on their behavior.","default":{},"additionalProperties":{}},"fault_tolerance":{"default":{"model":"none","checkpoint_interval_secs":60},"type":"object","description":"Fault-tolerance configuration.\\n\\nThe default [FtConfig] (via [FtConfig::default]) disables fault tolerance,\\nwhich is the configuration that one gets if [RuntimeConfig] omits fault\\ntolerance configuration.\\n\\nThe default value for [FtConfig::model] enables fault tolerance, as\\n`Some(FtModel::default())`.  This is the configuration that one gets if\\n[RuntimeConfig] includes a fault tolerance configuration but does not\\nspecify a particular model.","properties":{"checkpoint_interval_secs":{"type":"integer","format":"int64","description":"Interval between automatic checkpoints, in seconds.\\n\\nThe default is 60 seconds.  Values less than 1 or greater than 3600 will\\nbe forced into that range.","nullable":true,"minimum":0},"model":{"oneOf":[{"type":"string","description":"Fault tolerance model.\\n\\nThe ordering is significant: we consider [Self::ExactlyOnce] to be a \\"higher\\nlevel\\" of fault tolerance than [Self::AtLeastOnce].","enum":["at_least_once","exactly_once"]},{"type":"string","enum":["none"]}],"default":"exactly_once"}}},"http_workers":{"type":"integer","format":"int64","description":"Sets the number of available runtime threads for the http server.\\n\\nIn most cases, this does not need to be set explicitly and\\nthe default is sufficient. Can be increased in case the\\npipeline HTTP API operations are a bottleneck.\\n\\nIf not specified, the default is set to `workers`.","default":null,"nullable":true,"minimum":0},"init_containers":{"description":"Specification of additional (sidecar) containers.","nullable":true},"io_workers":{"type":"integer","format":"int64","description":"Sets the number of available runtime threads for async IO tasks.\\n\\nThis affects some networking and file I/O operations\\nespecially adapters and ad-hoc queries.\\n\\nIn most cases, this does not need to be set explicitly and\\nthe default is sufficient. Can be increased in case\\ningress, egress or ad-hoc queries are a bottleneck.\\n\\nIf not specified, the default is set to `workers`.","default":null,"nullable":true,"minimum":0},"logging":{"type":"string","description":"Log filtering directives.\\n\\nIf set to a valid [tracing-subscriber] filter, this controls the log\\nmessages emitted by the pipeline process.  Otherwise, or if the filter\\nhas invalid syntax, messages at \\"info\\" severity and higher are written\\nto the log and all others are discarded.\\n\\n[tracing-subscriber]: https://docs.rs/tracing-subscriber/latest/tracing_subscriber/filter/struct.EnvFilter.html#directives","default":null,"nullable":true},"max_buffering_delay_usecs":{"type":"integer","format":"int64","description":"Maximal delay in microseconds to wait for `min_batch_size_records` to\\nget buffered by the controller, defaults to 0.","default":0,"minimum":0},"max_parallel_connector_init":{"type":"integer","format":"int64","description":"The maximum number of connectors initialized in parallel during pipeline\\nstartup.\\n\\nAt startup, the pipeline must initialize all of its input and output connectors.\\nDepending on the number and types of connectors, this can take a long time.\\nTo accelerate the process, multiple connectors are initialized concurrently.\\nThis option controls the maximum number of connectors that can be initialized\\nin parallel.\\n\\nThe default is 10.","default":null,"nullable":true,"minimum":0},"min_batch_size_records":{"type":"integer","format":"int64","description":"Minimal input batch size.\\n\\nThe controller delays pushing input records to the circuit until at\\nleast `min_batch_size_records` records have been received (total\\nacross all endpoints) or `max_buffering_delay_usecs` microseconds\\nhave passed since at least one input records has been buffered.\\nDefaults to 0.","default":0,"minimum":0},"pin_cpus":{"type":"array","items":{"type":"integer","minimum":0},"description":"Optionally, a list of CPU numbers for CPUs to which the pipeline may pin\\nits worker threads.  Specify at least twice as many CPU numbers as\\nworkers.  CPUs are generally numbered starting from 0.  The pipeline\\nmight not be able to honor CPU pinning requests.\\n\\nCPU pinning can make pipelines run faster and perform more consistently,\\nas long as different pipelines running on the same machine are pinned to\\ndifferent CPUs.","default":[]},"provisioning_timeout_secs":{"type":"integer","format":"int64","description":"Timeout in seconds for the `Provisioning` phase of the pipeline.\\nSetting this value will override the default of the runner.","default":null,"nullable":true,"minimum":0},"resources":{"default":{"cpu_cores_min":null,"cpu_cores_max":null,"memory_mb_min":null,"memory_mb_max":null,"storage_mb_max":null,"storage_class":null},"type":"object","properties":{"cpu_cores_max":{"type":"integer","format":"int64","description":"The maximum number of CPU cores to reserve\\nfor an instance of this pipeline","default":null,"nullable":true,"minimum":0},"cpu_cores_min":{"type":"integer","format":"int64","description":"The minimum number of CPU cores to reserve\\nfor an instance of this pipeline","default":null,"nullable":true,"minimum":0},"memory_mb_max":{"type":"integer","format":"int64","description":"The maximum memory in Megabytes to reserve\\nfor an instance of this pipeline","default":null,"nullable":true,"minimum":0},"memory_mb_min":{"type":"integer","format":"int64","description":"The minimum memory in Megabytes to reserve\\nfor an instance of this pipeline","default":null,"nullable":true,"minimum":0},"storage_class":{"type":"string","description":"Storage class to use for an instance of this pipeline.\\nThe class determines storage performance such as IOPS and throughput.","default":null,"nullable":true},"storage_mb_max":{"type":"integer","format":"int64","description":"The total storage in Megabytes to reserve\\nfor an instance of this pipeline","default":null,"nullable":true,"minimum":0}}},"storage":{"default":{"backend":{"name":"default"},"min_storage_bytes":null,"min_step_storage_bytes":null,"compression":"default","cache_mib":null},"nullable":true,"type":"object","description":"Storage configuration for a pipeline.","properties":{"backend":{"default":{"name":"default"},"oneOf":[{"type":"object","required":["name"],"properties":{"name":{"type":"string","enum":["default"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","description":"Configuration for local file system access.","properties":{"async_threads":{"type":"boolean","description":"Whether to use background threads for file I/O.\\n\\nBackground threads should improve performance, but they can reduce\\nperformance if too few cores are available. This is provided for\\ndebugging and fine-tuning and should ordinarily be left unset.","default":null,"nullable":true},"ioop_delay":{"type":"integer","format":"int64","description":"Per-I/O operation sleep duration, in milliseconds.\\n\\nThis is for simulating slow storage devices.  Do not use this in\\nproduction.","default":null,"nullable":true,"minimum":0},"sync":{"default":null,"nullable":true,"type":"object","required":["bucket","start_from_checkpoint"],"properties":{"access_key":{"type":"string","description":"The access key used to authenticate with the storage provider.\\n\\nIf not provided, rclone will fall back to environment-based credentials, such as\\n`RCLONE_S3_ACCESS_KEY_ID`. In Kubernetes environments using IRSA (IAM Roles for Service Accounts),\\nthis can be left empty to allow automatic authentication via the pod\'s service account.","nullable":true},"bucket":{"type":"string","description":"The name of the storage bucket.\\n\\nThis may include a path to a folder inside the bucket (e.g., `my-bucket/data`)."},"endpoint":{"type":"string","description":"The endpoint URL for the storage service.\\n\\nThis is typically required for custom or local S3-compatible storage providers like MinIO.\\nExample: `http://localhost:9000`\\n\\nRelevant rclone config key: [`endpoint`](https://rclone.org/s3/#s3-endpoint)","nullable":true},"provider":{"type":"string","description":"The name of the cloud storage provider (e.g., `\\"AWS\\"`, `\\"Minio\\"`).\\n\\nUsed for provider-specific behavior in rclone.\\nIf omitted, defaults to `\\"Other\\"`.\\n\\nSee [rclone S3 provider documentation](https://rclone.org/s3/#s3-provider)","nullable":true},"region":{"type":"string","description":"The region that this bucket is in.\\n\\nLeave empty for Minio or the default region (`us-east-1` for AWS).","nullable":true},"secret_key":{"type":"string","description":"The secret key used together with the access key for authentication.\\n\\nIf not provided, rclone will fall back to environment-based credentials, such as\\n`RCLONE_S3_SECRET_ACCESS_KEY`. In Kubernetes environments using IRSA (IAM Roles for Service Accounts),\\nthis can be left empty to allow automatic authentication via the pod\'s service account.","nullable":true},"start_from_checkpoint":{"type":"boolean","description":"If `true`, will try to pull the latest checkpoint from the configured\\nobject store and resume from that point."}}}}},"name":{"type":"string","enum":["file"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","required":["url"],"properties":{"url":{"type":"string","description":"URL.\\n\\nThe following URL schemes are supported:\\n\\n* S3:\\n- `s3://<bucket>/<path>`\\n- `s3a://<bucket>/<path>`\\n- `https://s3.<region>.amazonaws.com/<bucket>`\\n- `https://<bucket>.s3.<region>.amazonaws.com`\\n- `https://ACCOUNT_ID.r2.cloudflarestorage.com/bucket`\\n* Google Cloud Storage:\\n- `gs://<bucket>/<path>`\\n* Microsoft Azure Blob Storage:\\n- `abfs[s]://<container>/<path>` (according to [fsspec](https://github.com/fsspec/adlfs))\\n- `abfs[s]://<file_system>@<account_name>.dfs.core.windows.net/<path>`\\n- `abfs[s]://<file_system>@<account_name>.dfs.fabric.microsoft.com/<path>`\\n- `az://<container>/<path>` (according to [fsspec](https://github.com/fsspec/adlfs))\\n- `adl://<container>/<path>` (according to [fsspec](https://github.com/fsspec/adlfs))\\n- `azure://<container>/<path>` (custom)\\n- `https://<account>.dfs.core.windows.net`\\n- `https://<account>.blob.core.windows.net`\\n- `https://<account>.blob.core.windows.net/<container>`\\n- `https://<account>.dfs.fabric.microsoft.com`\\n- `https://<account>.dfs.fabric.microsoft.com/<container>`\\n- `https://<account>.blob.fabric.microsoft.com`\\n- `https://<account>.blob.fabric.microsoft.com/<container>`\\n\\nSettings derived from the URL will override other settings."}},"additionalProperties":{"type":"string","description":"Additional options as key-value pairs.\\n\\nThe following keys are supported:\\n\\n* S3:\\n- `access_key_id`: AWS Access Key.\\n- `secret_access_key`: AWS Secret Access Key.\\n- `region`: Region.\\n- `default_region`: Default region.\\n- `endpoint`: Custom endpoint for communicating with S3,\\ne.g. `https://localhost:4566` for testing against a localstack\\ninstance.\\n- `token`: Token to use for requests (passed to underlying provider).\\n- [Other keys](https://docs.rs/object_store/latest/object_store/aws/enum.AmazonS3ConfigKey.html#variants).\\n* Google Cloud Storage:\\n- `service_account`: Path to the service account file.\\n- `service_account_key`: The serialized service account key.\\n- `google_application_credentials`: Application credentials path.\\n- [Other keys](https://docs.rs/object_store/latest/object_store/gcp/enum.GoogleConfigKey.html).\\n* Microsoft Azure Blob Storage:\\n- `access_key`: Azure Access Key.\\n- `container_name`: Azure Container Name.\\n- `account`: Azure Account.\\n- `bearer_token_authorization`: Static bearer token for authorizing requests.\\n- `client_id`: Client ID for use in client secret or Kubernetes federated credential flow.\\n- `client_secret`: Client secret for use in client secret flow.\\n- `tenant_id`: Tenant ID for use in client secret or Kubernetes federated credential flow.\\n- `endpoint`: Override the endpoint for communicating with blob storage.\\n- [Other keys](https://docs.rs/object_store/latest/object_store/azure/enum.AzureConfigKey.html#variants).\\n\\nOptions set through the URL take precedence over those set with these\\noptions."}},"name":{"type":"string","enum":["object"]}}}],"description":"Backend storage configuration.","discriminator":{"propertyName":"name"}},"cache_mib":{"type":"integer","description":"The maximum size of the in-memory storage cache, in MiB.\\n\\nIf set, the specified cache size is spread across all the foreground and\\nbackground threads. If unset, each foreground or background thread cache\\nis limited to 256 MiB.","default":null,"nullable":true,"minimum":0},"compression":{"default":"default","type":"string","description":"Storage compression algorithm.","enum":["default","none","snappy"]},"min_step_storage_bytes":{"type":"integer","description":"For a batch of data passed through the pipeline during a single step,\\nthe minimum estimated number of bytes to write it to storage.\\n\\nThis is provided for debugging and fine-tuning and should ordinarily be\\nleft unset.  A value of 0 will write even empty batches to storage, and\\nnonzero values provide a threshold.  `usize::MAX`, the default,\\neffectively disables storage for such batches.  If it is set to another\\nvalue, it should ordinarily be greater than or equal to\\n`min_storage_bytes`.","default":null,"nullable":true,"minimum":0},"min_storage_bytes":{"type":"integer","description":"For a batch of data maintained as part of a persistent index during a\\npipeline run, the minimum estimated number of bytes to write it to\\nstorage.\\n\\nThis is provided for debugging and fine-tuning and should ordinarily be\\nleft unset.\\n\\nA value of 0 will write even empty batches to storage, and nonzero\\nvalues provide a threshold.  `usize::MAX` would effectively disable\\nstorage for such batches.  The default is 1,048,576 (1 MiB).","default":null,"nullable":true,"minimum":0}}},"tracing":{"type":"boolean","description":"Enable pipeline tracing.","default":false},"tracing_endpoint_jaeger":{"type":"string","description":"Jaeger tracing endpoint to send tracing information to.","default":"127.0.0.1:6831"},"workers":{"type":"integer","format":"int32","description":"Number of DBSP worker threads.\\n\\nEach DBSP \\"foreground\\" worker thread is paired with a \\"background\\"\\nthread for LSM merging, making the total number of threads twice the\\nspecified number.\\n\\nThe typical sweet spot for the number of workers is between 4 and 16.\\nEach worker increases overall memory consumption for data structures\\nused during a step.","default":8,"minimum":0}}},"storage_status":{"type":"string","description":"Storage status.\\n\\nThe storage status can only transition when the pipeline status is `Stopped`.\\n\\n```text\\nCleared \u2500\u2500\u2500\u2510\\n\u25b2       \u2502\\n/clear \u2502       \u2502\\n\u2502       \u2502\\nClearing   \u2502\\n\u25b2       \u2502\\n\u2502       \u2502\\nInUse \u25c4\u2500\u2500\u2500\u2518\\n```","enum":["Cleared","InUse","Clearing"]},"udf_rust":{"type":"string"},"udf_toml":{"type":"string"},"version":{"type":"integer","format":"int64","description":"Version number."}}},"example":{"id":"67e55044-10b1-426f-9247-bb680e5fe0c8","name":"example1","description":"Description of the pipeline example1","created_at":"1970-01-01T00:00:00Z","version":4,"platform_version":"v0","runtime_config":{"workers":16,"storage":{"backend":{"name":"default"},"min_storage_bytes":null,"min_step_storage_bytes":null,"compression":"default","cache_mib":null},"fault_tolerance":{"model":"none","checkpoint_interval_secs":60},"cpu_profiler":true,"tracing":false,"tracing_endpoint_jaeger":"","min_batch_size_records":0,"max_buffering_delay_usecs":0,"resources":{"cpu_cores_min":null,"cpu_cores_max":null,"memory_mb_min":null,"memory_mb_max":null,"storage_mb_max":null,"storage_class":null},"clock_resolution_usecs":1000000,"pin_cpus":[],"provisioning_timeout_secs":null,"max_parallel_connector_init":null,"init_containers":null,"checkpoint_during_suspend":true,"http_workers":null,"io_workers":null,"dev_tweaks":{},"logging":null},"program_code":"CREATE TABLE table1 ( col1 INT );","udf_rust":"","udf_toml":"","program_config":{"profile":"optimized","cache":true,"runtime_version":null},"program_version":2,"program_status":"Pending","program_status_since":"1970-01-01T00:00:00Z","program_error":{"sql_compilation":null,"rust_compilation":null,"system_error":null},"program_info":null,"deployment_status":"Stopped","deployment_status_since":"1970-01-01T00:00:00Z","deployment_desired_status":"Stopped","deployment_error":null,"refresh_version":4,"storage_status":"Cleared"}}}},"400":{"description":"","content":{"application/json":{"schema":{"type":"object","description":"Information returned by REST API endpoints on error.","required":["message","error_code","details"],"properties":{"details":{"type":"object","description":"Detailed error metadata.\\nThe contents of this field is determined by `error_code`."},"error_code":{"type":"string","description":"Error code is a string that specifies this error type.","example":"CodeSpecifyingErrorType"},"message":{"type":"string","description":"Human-readable error message.","example":"Explanation of the error that occurred."}}},"examples":{"Cannot update non-stopped pipeline":{"value":{"message":"Pipeline can only be updated while stopped. Stop it first by invoking \'/stop\'.","error_code":"UpdateRestrictedToStopped","details":null}},"Name does not match pattern":{"value":{"message":"Name \'name-with-invalid-char-#\' contains characters which are not lowercase (a-z), uppercase (A-Z), numbers (0-9), underscores (_) or hyphens (-)","error_code":"NameDoesNotMatchPattern","details":{"name":"name-with-invalid-char-#"}}}}}}},"409":{"description":"Cannot rename pipeline as the new name already exists","content":{"application/json":{"schema":{"type":"object","description":"Information returned by REST API endpoints on error.","required":["message","error_code","details"],"properties":{"details":{"type":"object","description":"Detailed error metadata.\\nThe contents of this field is determined by `error_code`."},"error_code":{"type":"string","description":"Error code is a string that specifies this error type.","example":"CodeSpecifyingErrorType"},"message":{"type":"string","description":"Human-readable error message.","example":"Explanation of the error that occurred."}}},"example":{"message":"An entity with this name already exists","error_code":"DuplicateName","details":null}}}},"500":{"description":"","content":{"application/json":{"schema":{"type":"object","description":"Information returned by REST API endpoints on error.","required":["message","error_code","details"],"properties":{"details":{"type":"object","description":"Detailed error metadata.\\nThe contents of this field is determined by `error_code`."},"error_code":{"type":"string","description":"Error code is a string that specifies this error type.","example":"CodeSpecifyingErrorType"},"message":{"type":"string","description":"Human-readable error message.","example":"Explanation of the error that occurred."}}}}}}},"security":[{"JSON web token (JWT) or API key":[]}],"description":"Fully update a pipeline if it already exists, otherwise create a new pipeline.","method":"put","path":"/v0/pipelines/{pipeline_name}","securitySchemes":{"JSON web token (JWT) or API key":{"type":"http","scheme":"bearer","bearerFormat":"JWT","description":"Use a JWT token obtained via an OAuth2/OIDC\\n                               login workflow or an API key obtained via\\n                               the `/v0/api-keys` endpoint."}},"jsonRequestBodyExample":{"description":"string","name":"string","program_code":"string","program_config":{"cache":true,"profile":"dev","runtime_version":"string"},"runtime_config":{"checkpoint_during_suspend":true,"clock_resolution_usecs":0,"cpu_profiler":true,"dev_tweaks":{},"fault_tolerance":{"checkpoint_interval_secs":0},"http_workers":0,"io_workers":0,"logging":"string","max_buffering_delay_usecs":0,"max_parallel_connector_init":0,"min_batch_size_records":0,"pin_cpus":[0],"provisioning_timeout_secs":0,"resources":{"cpu_cores_max":0,"cpu_cores_min":0,"memory_mb_max":0,"memory_mb_min":0,"storage_class":"string","storage_mb_max":0},"storage":{"cache_mib":0,"compression":"default","min_step_storage_bytes":0,"min_storage_bytes":0},"tracing":false,"tracing_endpoint_jaeger":"string","workers":0},"udf_rust":"string","udf_toml":"string"},"info":{"title":"Feldera API","description":"\\nWith Feldera, users create data pipelines out of SQL programs.\\nA SQL program comprises tables and views, and includes as well the definition of\\ninput and output connectors for each respectively. A connector defines a data\\nsource or data sink to feed input data into tables or receive output data\\ncomputed by the views respectively.\\n\\n## Pipeline\\n\\nThe API is centered around the **pipeline**, which most importantly consists\\nout of the SQL program, but also has accompanying metadata and configuration parameters\\n(e.g., compilation profile, number of workers, etc.).\\n\\n* A pipeline is identified and referred to by its user-provided unique name.\\n* The pipeline program is asynchronously compiled when the pipeline is first created or\\n  when its program is subsequently updated.\\n* Pipeline deployment is only possible once the program is successfully compiled.\\n* A pipeline cannot be updated while it is deployed.\\n\\n## Concurrency\\n\\nEach pipeline has a version, which is incremented each time its core fields are updated.\\nThe version is monotonically increasing. There is additionally a program version which covers\\nonly the program-related core fields, and is used by the compiler to discern when to recompile.","contact":{"name":"Feldera Team","email":"dev@feldera.com"},"license":{"name":"MIT OR Apache-2.0"},"version":"0.104.0"},"postman":{"name":"Fully update a pipeline if it already exists, otherwise create a new pipeline.","description":{"type":"text/plain"},"url":{"path":["v0","pipelines",":pipeline_name"],"host":["{{baseUrl}}"],"query":[],"variable":[{"disabled":false,"description":{"content":"(Required) Unique pipeline name","type":"text/plain"},"type":"any","value":"<string>","key":"pipeline_name"}]},"header":[{"key":"Content-Type","value":"application/json"},{"key":"Accept","value":"application/json"}],"method":"PUT","body":{"mode":"raw","raw":"{\\n  \\"name\\": \\"<string>\\",\\n  \\"program_code\\": \\"<string>\\",\\n  \\"description\\": \\"<string>\\",\\n  \\"program_config\\": {\\n    \\"cache\\": true,\\n    \\"profile\\": null,\\n    \\"runtime_version\\": null\\n  },\\n  \\"runtime_config\\": {\\n    \\"checkpoint_during_suspend\\": true,\\n    \\"clock_resolution_usecs\\": 1000000,\\n    \\"cpu_profiler\\": true,\\n    \\"dev_tweaks\\": {},\\n    \\"fault_tolerance\\": {\\n      \\"model\\": \\"none\\",\\n      \\"checkpoint_interval_secs\\": 60\\n    },\\n    \\"http_workers\\": null,\\n    \\"init_containers\\": {},\\n    \\"io_workers\\": null,\\n    \\"logging\\": null,\\n    \\"max_buffering_delay_usecs\\": 0,\\n    \\"max_parallel_connector_init\\": null,\\n    \\"min_batch_size_records\\": 0,\\n    \\"provisioning_timeout_secs\\": null,\\n    \\"resources\\": {\\n      \\"cpu_cores_min\\": null,\\n      \\"cpu_cores_max\\": null,\\n      \\"memory_mb_min\\": null,\\n      \\"memory_mb_max\\": null,\\n      \\"storage_mb_max\\": null,\\n      \\"storage_class\\": null\\n    },\\n    \\"storage\\": {\\n      \\"backend\\": {\\n        \\"name\\": \\"default\\"\\n      },\\n      \\"min_storage_bytes\\": null,\\n      \\"min_step_storage_bytes\\": null,\\n      \\"compression\\": \\"default\\",\\n      \\"cache_mib\\": null\\n    },\\n    \\"tracing\\": false,\\n    \\"tracing_endpoint_jaeger\\": \\"127.0.0.1:6831\\",\\n    \\"workers\\": 8\\n  },\\n  \\"udf_rust\\": \\"<string>\\",\\n  \\"udf_toml\\": \\"<string>\\"\\n}","options":{"raw":{"language":"json"}}},"auth":{"type":"bearer","bearer":[{"type":"any","value":"{{bearerToken}}","key":"token"}]}}},"source":"@site/../openapi.json","sourceDirName":".","permalink":"/api/fully-update-a-pipeline-if-it-already-exists-otherwise-create-a-new-pipeline","previous":{"title":"Retrieve a pipeline.","permalink":"/api/retrieve-a-pipeline"},"next":{"title":"Delete a pipeline.","permalink":"/api/delete-a-pipeline"}}');var r=n(74848),s=n(28453);const o={},a="Fully update a pipeline if it already exists, otherwise create a new pipeline.",l=[{value:"The lifecycle of a pipeline",id:"the-lifecycle-of-a-pipeline",level:3},{value:"Desired and actual status",id:"desired-and-actual-status",level:3},{value:"The lifecycle of a pipeline",id:"the-lifecycle-of-a-pipeline-1",level:3},{value:"Desired and actual status",id:"desired-and-actual-status-1",level:3}];function c(e){const t={a:"a",code:"code",h1:"h1",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(t.header,{children:(0,r.jsx)(t.h1,{id:"fully-update-a-pipeline-if-it-already-exists-otherwise-create-a-new-pipeline",children:"Fully update a pipeline if it already exists, otherwise create a new pipeline."})}),"\n",(0,r.jsx)(t.p,{children:"Fully update a pipeline if it already exists, otherwise create a new pipeline."}),"\n",(0,r.jsxs)("table",{style:{display:"table",width:"100%"},children:[(0,r.jsx)("thead",{children:(0,r.jsx)("tr",{children:(0,r.jsx)("th",{style:{textAlign:"left"},children:"Path Parameters"})})}),(0,r.jsx)("tbody",{children:(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"pipeline_name"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" string"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-required)"},children:" REQUIRED"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsx)(t.p,{children:"Unique pipeline name"})})]})})})]}),"\n",(0,r.jsxs)("table",{style:{display:"table",width:"100%"},children:[(0,r.jsx)("thead",{children:(0,r.jsx)("tr",{children:(0,r.jsxs)("th",{style:{textAlign:"left"},children:[(0,r.jsx)("span",{children:"Request Body "}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-required)"},children:" REQUIRED"}),(0,r.jsx)("div",{})]})})}),(0,r.jsxs)("tbody",{children:[(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"description"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" string"})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"name"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" string"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-required)"},children:" REQUIRED"})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"program_code"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" string"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-required)"},children:" REQUIRED"})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"program_config"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" object"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsx)(t.p,{children:"Program configuration."})}),(0,r.jsx)("table",{style:{display:"table",width:"100%",marginTop:"var(--ifm-table-cell-padding)",marginBottom:"0px"},children:(0,r.jsxs)("tbody",{children:[(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"cache"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" boolean"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsxs)(t.p,{children:["If ",(0,r.jsx)(t.code,{children:"true"})," (default), when a prior compilation with the same checksum\nalready exists, the output of that (i.e., binary) is used.\nSet ",(0,r.jsx)(t.code,{children:"false"})," to always trigger a new compilation, which might take longer\nand as well can result in overriding an existing binary."]})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"profile"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" string"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsxs)(t.p,{children:[(0,r.jsx)(t.strong,{children:"Possible values:"})," [",(0,r.jsx)(t.code,{children:"dev"}),", ",(0,r.jsx)(t.code,{children:"unoptimized"}),", ",(0,r.jsx)(t.code,{children:"optimized"}),"]"]})}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsxs)(t.p,{children:["Enumeration of possible compilation profiles that can be passed to the Rust compiler\nas an argument via ",(0,r.jsx)(t.code,{children:"cargo build --profile <>"}),". A compilation profile affects among\nother things the compilation speed (how long till the program is ready to be run)\nand runtime speed (the performance while running)."]})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"runtime_version"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" string"}),(0,r.jsxs)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:[(0,r.jsx)(t.p,{children:"Override runtime version of the pipeline being executed."}),(0,r.jsx)(t.p,{children:"Warning: This option is experimental and may change in the future.\nShould only be used for CI/testing purposes, and requires network access."}),(0,r.jsxs)(t.p,{children:["A runtime version can be specified in the form of a version\nor SHA taken from the ",(0,r.jsx)(t.code,{children:"feldera/feldera"})," repository main branch."]}),(0,r.jsxs)(t.p,{children:["Examples: ",(0,r.jsx)(t.code,{children:"v0.96.0"})," or ",(0,r.jsx)(t.code,{children:"f4dcac0989ca0fda7d2eb93602a49d007cb3b0ae"})]}),(0,r.jsxs)(t.p,{children:["A platform of version ",(0,r.jsx)(t.code,{children:"0.x.y"})," may be capable of running future and past\nruntimes with versions ",(0,r.jsx)(t.code,{children:">=0.x.y"})," and ",(0,r.jsx)(t.code,{children:"<=0.x.y"})," until breaking API changes happen,\nthe exact bounds for each platform version are unspecified until we reach a\nstable version. Compatibility is only guaranteed if platform and runtime version\nare exact matches."]}),(0,r.jsx)(t.p,{children:"Note that any enterprise features are currently considered to be part of\nthe platform."}),(0,r.jsx)(t.p,{children:"If not set (null), the runtime version will be the same as the platform version."})]})]})})]})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"runtime_config"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" object"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsx)(t.p,{children:"Global pipeline configuration settings. This is the publicly\nexposed type for users to configure pipelines."})}),(0,r.jsx)("table",{style:{display:"table",width:"100%",marginTop:"var(--ifm-table-cell-padding)",marginBottom:"0px"},children:(0,r.jsxs)("tbody",{children:[(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"checkpoint_during_suspend"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" boolean"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsx)(t.p,{children:"Deprecated: setting this true or false does not have an effect anymore."})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"clock_resolution_usecs"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" int64"}),(0,r.jsxs)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:[(0,r.jsx)(t.p,{children:"Real-time clock resolution in microseconds."}),(0,r.jsxs)(t.p,{children:["This parameter controls the execution of queries that use the ",(0,r.jsx)(t.code,{children:"NOW()"})," function.  The output of such\nqueries depends on the real-time clock and can change over time without any external\ninputs.  The pipeline will update the clock value and trigger incremental recomputation\nat most each ",(0,r.jsx)(t.code,{children:"clock_resolution_usecs"})," microseconds."]}),(0,r.jsx)(t.p,{children:"It is set to 1 second (1,000,000 microseconds) by default."}),(0,r.jsxs)(t.p,{children:["Set to ",(0,r.jsx)(t.code,{children:"null"})," to disable periodic clock updates."]})]})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"cpu_profiler"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" boolean"}),(0,r.jsxs)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:[(0,r.jsx)(t.p,{children:"Enable CPU profiler."}),(0,r.jsxs)(t.p,{children:["The default value is ",(0,r.jsx)(t.code,{children:"true"}),"."]})]})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"dev_tweaks"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" object"}),(0,r.jsxs)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:[(0,r.jsx)(t.p,{children:"Optional settings for tweaking Feldera internals."}),(0,r.jsx)(t.p,{children:"The available key-value pairs change from one version of Feldera to\nanother, so users should not depend on particular settings being\navailable, or on their behavior."})]})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"fault_tolerance"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" object"}),(0,r.jsxs)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:[(0,r.jsx)(t.p,{children:"Fault-tolerance configuration."}),(0,r.jsx)(t.p,{children:"The default [FtConfig] (via [FtConfig::default]) disables fault tolerance,\nwhich is the configuration that one gets if [RuntimeConfig] omits fault\ntolerance configuration."}),(0,r.jsxs)(t.p,{children:["The default value for [FtConfig::model] enables fault tolerance, as\n",(0,r.jsx)(t.code,{children:"Some(FtModel::default())"}),".  This is the configuration that one gets if\n[RuntimeConfig] includes a fault tolerance configuration but does not\nspecify a particular model."]})]}),(0,r.jsx)("table",{style:{display:"table",width:"100%",marginTop:"var(--ifm-table-cell-padding)",marginBottom:"0px"},children:(0,r.jsxs)("tbody",{children:[(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"checkpoint_interval_secs"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" int64"}),(0,r.jsxs)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:[(0,r.jsx)(t.p,{children:"Interval between automatic checkpoints, in seconds."}),(0,r.jsx)(t.p,{children:"The default is 60 seconds.  Values less than 1 or greater than 3600 will\nbe forced into that range."})]})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"model"}),(0,r.jsx)("span",{style:{opacity:"0.6"}})]})})]})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"http_workers"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" int64"}),(0,r.jsxs)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:[(0,r.jsx)(t.p,{children:"Sets the number of available runtime threads for the http server."}),(0,r.jsx)(t.p,{children:"In most cases, this does not need to be set explicitly and\nthe default is sufficient. Can be increased in case the\npipeline HTTP API operations are a bottleneck."}),(0,r.jsxs)(t.p,{children:["If not specified, the default is set to ",(0,r.jsx)(t.code,{children:"workers"}),"."]})]})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"init_containers"}),(0,r.jsx)("span",{style:{opacity:"0.6"}}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsx)(t.p,{children:"Specification of additional (sidecar) containers."})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"io_workers"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" int64"}),(0,r.jsxs)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:[(0,r.jsx)(t.p,{children:"Sets the number of available runtime threads for async IO tasks."}),(0,r.jsx)(t.p,{children:"This affects some networking and file I/O operations\nespecially adapters and ad-hoc queries."}),(0,r.jsx)(t.p,{children:"In most cases, this does not need to be set explicitly and\nthe default is sufficient. Can be increased in case\ningress, egress or ad-hoc queries are a bottleneck."}),(0,r.jsxs)(t.p,{children:["If not specified, the default is set to ",(0,r.jsx)(t.code,{children:"workers"}),"."]})]})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"logging"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" string"}),(0,r.jsxs)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:[(0,r.jsx)(t.p,{children:"Log filtering directives."}),(0,r.jsxs)(t.p,{children:["If set to a valid ",(0,r.jsx)(t.a,{href:"https://docs.rs/tracing-subscriber/latest/tracing_subscriber/filter/struct.EnvFilter.html#directives",children:"tracing-subscriber"}),' filter, this controls the log\nmessages emitted by the pipeline process.  Otherwise, or if the filter\nhas invalid syntax, messages at "info" severity and higher are written\nto the log and all others are discarded.']})]})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"max_buffering_delay_usecs"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" int64"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsxs)(t.p,{children:["Maximal delay in microseconds to wait for ",(0,r.jsx)(t.code,{children:"min_batch_size_records"})," to\nget buffered by the controller, defaults to 0."]})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"max_parallel_connector_init"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" int64"}),(0,r.jsxs)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:[(0,r.jsx)(t.p,{children:"The maximum number of connectors initialized in parallel during pipeline\nstartup."}),(0,r.jsx)(t.p,{children:"At startup, the pipeline must initialize all of its input and output connectors.\nDepending on the number and types of connectors, this can take a long time.\nTo accelerate the process, multiple connectors are initialized concurrently.\nThis option controls the maximum number of connectors that can be initialized\nin parallel."}),(0,r.jsx)(t.p,{children:"The default is 10."})]})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"min_batch_size_records"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" int64"}),(0,r.jsxs)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:[(0,r.jsx)(t.p,{children:"Minimal input batch size."}),(0,r.jsxs)(t.p,{children:["The controller delays pushing input records to the circuit until at\nleast ",(0,r.jsx)(t.code,{children:"min_batch_size_records"})," records have been received (total\nacross all endpoints) or ",(0,r.jsx)(t.code,{children:"max_buffering_delay_usecs"})," microseconds\nhave passed since at least one input records has been buffered.\nDefaults to 0."]})]})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"pin_cpus"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" integer[]"}),(0,r.jsxs)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:[(0,r.jsx)(t.p,{children:"Optionally, a list of CPU numbers for CPUs to which the pipeline may pin\nits worker threads.  Specify at least twice as many CPU numbers as\nworkers.  CPUs are generally numbered starting from 0.  The pipeline\nmight not be able to honor CPU pinning requests."}),(0,r.jsx)(t.p,{children:"CPU pinning can make pipelines run faster and perform more consistently,\nas long as different pipelines running on the same machine are pinned to\ndifferent CPUs."})]})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"provisioning_timeout_secs"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" int64"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsxs)(t.p,{children:["Timeout in seconds for the ",(0,r.jsx)(t.code,{children:"Provisioning"})," phase of the pipeline.\nSetting this value will override the default of the runner."]})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"resources"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" object"}),(0,r.jsx)("table",{style:{display:"table",width:"100%",marginTop:"var(--ifm-table-cell-padding)",marginBottom:"0px"},children:(0,r.jsxs)("tbody",{children:[(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"cpu_cores_max"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" int64"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsx)(t.p,{children:"The maximum number of CPU cores to reserve\nfor an instance of this pipeline"})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"cpu_cores_min"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" int64"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsx)(t.p,{children:"The minimum number of CPU cores to reserve\nfor an instance of this pipeline"})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"memory_mb_max"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" int64"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsx)(t.p,{children:"The maximum memory in Megabytes to reserve\nfor an instance of this pipeline"})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"memory_mb_min"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" int64"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsx)(t.p,{children:"The minimum memory in Megabytes to reserve\nfor an instance of this pipeline"})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"storage_class"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" string"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsx)(t.p,{children:"Storage class to use for an instance of this pipeline.\nThe class determines storage performance such as IOPS and throughput."})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"storage_mb_max"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" int64"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsx)(t.p,{children:"The total storage in Megabytes to reserve\nfor an instance of this pipeline"})})]})})]})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"storage"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" object"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsx)(t.p,{children:"Storage configuration for a pipeline."})}),(0,r.jsx)("table",{style:{display:"table",width:"100%",marginTop:"var(--ifm-table-cell-padding)",marginBottom:"0px"},children:(0,r.jsxs)("tbody",{children:[(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"backend"}),(0,r.jsx)("span",{style:{opacity:"0.6"}}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsx)(t.p,{children:"Backend storage configuration."})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"cache_mib"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" integer"}),(0,r.jsxs)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:[(0,r.jsx)(t.p,{children:"The maximum size of the in-memory storage cache, in MiB."}),(0,r.jsx)(t.p,{children:"If set, the specified cache size is spread across all the foreground and\nbackground threads. If unset, each foreground or background thread cache\nis limited to 256 MiB."})]})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"compression"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" string"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsxs)(t.p,{children:[(0,r.jsx)(t.strong,{children:"Possible values:"})," [",(0,r.jsx)(t.code,{children:"default"}),", ",(0,r.jsx)(t.code,{children:"none"}),", ",(0,r.jsx)(t.code,{children:"snappy"}),"]"]})}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsx)(t.p,{children:"Storage compression algorithm."})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"min_step_storage_bytes"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" integer"}),(0,r.jsxs)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:[(0,r.jsx)(t.p,{children:"For a batch of data passed through the pipeline during a single step,\nthe minimum estimated number of bytes to write it to storage."}),(0,r.jsxs)(t.p,{children:["This is provided for debugging and fine-tuning and should ordinarily be\nleft unset.  A value of 0 will write even empty batches to storage, and\nnonzero values provide a threshold.  ",(0,r.jsx)(t.code,{children:"usize::MAX"}),", the default,\neffectively disables storage for such batches.  If it is set to another\nvalue, it should ordinarily be greater than or equal to\n",(0,r.jsx)(t.code,{children:"min_storage_bytes"}),"."]})]})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"min_storage_bytes"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" integer"}),(0,r.jsxs)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:[(0,r.jsx)(t.p,{children:"For a batch of data maintained as part of a persistent index during a\npipeline run, the minimum estimated number of bytes to write it to\nstorage."}),(0,r.jsx)(t.p,{children:"This is provided for debugging and fine-tuning and should ordinarily be\nleft unset."}),(0,r.jsxs)(t.p,{children:["A value of 0 will write even empty batches to storage, and nonzero\nvalues provide a threshold.  ",(0,r.jsx)(t.code,{children:"usize::MAX"})," would effectively disable\nstorage for such batches.  The default is 1,048,576 (1 MiB)."]})]})]})})]})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"tracing"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" boolean"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsx)(t.p,{children:"Enable pipeline tracing."})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"tracing_endpoint_jaeger"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" string"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsx)(t.p,{children:"Jaeger tracing endpoint to send tracing information to."})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"workers"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" int32"}),(0,r.jsxs)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:[(0,r.jsx)(t.p,{children:"Number of DBSP worker threads."}),(0,r.jsx)(t.p,{children:'Each DBSP "foreground" worker thread is paired with a "background"\nthread for LSM merging, making the total number of threads twice the\nspecified number.'}),(0,r.jsx)(t.p,{children:"The typical sweet spot for the number of workers is between 4 and 16.\nEach worker increases overall memory consumption for data structures\nused during a step."})]})]})})]})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"udf_rust"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" string"})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"udf_toml"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" string"})]})})]})]}),"\n",(0,r.jsxs)("table",{style:{display:"table",width:"100%"},children:[(0,r.jsx)("thead",{children:(0,r.jsx)("tr",{children:(0,r.jsx)("th",{style:{textAlign:"left"},children:"Responses"})})}),(0,r.jsxs)("tbody",{children:[(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsxs)("div",{style:{display:"flex"},children:[(0,r.jsx)("div",{style:{marginRight:"var(--ifm-table-cell-padding)"},children:(0,r.jsx)("code",{children:"200"})}),(0,r.jsx)("div",{children:(0,r.jsx)(t.p,{children:"Pipeline successfully updated"})})]}),(0,r.jsx)("div",{children:(0,r.jsxs)("table",{style:{display:"table",width:"100%",marginTop:"var(--ifm-table-cell-padding)",marginBottom:"0px"},children:[(0,r.jsx)("thead",{children:(0,r.jsx)("tr",{children:(0,r.jsxs)("th",{style:{textAlign:"left"},children:[(0,r.jsx)("span",{children:"Schema "}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"}),(0,r.jsx)("div",{})]})})}),(0,r.jsxs)("tbody",{children:[(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"created_at"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" date-time"})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"deployment_desired_status"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" string"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsxs)(t.p,{children:[(0,r.jsx)(t.strong,{children:"Possible values:"})," [",(0,r.jsx)(t.code,{children:"Stopped"}),", ",(0,r.jsx)(t.code,{children:"Paused"}),", ",(0,r.jsx)(t.code,{children:"Running"}),", ",(0,r.jsx)(t.code,{children:"Suspended"}),"]"]})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"deployment_error"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" object"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsx)(t.p,{children:"Information returned by REST API endpoints on error."})}),(0,r.jsx)("table",{style:{display:"table",width:"100%",marginTop:"var(--ifm-table-cell-padding)",marginBottom:"0px"},children:(0,r.jsxs)("tbody",{children:[(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"details"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" object"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsxs)(t.p,{children:["Detailed error metadata.\nThe contents of this field is determined by ",(0,r.jsx)(t.code,{children:"error_code"}),"."]})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"error_code"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" string"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsx)(t.p,{children:"Error code is a string that specifies this error type."})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"message"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" string"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsx)(t.p,{children:"Human-readable error message."})})]})})]})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"deployment_status"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" string"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsxs)(t.p,{children:[(0,r.jsx)(t.strong,{children:"Possible values:"})," [",(0,r.jsx)(t.code,{children:"Stopped"}),", ",(0,r.jsx)(t.code,{children:"Provisioning"}),", ",(0,r.jsx)(t.code,{children:"Initializing"}),", ",(0,r.jsx)(t.code,{children:"Paused"}),", ",(0,r.jsx)(t.code,{children:"Running"}),", ",(0,r.jsx)(t.code,{children:"Unavailable"}),", ",(0,r.jsx)(t.code,{children:"Suspending"}),", ",(0,r.jsx)(t.code,{children:"Stopping"}),"]"]})}),(0,r.jsxs)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:[(0,r.jsx)(t.p,{children:"Pipeline status."}),(0,r.jsxs)(t.p,{children:["This type represents the state of the pipeline tracked by the pipeline\nrunner and observed by the API client via the ",(0,r.jsx)(t.code,{children:"GET /v0/pipelines/{name}"})," endpoint."]}),(0,r.jsx)(t.h3,{id:"the-lifecycle-of-a-pipeline",children:"The lifecycle of a pipeline"}),(0,r.jsx)(t.p,{children:"The following automaton captures the lifecycle of the pipeline.\nIndividual states and transitions of the automaton are described below."}),(0,r.jsxs)(t.ul,{children:["\n",(0,r.jsxs)(t.li,{children:["\n",(0,r.jsxs)(t.p,{children:["States labeled with the hourglass symbol (\u231b) are ",(0,r.jsx)(t.strong,{children:"timed"})," states. The\nautomaton stays in timed state until the corresponding operation completes\nor until it transitions to become failed after the pre-defined timeout\nperiod expires."]}),"\n"]}),"\n",(0,r.jsxs)(t.li,{children:["\n",(0,r.jsxs)(t.p,{children:["State transitions labeled with API endpoint names (",(0,r.jsx)(t.code,{children:"/start"}),", ",(0,r.jsx)(t.code,{children:"/pause"}),",\n",(0,r.jsx)(t.code,{children:"/stop"}),") are triggered by invoking corresponding endpoint,\ne.g., ",(0,r.jsx)(t.code,{children:"POST /v0/pipelines/{name}/start"}),". Note that these only express\ndesired state, and are applied asynchronously by the automata."]}),"\n"]}),"\n"]}),(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-text",children:"Stopped \u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Stopping \u25c4\u2500\u2500\u2500\u2500\u2500 All states can transition\n\u2502                    \u25b2            to Stopping by either:\n/start or /pause \u2502                    \u2502            (1) user calling /stop?force=true, or;\n\u25bc                    \u2502            (2) pipeline encountering a fatal\n\u231bProvisioning          Suspending            resource or runtime error,\n\u2502                    \u25b2                having the system call /stop?force=true\n\u25bc                    \u2502 /stop          effectively\n\u231bInitializing \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524  ?force=false\n\u2502                    \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502         \u25bc                          \u2502\n\u2502       Paused  \u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u25ba Unavailable \u2502\n\u2502        \u2502   \u25b2                \u25b2      \u2502\n\u2502 /start \u2502   \u2502  /pause        \u2502      \u2502\n\u2502        \u25bc   \u2502                \u2502      \u2502\n\u2502       Running \u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),(0,r.jsx)(t.h3,{id:"desired-and-actual-status",children:"Desired and actual status"}),(0,r.jsxs)(t.p,{children:["We use the desired state model to manage the lifecycle of a pipeline.\nIn this model, the pipeline has two status attributes associated with\nit at runtime: the ",(0,r.jsx)(t.strong,{children:"desired"})," status, which represents what the user\nwould like the pipeline to do, and the ",(0,r.jsx)(t.strong,{children:"current"})," status, which\nrepresents the actual state of the pipeline.  The pipeline runner\nservice continuously monitors both fields and steers the pipeline\ntowards the desired state specified by the user."]}),(0,r.jsxs)(t.p,{children:["Only four of the states in the pipeline automaton above can be\nused as desired statuses: ",(0,r.jsx)(t.code,{children:"Paused"}),", ",(0,r.jsx)(t.code,{children:"Running"}),", ",(0,r.jsx)(t.code,{children:"Suspended"})," and\n",(0,r.jsx)(t.code,{children:"Stopped"}),". These statuses are selected by invoking REST endpoints\nshown in the diagram (respectively, ",(0,r.jsx)(t.code,{children:"/pause"}),", ",(0,r.jsx)(t.code,{children:"/start"}),", and ",(0,r.jsx)(t.code,{children:"/stop"}),")."]}),(0,r.jsxs)(t.p,{children:["The user can monitor the current state of the pipeline via the\n",(0,r.jsx)(t.code,{children:"GET /v0/pipelines/{name}"})," endpoint. In a typical scenario,\nthe user first sets the desired state, e.g., by invoking the\n",(0,r.jsx)(t.code,{children:"/start"})," endpoint, and then polls the ",(0,r.jsx)(t.code,{children:"GET /v0/pipelines/{name}"}),"\nendpoint to monitor the actual status of the pipeline until its\n",(0,r.jsx)(t.code,{children:"deployment_status"})," attribute changes to ",(0,r.jsx)(t.code,{children:"Running"})," indicating\nthat the pipeline has been successfully initialized and is\nprocessing data, or ",(0,r.jsx)(t.code,{children:"Stopped"})," with ",(0,r.jsx)(t.code,{children:"deployment_error"})," being set."]})]})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"deployment_status_since"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" date-time"})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"description"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" string"})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"id"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" uuid"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsx)(t.p,{children:"Pipeline identifier."})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"name"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" string"})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"platform_version"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" string"})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"program_code"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" string"})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"program_config"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" object"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsx)(t.p,{children:"Program configuration."})}),(0,r.jsx)("table",{style:{display:"table",width:"100%",marginTop:"var(--ifm-table-cell-padding)",marginBottom:"0px"},children:(0,r.jsxs)("tbody",{children:[(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"cache"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" boolean"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsxs)(t.p,{children:["If ",(0,r.jsx)(t.code,{children:"true"})," (default), when a prior compilation with the same checksum\nalready exists, the output of that (i.e., binary) is used.\nSet ",(0,r.jsx)(t.code,{children:"false"})," to always trigger a new compilation, which might take longer\nand as well can result in overriding an existing binary."]})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"profile"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" string"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsxs)(t.p,{children:[(0,r.jsx)(t.strong,{children:"Possible values:"})," [",(0,r.jsx)(t.code,{children:"dev"}),", ",(0,r.jsx)(t.code,{children:"unoptimized"}),", ",(0,r.jsx)(t.code,{children:"optimized"}),"]"]})}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsxs)(t.p,{children:["Enumeration of possible compilation profiles that can be passed to the Rust compiler\nas an argument via ",(0,r.jsx)(t.code,{children:"cargo build --profile <>"}),". A compilation profile affects among\nother things the compilation speed (how long till the program is ready to be run)\nand runtime speed (the performance while running)."]})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"runtime_version"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" string"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"}),(0,r.jsxs)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:[(0,r.jsx)(t.p,{children:"Override runtime version of the pipeline being executed."}),(0,r.jsx)(t.p,{children:"Warning: This option is experimental and may change in the future.\nShould only be used for CI/testing purposes, and requires network access."}),(0,r.jsxs)(t.p,{children:["A runtime version can be specified in the form of a version\nor SHA taken from the ",(0,r.jsx)(t.code,{children:"feldera/feldera"})," repository main branch."]}),(0,r.jsxs)(t.p,{children:["Examples: ",(0,r.jsx)(t.code,{children:"v0.96.0"})," or ",(0,r.jsx)(t.code,{children:"f4dcac0989ca0fda7d2eb93602a49d007cb3b0ae"})]}),(0,r.jsxs)(t.p,{children:["A platform of version ",(0,r.jsx)(t.code,{children:"0.x.y"})," may be capable of running future and past\nruntimes with versions ",(0,r.jsx)(t.code,{children:">=0.x.y"})," and ",(0,r.jsx)(t.code,{children:"<=0.x.y"})," until breaking API changes happen,\nthe exact bounds for each platform version are unspecified until we reach a\nstable version. Compatibility is only guaranteed if platform and runtime version\nare exact matches."]}),(0,r.jsx)(t.p,{children:"Note that any enterprise features are currently considered to be part of\nthe platform."}),(0,r.jsx)(t.p,{children:"If not set (null), the runtime version will be the same as the platform version."})]})]})})]})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"program_error"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" object"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsx)(t.p,{children:"Log, warning and error information about the program compilation."})}),(0,r.jsx)("table",{style:{display:"table",width:"100%",marginTop:"var(--ifm-table-cell-padding)",marginBottom:"0px"},children:(0,r.jsxs)("tbody",{children:[(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"rust_compilation"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" object"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsx)(t.p,{children:"Rust compilation information."})}),(0,r.jsx)("table",{style:{display:"table",width:"100%",marginTop:"var(--ifm-table-cell-padding)",marginBottom:"0px"},children:(0,r.jsxs)("tbody",{children:[(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"exit_code"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" int32"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsxs)(t.p,{children:["Exit code of the ",(0,r.jsx)(t.code,{children:"cargo"})," compilation command."]})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"stderr"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" string"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsxs)(t.p,{children:["Output printed to stderr by the ",(0,r.jsx)(t.code,{children:"cargo"})," compilation command."]})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"stdout"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" string"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsxs)(t.p,{children:["Output printed to stdout by the ",(0,r.jsx)(t.code,{children:"cargo"})," compilation command."]})})]})})]})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"sql_compilation"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" object"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsx)(t.p,{children:"SQL compilation information."})}),(0,r.jsx)("table",{style:{display:"table",width:"100%",marginTop:"var(--ifm-table-cell-padding)",marginBottom:"0px"},children:(0,r.jsxs)("tbody",{children:[(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"exit_code"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" int32"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsx)(t.p,{children:"Exit code of the SQL compiler."})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"messages"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" object[]"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsx)(t.p,{children:"Messages (warnings and errors) generated by the SQL compiler."})}),(0,r.jsx)("table",{style:{display:"table",width:"100%",marginTop:"var(--ifm-table-cell-padding)",marginBottom:"0px"},children:(0,r.jsxs)("tbody",{children:[(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"end_column"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" integer"})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"end_line_number"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" integer"})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"error_type"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" string"})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"message"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" string"})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"snippet"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" string"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"start_column"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" integer"})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"start_line_number"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" integer"})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"warning"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" boolean"})]})})]})})]})})]})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"system_error"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" string"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"}),(0,r.jsxs)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:[(0,r.jsx)(t.p,{children:"System error that occurred."}),(0,r.jsxs)(t.ul,{children:["\n",(0,r.jsxs)(t.li,{children:["Set ",(0,r.jsx)(t.code,{children:"Some(...)"})," upon transition to ",(0,r.jsx)(t.code,{children:"SystemError"})]}),"\n",(0,r.jsxs)(t.li,{children:["Set ",(0,r.jsx)(t.code,{children:"None"})," upon transition to ",(0,r.jsx)(t.code,{children:"Pending"})]}),"\n"]})]})]})})]})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"program_info"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" object"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsx)(t.p,{children:"Program information is the result of the SQL compilation."})}),(0,r.jsx)("table",{style:{display:"table",width:"100%",marginTop:"var(--ifm-table-cell-padding)",marginBottom:"0px"},children:(0,r.jsxs)("tbody",{children:[(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"input_connectors"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" object"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsx)(t.p,{children:"Input connectors derived from the schema."})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"output_connectors"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" object"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsx)(t.p,{children:"Output connectors derived from the schema."})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"schema"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" object"}),(0,r.jsxs)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:[(0,r.jsx)(t.p,{children:"A struct containing the tables (inputs) and views for a program."}),(0,r.jsx)(t.p,{children:"Parse from the JSON data-type of the DDL generated by the SQL compiler."})]}),(0,r.jsx)("table",{style:{display:"table",width:"100%",marginTop:"var(--ifm-table-cell-padding)",marginBottom:"0px"},children:(0,r.jsxs)("tbody",{children:[(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"inputs"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" object[]"}),(0,r.jsx)("table",{style:{display:"table",width:"100%",marginTop:"var(--ifm-table-cell-padding)",marginBottom:"0px"},children:(0,r.jsxs)("tbody",{children:[(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"case_sensitive"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" boolean"})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"name"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" string"})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"fields"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" object[]"}),(0,r.jsx)("table",{style:{display:"table",width:"100%",marginTop:"var(--ifm-table-cell-padding)",marginBottom:"0px"},children:(0,r.jsxs)("tbody",{children:[(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"case_sensitive"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" boolean"})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"name"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" string"})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"columntype"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:"  (circular)"})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"default"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" string"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"lateness"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" string"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"unused"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" boolean"})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"watermark"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" string"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"})]})})]})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"materialized"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" boolean"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"properties"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" object"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"})]})})]})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"outputs"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" object[]"}),(0,r.jsx)("table",{style:{display:"table",width:"100%",marginTop:"var(--ifm-table-cell-padding)",marginBottom:"0px"},children:(0,r.jsxs)("tbody",{children:[(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"case_sensitive"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" boolean"})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"name"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" string"})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"fields"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" object[]"}),(0,r.jsx)("table",{style:{display:"table",width:"100%",marginTop:"var(--ifm-table-cell-padding)",marginBottom:"0px"},children:(0,r.jsxs)("tbody",{children:[(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"case_sensitive"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" boolean"})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"name"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" string"})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"columntype"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:"  (circular)"})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"default"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" string"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"lateness"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" string"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"unused"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" boolean"})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"watermark"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" string"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"})]})})]})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"materialized"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" boolean"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"properties"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" object"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"})]})})]})})]})})]})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"udf_stubs"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" string"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsx)(t.p,{children:"Generated user defined function (UDF) stubs Rust code: stubs.rs"})})]})})]})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"program_status"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" string"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsxs)(t.p,{children:[(0,r.jsx)(t.strong,{children:"Possible values:"})," [",(0,r.jsx)(t.code,{children:"Pending"}),", ",(0,r.jsx)(t.code,{children:"CompilingSql"}),", ",(0,r.jsx)(t.code,{children:"SqlCompiled"}),", ",(0,r.jsx)(t.code,{children:"CompilingRust"}),", ",(0,r.jsx)(t.code,{children:"Success"}),", ",(0,r.jsx)(t.code,{children:"SqlError"}),", ",(0,r.jsx)(t.code,{children:"RustError"}),", ",(0,r.jsx)(t.code,{children:"SystemError"}),"]"]})}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsx)(t.p,{children:"Program compilation status."})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"program_status_since"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" date-time"})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"program_version"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" int64"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsx)(t.p,{children:"Version number."})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"refresh_version"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" int64"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsx)(t.p,{children:"Version number."})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"runtime_config"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" object"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsx)(t.p,{children:"Global pipeline configuration settings. This is the publicly\nexposed type for users to configure pipelines."})}),(0,r.jsx)("table",{style:{display:"table",width:"100%",marginTop:"var(--ifm-table-cell-padding)",marginBottom:"0px"},children:(0,r.jsxs)("tbody",{children:[(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"checkpoint_during_suspend"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" boolean"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsx)(t.p,{children:"Deprecated: setting this true or false does not have an effect anymore."})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"clock_resolution_usecs"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" int64"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"}),(0,r.jsxs)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:[(0,r.jsx)(t.p,{children:"Real-time clock resolution in microseconds."}),(0,r.jsxs)(t.p,{children:["This parameter controls the execution of queries that use the ",(0,r.jsx)(t.code,{children:"NOW()"})," function.  The output of such\nqueries depends on the real-time clock and can change over time without any external\ninputs.  The pipeline will update the clock value and trigger incremental recomputation\nat most each ",(0,r.jsx)(t.code,{children:"clock_resolution_usecs"})," microseconds."]}),(0,r.jsx)(t.p,{children:"It is set to 1 second (1,000,000 microseconds) by default."}),(0,r.jsxs)(t.p,{children:["Set to ",(0,r.jsx)(t.code,{children:"null"})," to disable periodic clock updates."]})]})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"cpu_profiler"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" boolean"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"}),(0,r.jsxs)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:[(0,r.jsx)(t.p,{children:"Enable CPU profiler."}),(0,r.jsxs)(t.p,{children:["The default value is ",(0,r.jsx)(t.code,{children:"true"}),"."]})]})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"dev_tweaks"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" object"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"}),(0,r.jsxs)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:[(0,r.jsx)(t.p,{children:"Optional settings for tweaking Feldera internals."}),(0,r.jsx)(t.p,{children:"The available key-value pairs change from one version of Feldera to\nanother, so users should not depend on particular settings being\navailable, or on their behavior."})]})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"fault_tolerance"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" object"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"}),(0,r.jsxs)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:[(0,r.jsx)(t.p,{children:"Fault-tolerance configuration."}),(0,r.jsx)(t.p,{children:"The default [FtConfig] (via [FtConfig::default]) disables fault tolerance,\nwhich is the configuration that one gets if [RuntimeConfig] omits fault\ntolerance configuration."}),(0,r.jsxs)(t.p,{children:["The default value for [FtConfig::model] enables fault tolerance, as\n",(0,r.jsx)(t.code,{children:"Some(FtModel::default())"}),".  This is the configuration that one gets if\n[RuntimeConfig] includes a fault tolerance configuration but does not\nspecify a particular model."]})]}),(0,r.jsx)("table",{style:{display:"table",width:"100%",marginTop:"var(--ifm-table-cell-padding)",marginBottom:"0px"},children:(0,r.jsxs)("tbody",{children:[(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"checkpoint_interval_secs"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" int64"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"}),(0,r.jsxs)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:[(0,r.jsx)(t.p,{children:"Interval between automatic checkpoints, in seconds."}),(0,r.jsx)(t.p,{children:"The default is 60 seconds.  Values less than 1 or greater than 3600 will\nbe forced into that range."})]})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"model"}),(0,r.jsx)("span",{style:{opacity:"0.6"}}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"})]})})]})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"http_workers"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" int64"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"}),(0,r.jsxs)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:[(0,r.jsx)(t.p,{children:"Sets the number of available runtime threads for the http server."}),(0,r.jsx)(t.p,{children:"In most cases, this does not need to be set explicitly and\nthe default is sufficient. Can be increased in case the\npipeline HTTP API operations are a bottleneck."}),(0,r.jsxs)(t.p,{children:["If not specified, the default is set to ",(0,r.jsx)(t.code,{children:"workers"}),"."]})]})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"init_containers"}),(0,r.jsx)("span",{style:{opacity:"0.6"}}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsx)(t.p,{children:"Specification of additional (sidecar) containers."})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"io_workers"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" int64"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"}),(0,r.jsxs)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:[(0,r.jsx)(t.p,{children:"Sets the number of available runtime threads for async IO tasks."}),(0,r.jsx)(t.p,{children:"This affects some networking and file I/O operations\nespecially adapters and ad-hoc queries."}),(0,r.jsx)(t.p,{children:"In most cases, this does not need to be set explicitly and\nthe default is sufficient. Can be increased in case\ningress, egress or ad-hoc queries are a bottleneck."}),(0,r.jsxs)(t.p,{children:["If not specified, the default is set to ",(0,r.jsx)(t.code,{children:"workers"}),"."]})]})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"logging"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" string"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"}),(0,r.jsxs)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:[(0,r.jsx)(t.p,{children:"Log filtering directives."}),(0,r.jsxs)(t.p,{children:["If set to a valid ",(0,r.jsx)(t.a,{href:"https://docs.rs/tracing-subscriber/latest/tracing_subscriber/filter/struct.EnvFilter.html#directives",children:"tracing-subscriber"}),' filter, this controls the log\nmessages emitted by the pipeline process.  Otherwise, or if the filter\nhas invalid syntax, messages at "info" severity and higher are written\nto the log and all others are discarded.']})]})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"max_buffering_delay_usecs"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" int64"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsxs)(t.p,{children:["Maximal delay in microseconds to wait for ",(0,r.jsx)(t.code,{children:"min_batch_size_records"})," to\nget buffered by the controller, defaults to 0."]})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"max_parallel_connector_init"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" int64"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"}),(0,r.jsxs)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:[(0,r.jsx)(t.p,{children:"The maximum number of connectors initialized in parallel during pipeline\nstartup."}),(0,r.jsx)(t.p,{children:"At startup, the pipeline must initialize all of its input and output connectors.\nDepending on the number and types of connectors, this can take a long time.\nTo accelerate the process, multiple connectors are initialized concurrently.\nThis option controls the maximum number of connectors that can be initialized\nin parallel."}),(0,r.jsx)(t.p,{children:"The default is 10."})]})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"min_batch_size_records"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" int64"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"}),(0,r.jsxs)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:[(0,r.jsx)(t.p,{children:"Minimal input batch size."}),(0,r.jsxs)(t.p,{children:["The controller delays pushing input records to the circuit until at\nleast ",(0,r.jsx)(t.code,{children:"min_batch_size_records"})," records have been received (total\nacross all endpoints) or ",(0,r.jsx)(t.code,{children:"max_buffering_delay_usecs"})," microseconds\nhave passed since at least one input records has been buffered.\nDefaults to 0."]})]})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"pin_cpus"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" integer[]"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"}),(0,r.jsxs)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:[(0,r.jsx)(t.p,{children:"Optionally, a list of CPU numbers for CPUs to which the pipeline may pin\nits worker threads.  Specify at least twice as many CPU numbers as\nworkers.  CPUs are generally numbered starting from 0.  The pipeline\nmight not be able to honor CPU pinning requests."}),(0,r.jsx)(t.p,{children:"CPU pinning can make pipelines run faster and perform more consistently,\nas long as different pipelines running on the same machine are pinned to\ndifferent CPUs."})]})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"provisioning_timeout_secs"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" int64"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsxs)(t.p,{children:["Timeout in seconds for the ",(0,r.jsx)(t.code,{children:"Provisioning"})," phase of the pipeline.\nSetting this value will override the default of the runner."]})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"resources"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" object"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"}),(0,r.jsx)("table",{style:{display:"table",width:"100%",marginTop:"var(--ifm-table-cell-padding)",marginBottom:"0px"},children:(0,r.jsxs)("tbody",{children:[(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"cpu_cores_max"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" int64"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsx)(t.p,{children:"The maximum number of CPU cores to reserve\nfor an instance of this pipeline"})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"cpu_cores_min"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" int64"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsx)(t.p,{children:"The minimum number of CPU cores to reserve\nfor an instance of this pipeline"})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"memory_mb_max"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" int64"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsx)(t.p,{children:"The maximum memory in Megabytes to reserve\nfor an instance of this pipeline"})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"memory_mb_min"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" int64"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsx)(t.p,{children:"The minimum memory in Megabytes to reserve\nfor an instance of this pipeline"})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"storage_class"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" string"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsx)(t.p,{children:"Storage class to use for an instance of this pipeline.\nThe class determines storage performance such as IOPS and throughput."})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"storage_mb_max"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" int64"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsx)(t.p,{children:"The total storage in Megabytes to reserve\nfor an instance of this pipeline"})})]})})]})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"storage"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" object"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsx)(t.p,{children:"Storage configuration for a pipeline."})}),(0,r.jsx)("table",{style:{display:"table",width:"100%",marginTop:"var(--ifm-table-cell-padding)",marginBottom:"0px"},children:(0,r.jsxs)("tbody",{children:[(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"backend"}),(0,r.jsx)("span",{style:{opacity:"0.6"}}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsx)(t.p,{children:"Backend storage configuration."})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"cache_mib"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" integer"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"}),(0,r.jsxs)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:[(0,r.jsx)(t.p,{children:"The maximum size of the in-memory storage cache, in MiB."}),(0,r.jsx)(t.p,{children:"If set, the specified cache size is spread across all the foreground and\nbackground threads. If unset, each foreground or background thread cache\nis limited to 256 MiB."})]})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"compression"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" string"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsxs)(t.p,{children:[(0,r.jsx)(t.strong,{children:"Possible values:"})," [",(0,r.jsx)(t.code,{children:"default"}),", ",(0,r.jsx)(t.code,{children:"none"}),", ",(0,r.jsx)(t.code,{children:"snappy"}),"]"]})}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsx)(t.p,{children:"Storage compression algorithm."})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"min_step_storage_bytes"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" integer"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"}),(0,r.jsxs)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:[(0,r.jsx)(t.p,{children:"For a batch of data passed through the pipeline during a single step,\nthe minimum estimated number of bytes to write it to storage."}),(0,r.jsxs)(t.p,{children:["This is provided for debugging and fine-tuning and should ordinarily be\nleft unset.  A value of 0 will write even empty batches to storage, and\nnonzero values provide a threshold.  ",(0,r.jsx)(t.code,{children:"usize::MAX"}),", the default,\neffectively disables storage for such batches.  If it is set to another\nvalue, it should ordinarily be greater than or equal to\n",(0,r.jsx)(t.code,{children:"min_storage_bytes"}),"."]})]})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"min_storage_bytes"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" integer"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"}),(0,r.jsxs)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:[(0,r.jsx)(t.p,{children:"For a batch of data maintained as part of a persistent index during a\npipeline run, the minimum estimated number of bytes to write it to\nstorage."}),(0,r.jsx)(t.p,{children:"This is provided for debugging and fine-tuning and should ordinarily be\nleft unset."}),(0,r.jsxs)(t.p,{children:["A value of 0 will write even empty batches to storage, and nonzero\nvalues provide a threshold.  ",(0,r.jsx)(t.code,{children:"usize::MAX"})," would effectively disable\nstorage for such batches.  The default is 1,048,576 (1 MiB)."]})]})]})})]})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"tracing"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" boolean"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsx)(t.p,{children:"Enable pipeline tracing."})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"tracing_endpoint_jaeger"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" string"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsx)(t.p,{children:"Jaeger tracing endpoint to send tracing information to."})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"workers"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" int32"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"}),(0,r.jsxs)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:[(0,r.jsx)(t.p,{children:"Number of DBSP worker threads."}),(0,r.jsx)(t.p,{children:'Each DBSP "foreground" worker thread is paired with a "background"\nthread for LSM merging, making the total number of threads twice the\nspecified number.'}),(0,r.jsx)(t.p,{children:"The typical sweet spot for the number of workers is between 4 and 16.\nEach worker increases overall memory consumption for data structures\nused during a step."})]})]})})]})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"storage_status"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" string"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsxs)(t.p,{children:[(0,r.jsx)(t.strong,{children:"Possible values:"})," [",(0,r.jsx)(t.code,{children:"Cleared"}),", ",(0,r.jsx)(t.code,{children:"InUse"}),", ",(0,r.jsx)(t.code,{children:"Clearing"}),"]"]})}),(0,r.jsxs)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:[(0,r.jsx)(t.p,{children:"Storage status."}),(0,r.jsxs)(t.p,{children:["The storage status can only transition when the pipeline status is ",(0,r.jsx)(t.code,{children:"Stopped"}),"."]}),(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-text",children:"Cleared \u2500\u2500\u2500\u2510\n\u25b2       \u2502\n/clear \u2502       \u2502\n\u2502       \u2502\nClearing   \u2502\n\u25b2       \u2502\n\u2502       \u2502\nInUse \u25c4\u2500\u2500\u2500\u2518\n"})})]})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"udf_rust"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" string"})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"udf_toml"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" string"})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"version"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" int64"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsx)(t.p,{children:"Version number."})})]})})]})]})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsxs)("div",{style:{display:"flex"},children:[(0,r.jsx)("div",{style:{marginRight:"var(--ifm-table-cell-padding)"},children:(0,r.jsx)("code",{children:"201"})}),(0,r.jsx)("div",{children:(0,r.jsx)(t.p,{children:"Pipeline successfully created"})})]}),(0,r.jsx)("div",{children:(0,r.jsxs)("table",{style:{display:"table",width:"100%",marginTop:"var(--ifm-table-cell-padding)",marginBottom:"0px"},children:[(0,r.jsx)("thead",{children:(0,r.jsx)("tr",{children:(0,r.jsxs)("th",{style:{textAlign:"left"},children:[(0,r.jsx)("span",{children:"Schema "}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"}),(0,r.jsx)("div",{})]})})}),(0,r.jsxs)("tbody",{children:[(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"created_at"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" date-time"})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"deployment_desired_status"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" string"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsxs)(t.p,{children:[(0,r.jsx)(t.strong,{children:"Possible values:"})," [",(0,r.jsx)(t.code,{children:"Stopped"}),", ",(0,r.jsx)(t.code,{children:"Paused"}),", ",(0,r.jsx)(t.code,{children:"Running"}),", ",(0,r.jsx)(t.code,{children:"Suspended"}),"]"]})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"deployment_error"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" object"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsx)(t.p,{children:"Information returned by REST API endpoints on error."})}),(0,r.jsx)("table",{style:{display:"table",width:"100%",marginTop:"var(--ifm-table-cell-padding)",marginBottom:"0px"},children:(0,r.jsxs)("tbody",{children:[(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"details"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" object"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsxs)(t.p,{children:["Detailed error metadata.\nThe contents of this field is determined by ",(0,r.jsx)(t.code,{children:"error_code"}),"."]})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"error_code"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" string"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsx)(t.p,{children:"Error code is a string that specifies this error type."})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"message"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" string"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsx)(t.p,{children:"Human-readable error message."})})]})})]})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"deployment_status"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" string"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsxs)(t.p,{children:[(0,r.jsx)(t.strong,{children:"Possible values:"})," [",(0,r.jsx)(t.code,{children:"Stopped"}),", ",(0,r.jsx)(t.code,{children:"Provisioning"}),", ",(0,r.jsx)(t.code,{children:"Initializing"}),", ",(0,r.jsx)(t.code,{children:"Paused"}),", ",(0,r.jsx)(t.code,{children:"Running"}),", ",(0,r.jsx)(t.code,{children:"Unavailable"}),", ",(0,r.jsx)(t.code,{children:"Suspending"}),", ",(0,r.jsx)(t.code,{children:"Stopping"}),"]"]})}),(0,r.jsxs)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:[(0,r.jsx)(t.p,{children:"Pipeline status."}),(0,r.jsxs)(t.p,{children:["This type represents the state of the pipeline tracked by the pipeline\nrunner and observed by the API client via the ",(0,r.jsx)(t.code,{children:"GET /v0/pipelines/{name}"})," endpoint."]}),(0,r.jsx)(t.h3,{id:"the-lifecycle-of-a-pipeline-1",children:"The lifecycle of a pipeline"}),(0,r.jsx)(t.p,{children:"The following automaton captures the lifecycle of the pipeline.\nIndividual states and transitions of the automaton are described below."}),(0,r.jsxs)(t.ul,{children:["\n",(0,r.jsxs)(t.li,{children:["\n",(0,r.jsxs)(t.p,{children:["States labeled with the hourglass symbol (\u231b) are ",(0,r.jsx)(t.strong,{children:"timed"})," states. The\nautomaton stays in timed state until the corresponding operation completes\nor until it transitions to become failed after the pre-defined timeout\nperiod expires."]}),"\n"]}),"\n",(0,r.jsxs)(t.li,{children:["\n",(0,r.jsxs)(t.p,{children:["State transitions labeled with API endpoint names (",(0,r.jsx)(t.code,{children:"/start"}),", ",(0,r.jsx)(t.code,{children:"/pause"}),",\n",(0,r.jsx)(t.code,{children:"/stop"}),") are triggered by invoking corresponding endpoint,\ne.g., ",(0,r.jsx)(t.code,{children:"POST /v0/pipelines/{name}/start"}),". Note that these only express\ndesired state, and are applied asynchronously by the automata."]}),"\n"]}),"\n"]}),(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-text",children:"Stopped \u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Stopping \u25c4\u2500\u2500\u2500\u2500\u2500 All states can transition\n\u2502                    \u25b2            to Stopping by either:\n/start or /pause \u2502                    \u2502            (1) user calling /stop?force=true, or;\n\u25bc                    \u2502            (2) pipeline encountering a fatal\n\u231bProvisioning          Suspending            resource or runtime error,\n\u2502                    \u25b2                having the system call /stop?force=true\n\u25bc                    \u2502 /stop          effectively\n\u231bInitializing \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524  ?force=false\n\u2502                    \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502         \u25bc                          \u2502\n\u2502       Paused  \u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u25ba Unavailable \u2502\n\u2502        \u2502   \u25b2                \u25b2      \u2502\n\u2502 /start \u2502   \u2502  /pause        \u2502      \u2502\n\u2502        \u25bc   \u2502                \u2502      \u2502\n\u2502       Running \u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),(0,r.jsx)(t.h3,{id:"desired-and-actual-status-1",children:"Desired and actual status"}),(0,r.jsxs)(t.p,{children:["We use the desired state model to manage the lifecycle of a pipeline.\nIn this model, the pipeline has two status attributes associated with\nit at runtime: the ",(0,r.jsx)(t.strong,{children:"desired"})," status, which represents what the user\nwould like the pipeline to do, and the ",(0,r.jsx)(t.strong,{children:"current"})," status, which\nrepresents the actual state of the pipeline.  The pipeline runner\nservice continuously monitors both fields and steers the pipeline\ntowards the desired state specified by the user."]}),(0,r.jsxs)(t.p,{children:["Only four of the states in the pipeline automaton above can be\nused as desired statuses: ",(0,r.jsx)(t.code,{children:"Paused"}),", ",(0,r.jsx)(t.code,{children:"Running"}),", ",(0,r.jsx)(t.code,{children:"Suspended"})," and\n",(0,r.jsx)(t.code,{children:"Stopped"}),". These statuses are selected by invoking REST endpoints\nshown in the diagram (respectively, ",(0,r.jsx)(t.code,{children:"/pause"}),", ",(0,r.jsx)(t.code,{children:"/start"}),", and ",(0,r.jsx)(t.code,{children:"/stop"}),")."]}),(0,r.jsxs)(t.p,{children:["The user can monitor the current state of the pipeline via the\n",(0,r.jsx)(t.code,{children:"GET /v0/pipelines/{name}"})," endpoint. In a typical scenario,\nthe user first sets the desired state, e.g., by invoking the\n",(0,r.jsx)(t.code,{children:"/start"})," endpoint, and then polls the ",(0,r.jsx)(t.code,{children:"GET /v0/pipelines/{name}"}),"\nendpoint to monitor the actual status of the pipeline until its\n",(0,r.jsx)(t.code,{children:"deployment_status"})," attribute changes to ",(0,r.jsx)(t.code,{children:"Running"})," indicating\nthat the pipeline has been successfully initialized and is\nprocessing data, or ",(0,r.jsx)(t.code,{children:"Stopped"})," with ",(0,r.jsx)(t.code,{children:"deployment_error"})," being set."]})]})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"deployment_status_since"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" date-time"})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"description"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" string"})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"id"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" uuid"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsx)(t.p,{children:"Pipeline identifier."})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"name"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" string"})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"platform_version"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" string"})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"program_code"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" string"})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"program_config"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" object"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsx)(t.p,{children:"Program configuration."})}),(0,r.jsx)("table",{style:{display:"table",width:"100%",marginTop:"var(--ifm-table-cell-padding)",marginBottom:"0px"},children:(0,r.jsxs)("tbody",{children:[(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"cache"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" boolean"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsxs)(t.p,{children:["If ",(0,r.jsx)(t.code,{children:"true"})," (default), when a prior compilation with the same checksum\nalready exists, the output of that (i.e., binary) is used.\nSet ",(0,r.jsx)(t.code,{children:"false"})," to always trigger a new compilation, which might take longer\nand as well can result in overriding an existing binary."]})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"profile"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" string"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsxs)(t.p,{children:[(0,r.jsx)(t.strong,{children:"Possible values:"})," [",(0,r.jsx)(t.code,{children:"dev"}),", ",(0,r.jsx)(t.code,{children:"unoptimized"}),", ",(0,r.jsx)(t.code,{children:"optimized"}),"]"]})}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsxs)(t.p,{children:["Enumeration of possible compilation profiles that can be passed to the Rust compiler\nas an argument via ",(0,r.jsx)(t.code,{children:"cargo build --profile <>"}),". A compilation profile affects among\nother things the compilation speed (how long till the program is ready to be run)\nand runtime speed (the performance while running)."]})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"runtime_version"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" string"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"}),(0,r.jsxs)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:[(0,r.jsx)(t.p,{children:"Override runtime version of the pipeline being executed."}),(0,r.jsx)(t.p,{children:"Warning: This option is experimental and may change in the future.\nShould only be used for CI/testing purposes, and requires network access."}),(0,r.jsxs)(t.p,{children:["A runtime version can be specified in the form of a version\nor SHA taken from the ",(0,r.jsx)(t.code,{children:"feldera/feldera"})," repository main branch."]}),(0,r.jsxs)(t.p,{children:["Examples: ",(0,r.jsx)(t.code,{children:"v0.96.0"})," or ",(0,r.jsx)(t.code,{children:"f4dcac0989ca0fda7d2eb93602a49d007cb3b0ae"})]}),(0,r.jsxs)(t.p,{children:["A platform of version ",(0,r.jsx)(t.code,{children:"0.x.y"})," may be capable of running future and past\nruntimes with versions ",(0,r.jsx)(t.code,{children:">=0.x.y"})," and ",(0,r.jsx)(t.code,{children:"<=0.x.y"})," until breaking API changes happen,\nthe exact bounds for each platform version are unspecified until we reach a\nstable version. Compatibility is only guaranteed if platform and runtime version\nare exact matches."]}),(0,r.jsx)(t.p,{children:"Note that any enterprise features are currently considered to be part of\nthe platform."}),(0,r.jsx)(t.p,{children:"If not set (null), the runtime version will be the same as the platform version."})]})]})})]})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"program_error"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" object"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsx)(t.p,{children:"Log, warning and error information about the program compilation."})}),(0,r.jsx)("table",{style:{display:"table",width:"100%",marginTop:"var(--ifm-table-cell-padding)",marginBottom:"0px"},children:(0,r.jsxs)("tbody",{children:[(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"rust_compilation"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" object"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsx)(t.p,{children:"Rust compilation information."})}),(0,r.jsx)("table",{style:{display:"table",width:"100%",marginTop:"var(--ifm-table-cell-padding)",marginBottom:"0px"},children:(0,r.jsxs)("tbody",{children:[(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"exit_code"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" int32"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsxs)(t.p,{children:["Exit code of the ",(0,r.jsx)(t.code,{children:"cargo"})," compilation command."]})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"stderr"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" string"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsxs)(t.p,{children:["Output printed to stderr by the ",(0,r.jsx)(t.code,{children:"cargo"})," compilation command."]})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"stdout"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" string"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsxs)(t.p,{children:["Output printed to stdout by the ",(0,r.jsx)(t.code,{children:"cargo"})," compilation command."]})})]})})]})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"sql_compilation"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" object"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsx)(t.p,{children:"SQL compilation information."})}),(0,r.jsx)("table",{style:{display:"table",width:"100%",marginTop:"var(--ifm-table-cell-padding)",marginBottom:"0px"},children:(0,r.jsxs)("tbody",{children:[(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"exit_code"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" int32"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsx)(t.p,{children:"Exit code of the SQL compiler."})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"messages"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" object[]"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsx)(t.p,{children:"Messages (warnings and errors) generated by the SQL compiler."})}),(0,r.jsx)("table",{style:{display:"table",width:"100%",marginTop:"var(--ifm-table-cell-padding)",marginBottom:"0px"},children:(0,r.jsxs)("tbody",{children:[(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"end_column"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" integer"})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"end_line_number"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" integer"})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"error_type"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" string"})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"message"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" string"})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"snippet"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" string"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"start_column"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" integer"})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"start_line_number"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" integer"})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"warning"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" boolean"})]})})]})})]})})]})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"system_error"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" string"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"}),(0,r.jsxs)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:[(0,r.jsx)(t.p,{children:"System error that occurred."}),(0,r.jsxs)(t.ul,{children:["\n",(0,r.jsxs)(t.li,{children:["Set ",(0,r.jsx)(t.code,{children:"Some(...)"})," upon transition to ",(0,r.jsx)(t.code,{children:"SystemError"})]}),"\n",(0,r.jsxs)(t.li,{children:["Set ",(0,r.jsx)(t.code,{children:"None"})," upon transition to ",(0,r.jsx)(t.code,{children:"Pending"})]}),"\n"]})]})]})})]})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"program_info"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" object"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsx)(t.p,{children:"Program information is the result of the SQL compilation."})}),(0,r.jsx)("table",{style:{display:"table",width:"100%",marginTop:"var(--ifm-table-cell-padding)",marginBottom:"0px"},children:(0,r.jsxs)("tbody",{children:[(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"input_connectors"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" object"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsx)(t.p,{children:"Input connectors derived from the schema."})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"output_connectors"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" object"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsx)(t.p,{children:"Output connectors derived from the schema."})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"schema"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" object"}),(0,r.jsxs)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:[(0,r.jsx)(t.p,{children:"A struct containing the tables (inputs) and views for a program."}),(0,r.jsx)(t.p,{children:"Parse from the JSON data-type of the DDL generated by the SQL compiler."})]}),(0,r.jsx)("table",{style:{display:"table",width:"100%",marginTop:"var(--ifm-table-cell-padding)",marginBottom:"0px"},children:(0,r.jsxs)("tbody",{children:[(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"inputs"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" object[]"}),(0,r.jsx)("table",{style:{display:"table",width:"100%",marginTop:"var(--ifm-table-cell-padding)",marginBottom:"0px"},children:(0,r.jsxs)("tbody",{children:[(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"case_sensitive"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" boolean"})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"name"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" string"})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"fields"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" object[]"}),(0,r.jsx)("table",{style:{display:"table",width:"100%",marginTop:"var(--ifm-table-cell-padding)",marginBottom:"0px"},children:(0,r.jsxs)("tbody",{children:[(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"case_sensitive"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" boolean"})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"name"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" string"})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"columntype"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:"  (circular)"})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"default"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" string"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"lateness"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" string"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"unused"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" boolean"})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"watermark"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" string"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"})]})})]})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"materialized"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" boolean"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"properties"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" object"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"})]})})]})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"outputs"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" object[]"}),(0,r.jsx)("table",{style:{display:"table",width:"100%",marginTop:"var(--ifm-table-cell-padding)",marginBottom:"0px"},children:(0,r.jsxs)("tbody",{children:[(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"case_sensitive"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" boolean"})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"name"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" string"})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"fields"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" object[]"}),(0,r.jsx)("table",{style:{display:"table",width:"100%",marginTop:"var(--ifm-table-cell-padding)",marginBottom:"0px"},children:(0,r.jsxs)("tbody",{children:[(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"case_sensitive"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" boolean"})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"name"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" string"})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"columntype"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:"  (circular)"})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"default"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" string"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"lateness"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" string"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"unused"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" boolean"})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"watermark"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" string"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"})]})})]})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"materialized"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" boolean"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"properties"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" object"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"})]})})]})})]})})]})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"udf_stubs"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" string"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsx)(t.p,{children:"Generated user defined function (UDF) stubs Rust code: stubs.rs"})})]})})]})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"program_status"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" string"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsxs)(t.p,{children:[(0,r.jsx)(t.strong,{children:"Possible values:"})," [",(0,r.jsx)(t.code,{children:"Pending"}),", ",(0,r.jsx)(t.code,{children:"CompilingSql"}),", ",(0,r.jsx)(t.code,{children:"SqlCompiled"}),", ",(0,r.jsx)(t.code,{children:"CompilingRust"}),", ",(0,r.jsx)(t.code,{children:"Success"}),", ",(0,r.jsx)(t.code,{children:"SqlError"}),", ",(0,r.jsx)(t.code,{children:"RustError"}),", ",(0,r.jsx)(t.code,{children:"SystemError"}),"]"]})}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsx)(t.p,{children:"Program compilation status."})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"program_status_since"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" date-time"})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"program_version"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" int64"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsx)(t.p,{children:"Version number."})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"refresh_version"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" int64"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsx)(t.p,{children:"Version number."})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"runtime_config"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" object"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsx)(t.p,{children:"Global pipeline configuration settings. This is the publicly\nexposed type for users to configure pipelines."})}),(0,r.jsx)("table",{style:{display:"table",width:"100%",marginTop:"var(--ifm-table-cell-padding)",marginBottom:"0px"},children:(0,r.jsxs)("tbody",{children:[(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"checkpoint_during_suspend"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" boolean"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsx)(t.p,{children:"Deprecated: setting this true or false does not have an effect anymore."})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"clock_resolution_usecs"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" int64"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"}),(0,r.jsxs)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:[(0,r.jsx)(t.p,{children:"Real-time clock resolution in microseconds."}),(0,r.jsxs)(t.p,{children:["This parameter controls the execution of queries that use the ",(0,r.jsx)(t.code,{children:"NOW()"})," function.  The output of such\nqueries depends on the real-time clock and can change over time without any external\ninputs.  The pipeline will update the clock value and trigger incremental recomputation\nat most each ",(0,r.jsx)(t.code,{children:"clock_resolution_usecs"})," microseconds."]}),(0,r.jsx)(t.p,{children:"It is set to 1 second (1,000,000 microseconds) by default."}),(0,r.jsxs)(t.p,{children:["Set to ",(0,r.jsx)(t.code,{children:"null"})," to disable periodic clock updates."]})]})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"cpu_profiler"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" boolean"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"}),(0,r.jsxs)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:[(0,r.jsx)(t.p,{children:"Enable CPU profiler."}),(0,r.jsxs)(t.p,{children:["The default value is ",(0,r.jsx)(t.code,{children:"true"}),"."]})]})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"dev_tweaks"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" object"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"}),(0,r.jsxs)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:[(0,r.jsx)(t.p,{children:"Optional settings for tweaking Feldera internals."}),(0,r.jsx)(t.p,{children:"The available key-value pairs change from one version of Feldera to\nanother, so users should not depend on particular settings being\navailable, or on their behavior."})]})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"fault_tolerance"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" object"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"}),(0,r.jsxs)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:[(0,r.jsx)(t.p,{children:"Fault-tolerance configuration."}),(0,r.jsx)(t.p,{children:"The default [FtConfig] (via [FtConfig::default]) disables fault tolerance,\nwhich is the configuration that one gets if [RuntimeConfig] omits fault\ntolerance configuration."}),(0,r.jsxs)(t.p,{children:["The default value for [FtConfig::model] enables fault tolerance, as\n",(0,r.jsx)(t.code,{children:"Some(FtModel::default())"}),".  This is the configuration that one gets if\n[RuntimeConfig] includes a fault tolerance configuration but does not\nspecify a particular model."]})]}),(0,r.jsx)("table",{style:{display:"table",width:"100%",marginTop:"var(--ifm-table-cell-padding)",marginBottom:"0px"},children:(0,r.jsxs)("tbody",{children:[(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"checkpoint_interval_secs"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" int64"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"}),(0,r.jsxs)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:[(0,r.jsx)(t.p,{children:"Interval between automatic checkpoints, in seconds."}),(0,r.jsx)(t.p,{children:"The default is 60 seconds.  Values less than 1 or greater than 3600 will\nbe forced into that range."})]})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"model"}),(0,r.jsx)("span",{style:{opacity:"0.6"}}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"})]})})]})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"http_workers"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" int64"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"}),(0,r.jsxs)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:[(0,r.jsx)(t.p,{children:"Sets the number of available runtime threads for the http server."}),(0,r.jsx)(t.p,{children:"In most cases, this does not need to be set explicitly and\nthe default is sufficient. Can be increased in case the\npipeline HTTP API operations are a bottleneck."}),(0,r.jsxs)(t.p,{children:["If not specified, the default is set to ",(0,r.jsx)(t.code,{children:"workers"}),"."]})]})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"init_containers"}),(0,r.jsx)("span",{style:{opacity:"0.6"}}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsx)(t.p,{children:"Specification of additional (sidecar) containers."})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"io_workers"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" int64"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"}),(0,r.jsxs)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:[(0,r.jsx)(t.p,{children:"Sets the number of available runtime threads for async IO tasks."}),(0,r.jsx)(t.p,{children:"This affects some networking and file I/O operations\nespecially adapters and ad-hoc queries."}),(0,r.jsx)(t.p,{children:"In most cases, this does not need to be set explicitly and\nthe default is sufficient. Can be increased in case\ningress, egress or ad-hoc queries are a bottleneck."}),(0,r.jsxs)(t.p,{children:["If not specified, the default is set to ",(0,r.jsx)(t.code,{children:"workers"}),"."]})]})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"logging"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" string"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"}),(0,r.jsxs)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:[(0,r.jsx)(t.p,{children:"Log filtering directives."}),(0,r.jsxs)(t.p,{children:["If set to a valid ",(0,r.jsx)(t.a,{href:"https://docs.rs/tracing-subscriber/latest/tracing_subscriber/filter/struct.EnvFilter.html#directives",children:"tracing-subscriber"}),' filter, this controls the log\nmessages emitted by the pipeline process.  Otherwise, or if the filter\nhas invalid syntax, messages at "info" severity and higher are written\nto the log and all others are discarded.']})]})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"max_buffering_delay_usecs"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" int64"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsxs)(t.p,{children:["Maximal delay in microseconds to wait for ",(0,r.jsx)(t.code,{children:"min_batch_size_records"})," to\nget buffered by the controller, defaults to 0."]})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"max_parallel_connector_init"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" int64"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"}),(0,r.jsxs)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:[(0,r.jsx)(t.p,{children:"The maximum number of connectors initialized in parallel during pipeline\nstartup."}),(0,r.jsx)(t.p,{children:"At startup, the pipeline must initialize all of its input and output connectors.\nDepending on the number and types of connectors, this can take a long time.\nTo accelerate the process, multiple connectors are initialized concurrently.\nThis option controls the maximum number of connectors that can be initialized\nin parallel."}),(0,r.jsx)(t.p,{children:"The default is 10."})]})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"min_batch_size_records"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" int64"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"}),(0,r.jsxs)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:[(0,r.jsx)(t.p,{children:"Minimal input batch size."}),(0,r.jsxs)(t.p,{children:["The controller delays pushing input records to the circuit until at\nleast ",(0,r.jsx)(t.code,{children:"min_batch_size_records"})," records have been received (total\nacross all endpoints) or ",(0,r.jsx)(t.code,{children:"max_buffering_delay_usecs"})," microseconds\nhave passed since at least one input records has been buffered.\nDefaults to 0."]})]})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"pin_cpus"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" integer[]"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"}),(0,r.jsxs)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:[(0,r.jsx)(t.p,{children:"Optionally, a list of CPU numbers for CPUs to which the pipeline may pin\nits worker threads.  Specify at least twice as many CPU numbers as\nworkers.  CPUs are generally numbered starting from 0.  The pipeline\nmight not be able to honor CPU pinning requests."}),(0,r.jsx)(t.p,{children:"CPU pinning can make pipelines run faster and perform more consistently,\nas long as different pipelines running on the same machine are pinned to\ndifferent CPUs."})]})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"provisioning_timeout_secs"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" int64"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsxs)(t.p,{children:["Timeout in seconds for the ",(0,r.jsx)(t.code,{children:"Provisioning"})," phase of the pipeline.\nSetting this value will override the default of the runner."]})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"resources"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" object"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"}),(0,r.jsx)("table",{style:{display:"table",width:"100%",marginTop:"var(--ifm-table-cell-padding)",marginBottom:"0px"},children:(0,r.jsxs)("tbody",{children:[(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"cpu_cores_max"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" int64"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsx)(t.p,{children:"The maximum number of CPU cores to reserve\nfor an instance of this pipeline"})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"cpu_cores_min"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" int64"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsx)(t.p,{children:"The minimum number of CPU cores to reserve\nfor an instance of this pipeline"})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"memory_mb_max"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" int64"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsx)(t.p,{children:"The maximum memory in Megabytes to reserve\nfor an instance of this pipeline"})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"memory_mb_min"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" int64"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsx)(t.p,{children:"The minimum memory in Megabytes to reserve\nfor an instance of this pipeline"})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"storage_class"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" string"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsx)(t.p,{children:"Storage class to use for an instance of this pipeline.\nThe class determines storage performance such as IOPS and throughput."})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"storage_mb_max"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" int64"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsx)(t.p,{children:"The total storage in Megabytes to reserve\nfor an instance of this pipeline"})})]})})]})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"storage"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" object"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsx)(t.p,{children:"Storage configuration for a pipeline."})}),(0,r.jsx)("table",{style:{display:"table",width:"100%",marginTop:"var(--ifm-table-cell-padding)",marginBottom:"0px"},children:(0,r.jsxs)("tbody",{children:[(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"backend"}),(0,r.jsx)("span",{style:{opacity:"0.6"}}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsx)(t.p,{children:"Backend storage configuration."})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"cache_mib"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" integer"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"}),(0,r.jsxs)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:[(0,r.jsx)(t.p,{children:"The maximum size of the in-memory storage cache, in MiB."}),(0,r.jsx)(t.p,{children:"If set, the specified cache size is spread across all the foreground and\nbackground threads. If unset, each foreground or background thread cache\nis limited to 256 MiB."})]})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"compression"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" string"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsxs)(t.p,{children:[(0,r.jsx)(t.strong,{children:"Possible values:"})," [",(0,r.jsx)(t.code,{children:"default"}),", ",(0,r.jsx)(t.code,{children:"none"}),", ",(0,r.jsx)(t.code,{children:"snappy"}),"]"]})}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsx)(t.p,{children:"Storage compression algorithm."})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"min_step_storage_bytes"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" integer"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"}),(0,r.jsxs)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:[(0,r.jsx)(t.p,{children:"For a batch of data passed through the pipeline during a single step,\nthe minimum estimated number of bytes to write it to storage."}),(0,r.jsxs)(t.p,{children:["This is provided for debugging and fine-tuning and should ordinarily be\nleft unset.  A value of 0 will write even empty batches to storage, and\nnonzero values provide a threshold.  ",(0,r.jsx)(t.code,{children:"usize::MAX"}),", the default,\neffectively disables storage for such batches.  If it is set to another\nvalue, it should ordinarily be greater than or equal to\n",(0,r.jsx)(t.code,{children:"min_storage_bytes"}),"."]})]})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"min_storage_bytes"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" integer"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"}),(0,r.jsxs)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:[(0,r.jsx)(t.p,{children:"For a batch of data maintained as part of a persistent index during a\npipeline run, the minimum estimated number of bytes to write it to\nstorage."}),(0,r.jsx)(t.p,{children:"This is provided for debugging and fine-tuning and should ordinarily be\nleft unset."}),(0,r.jsxs)(t.p,{children:["A value of 0 will write even empty batches to storage, and nonzero\nvalues provide a threshold.  ",(0,r.jsx)(t.code,{children:"usize::MAX"})," would effectively disable\nstorage for such batches.  The default is 1,048,576 (1 MiB)."]})]})]})})]})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"tracing"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" boolean"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsx)(t.p,{children:"Enable pipeline tracing."})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"tracing_endpoint_jaeger"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" string"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsx)(t.p,{children:"Jaeger tracing endpoint to send tracing information to."})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"workers"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" int32"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"}),(0,r.jsxs)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:[(0,r.jsx)(t.p,{children:"Number of DBSP worker threads."}),(0,r.jsx)(t.p,{children:'Each DBSP "foreground" worker thread is paired with a "background"\nthread for LSM merging, making the total number of threads twice the\nspecified number.'}),(0,r.jsx)(t.p,{children:"The typical sweet spot for the number of workers is between 4 and 16.\nEach worker increases overall memory consumption for data structures\nused during a step."})]})]})})]})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"storage_status"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" string"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsxs)(t.p,{children:[(0,r.jsx)(t.strong,{children:"Possible values:"})," [",(0,r.jsx)(t.code,{children:"Cleared"}),", ",(0,r.jsx)(t.code,{children:"InUse"}),", ",(0,r.jsx)(t.code,{children:"Clearing"}),"]"]})}),(0,r.jsxs)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:[(0,r.jsx)(t.p,{children:"Storage status."}),(0,r.jsxs)(t.p,{children:["The storage status can only transition when the pipeline status is ",(0,r.jsx)(t.code,{children:"Stopped"}),"."]}),(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-text",children:"Cleared \u2500\u2500\u2500\u2510\n\u25b2       \u2502\n/clear \u2502       \u2502\n\u2502       \u2502\nClearing   \u2502\n\u25b2       \u2502\n\u2502       \u2502\nInUse \u25c4\u2500\u2500\u2500\u2518\n"})})]})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"udf_rust"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" string"})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"udf_toml"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" string"})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"version"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" int64"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsx)(t.p,{children:"Version number."})})]})})]})]})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsxs)("div",{style:{display:"flex"},children:[(0,r.jsx)("div",{style:{marginRight:"var(--ifm-table-cell-padding)"},children:(0,r.jsx)("code",{children:"400"})}),(0,r.jsx)("div",{})]}),(0,r.jsx)("div",{children:(0,r.jsxs)("table",{style:{display:"table",width:"100%",marginTop:"var(--ifm-table-cell-padding)",marginBottom:"0px"},children:[(0,r.jsx)("thead",{children:(0,r.jsx)("tr",{children:(0,r.jsxs)("th",{style:{textAlign:"left"},children:[(0,r.jsx)("span",{children:"Schema "}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"}),(0,r.jsx)("div",{})]})})}),(0,r.jsxs)("tbody",{children:[(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"details"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" object"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsxs)(t.p,{children:["Detailed error metadata.\nThe contents of this field is determined by ",(0,r.jsx)(t.code,{children:"error_code"}),"."]})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"error_code"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" string"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsx)(t.p,{children:"Error code is a string that specifies this error type."})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"message"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" string"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsx)(t.p,{children:"Human-readable error message."})})]})})]})]})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsxs)("div",{style:{display:"flex"},children:[(0,r.jsx)("div",{style:{marginRight:"var(--ifm-table-cell-padding)"},children:(0,r.jsx)("code",{children:"409"})}),(0,r.jsx)("div",{children:(0,r.jsx)(t.p,{children:"Cannot rename pipeline as the new name already exists"})})]}),(0,r.jsx)("div",{children:(0,r.jsxs)("table",{style:{display:"table",width:"100%",marginTop:"var(--ifm-table-cell-padding)",marginBottom:"0px"},children:[(0,r.jsx)("thead",{children:(0,r.jsx)("tr",{children:(0,r.jsxs)("th",{style:{textAlign:"left"},children:[(0,r.jsx)("span",{children:"Schema "}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"}),(0,r.jsx)("div",{})]})})}),(0,r.jsxs)("tbody",{children:[(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"details"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" object"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsxs)(t.p,{children:["Detailed error metadata.\nThe contents of this field is determined by ",(0,r.jsx)(t.code,{children:"error_code"}),"."]})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"error_code"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" string"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsx)(t.p,{children:"Error code is a string that specifies this error type."})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"message"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" string"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsx)(t.p,{children:"Human-readable error message."})})]})})]})]})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsxs)("div",{style:{display:"flex"},children:[(0,r.jsx)("div",{style:{marginRight:"var(--ifm-table-cell-padding)"},children:(0,r.jsx)("code",{children:"500"})}),(0,r.jsx)("div",{})]}),(0,r.jsx)("div",{children:(0,r.jsxs)("table",{style:{display:"table",width:"100%",marginTop:"var(--ifm-table-cell-padding)",marginBottom:"0px"},children:[(0,r.jsx)("thead",{children:(0,r.jsx)("tr",{children:(0,r.jsxs)("th",{style:{textAlign:"left"},children:[(0,r.jsx)("span",{children:"Schema "}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,r.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"}),(0,r.jsx)("div",{})]})})}),(0,r.jsxs)("tbody",{children:[(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"details"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" object"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsxs)(t.p,{children:["Detailed error metadata.\nThe contents of this field is determined by ",(0,r.jsx)(t.code,{children:"error_code"}),"."]})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"error_code"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" string"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsx)(t.p,{children:"Error code is a string that specifies this error type."})})]})}),(0,r.jsx)("tr",{children:(0,r.jsxs)("td",{children:[(0,r.jsx)("code",{children:"message"}),(0,r.jsx)("span",{style:{opacity:"0.6"},children:" string"}),(0,r.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,r.jsx)(t.p,{children:"Human-readable error message."})})]})})]})]})})]})})]})]})]})}function d(e={}){const{wrapper:t}={...(0,s.R)(),...e.components};return t?(0,r.jsx)(t,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}},28453:(e,t,n)=>{n.d(t,{R:()=>o,x:()=>a});var i=n(96540);const r={},s=i.createContext(r);function o(e){const t=i.useContext(s);return i.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function a(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),i.createElement(s.Provider,{value:t},e.children)}}}]);