"use strict";(self.webpackChunkfeldera_docs=self.webpackChunkfeldera_docs||[]).push([[6429],{28453:(t,e,n)=>{n.d(e,{R:()=>i,x:()=>o});var a=n(96540);const r={},s=a.createContext(r);function i(t){const e=a.useContext(s);return a.useMemo((function(){return"function"==typeof t?t(e):{...e,...t}}),[e,t])}function o(t){let e;return e=t.disableParentContext?"function"==typeof t.components?t.components(r):t.components||r:i(t.components),a.createElement(s.Provider,{value:e},t.children)}},84164:(t,e,n)=>{n.d(e,{A:()=>a});const a=n.p+"assets/images/tpch-schema-aa12cac3f176594b9c929c5686f9b9bc.png"},90310:(t,e,n)=>{n.r(e),n.d(e,{assets:()=>d,contentTitle:()=>o,default:()=>p,frontMatter:()=>i,metadata:()=>a,toc:()=>l});const a=JSON.parse('{"id":"use_cases/batch/part1","title":"Part 1: Create a Spark SQL batch job","description":"This tutorial demonstrates how to convert a traditional batch job into an","source":"@site/docs/use_cases/batch/part1.md","sourceDirName":"use_cases/batch","slug":"/use_cases/batch/part1","permalink":"/use_cases/batch/part1","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"docsSidebar","previous":{"title":"Accelerating Batch Analytics with Feldera","permalink":"/use_cases/batch/intro"},"next":{"title":"Part 2. Convert the Batch Job into a Feldera Pipeline","permalink":"/use_cases/batch/part2"}}');var r=n(74848),s=n(28453);const i={},o="Part 1: Create a Spark SQL batch job",d={},l=[{value:"TPC-H Schema",id:"tpc-h-schema",level:2},{value:"Step-by-Step Guide",id:"step-by-step-guide",level:2},{value:"Table Definitions",id:"table-definitions",level:3},{value:"Queries",id:"queries",level:3},{value:"Running the batch job",id:"running-the-batch-job",level:3},{value:"Takeaways",id:"takeaways",level:2}];function c(t){const e={a:"a",blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...t.components},{Details:a}=e;return a||function(t,e){throw new Error("Expected "+(e?"component":"object")+" `"+t+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(e.header,{children:(0,r.jsx)(e.h1,{id:"part-1-create-a-spark-sql-batch-job",children:"Part 1: Create a Spark SQL batch job"})}),"\n",(0,r.jsx)(e.p,{children:"This tutorial demonstrates how to convert a traditional batch job into an\nincremental pipeline. As a starting point, in this section, we build a simple\nbatch job using Apache Spark in Databricks. For this purpose, we utilize the\nTPC-H workload."}),"\n",(0,r.jsxs)(e.p,{children:["The ",(0,r.jsx)(e.a,{href:"https://www.tpc.org/tpch/",children:"TPC-H specification"})," describes itself as\nfollows:"]}),"\n",(0,r.jsxs)(e.blockquote,{children:["\n",(0,r.jsx)(e.p,{children:"The TPC-H is a decision support benchmark. It consists of a suite of business\noriented ad-hoc queries and concurrent data modifications. The queries and the\ndata populating the database have been chosen to have broad industry-wide\nrelevance. This benchmark illustrates decision support systems that examine\nlarge volumes of data, execute queries with a high degree of complexity, and\ngive answers to critical business questions."}),"\n"]}),"\n",(0,r.jsxs)(e.p,{children:["The raw data, stored in Delta Lake format, is publicly available in a S3 bucket\nat ",(0,r.jsx)(e.code,{children:"s3://batchtofeldera"}),"."]}),"\n",(0,r.jsx)(e.h2,{id:"tpc-h-schema",children:"TPC-H Schema"}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.img,{alt:"TPC-H Schema",src:n(84164).A+"",width:"600",height:"596"})}),"\n",(0,r.jsx)(e.h2,{id:"step-by-step-guide",children:"Step-by-Step Guide"}),"\n",(0,r.jsx)(e.p,{children:"Before we can create a table from our Delta Tables in S3, we must first setup\na datasource."}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Expand the Databricks Console Sidebar."}),"\n",(0,r.jsxs)(e.li,{children:["Click on ",(0,r.jsx)(e.strong,{children:"Data Ingestion"})," under the ",(0,r.jsx)(e.strong,{children:"Data Engineering"})," section."]}),"\n",(0,r.jsxs)(e.li,{children:["Click on ",(0,r.jsx)(e.strong,{children:"Create table from Amazon S3"})," under the ",(0,r.jsx)(e.strong,{children:"Files"})," section."]}),"\n",(0,r.jsx)(e.li,{children:"Provide credentials / IAM role to connect to S3."}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"table-definitions",children:"Table Definitions"}),"\n",(0,r.jsx)(e.p,{children:"Create a new SQL notebook with the following table definitions:"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-sql",children:"-- Spark SQL\nCREATE TABLE IF NOT EXISTS lineitem LOCATION 's3://batchtofeldera/lineitem';\nCREATE TABLE IF NOT EXISTS orders LOCATION 's3://batchtofeldera/orders';\nCREATE TABLE IF NOT EXISTS part LOCATION 's3://batchtofeldera/part';\nCREATE TABLE IF NOT EXISTS customer LOCATION 's3://batchtofeldera/customer';\nCREATE TABLE IF NOT EXISTS supplier LOCATION 's3://batchtofeldera/supplier';\nCREATE TABLE IF NOT EXISTS nation LOCATION 's3://batchtofeldera/nation';\nCREATE TABLE IF NOT EXISTS region LOCATION 's3://batchtofeldera/region';\nCREATE TABLE IF NOT EXISTS partsupp LOCATION 's3://batchtofeldera/partsupp';\n"})}),"\n",(0,r.jsx)(e.p,{children:"The tables in our S3 bucket have the following sizes:"}),"\n",(0,r.jsxs)(e.table,{children:[(0,r.jsx)(e.thead,{children:(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.th,{children:"Table"}),(0,r.jsx)(e.th,{children:"Records"})]})}),(0,r.jsxs)(e.tbody,{children:[(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.td,{children:"customer"}),(0,r.jsx)(e.td,{children:"15.0k"})]}),(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.td,{children:"lineitem"}),(0,r.jsx)(e.td,{children:"601k"})]}),(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.td,{children:"nation"}),(0,r.jsx)(e.td,{children:"25"})]}),(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.td,{children:"orders"}),(0,r.jsx)(e.td,{children:"150k"})]}),(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.td,{children:"part"}),(0,r.jsx)(e.td,{children:"20.0k"})]}),(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.td,{children:"partsupp"}),(0,r.jsx)(e.td,{children:"80.0k"})]}),(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.td,{children:"region"}),(0,r.jsx)(e.td,{children:"5"})]}),(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.td,{children:"supplier"}),(0,r.jsx)(e.td,{children:"1.00k"})]})]})]}),"\n",(0,r.jsx)(e.h3,{id:"queries",children:"Queries"}),"\n",(0,r.jsxs)(e.p,{children:["Add TPC-H queries as views to the notebook. For instance, the following view\nspecifies query ",(0,r.jsx)(e.strong,{children:"Q1: Pricing Summary Report"})]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-sql",children:"create view q1\nas select\n\tl_returnflag,\n\tl_linestatus,\n\tsum(l_quantity) as sum_qty,\n\tsum(l_extendedprice) as sum_base_price,\n\tsum(l_extendedprice * (1 - l_discount)) as sum_disc_price,\n\tsum(l_extendedprice * (1 - l_discount) * (1 + l_tax)) as sum_charge,\n\tavg(l_quantity) as avg_qty,\n\tavg(l_extendedprice) as avg_price,\n\tavg(l_discount) as avg_disc,\n\tcount(*) as count_order\nfrom\n\tlineitem\nwhere\n\tl_shipdate <= date '1998-12-01' - interval '90' day\ngroup by\n\tl_returnflag,\n\tl_linestatus\norder by\n\tl_returnflag,\n\tl_linestatus;\n"})}),"\n",(0,r.jsx)(e.p,{children:"Similarly, we define the remaining queries up to TPC-H Q10."}),"\n",(0,r.jsxs)(a,{children:[(0,r.jsx)("summary",{children:" Full Spark SQL Code "}),(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-sql",children:"-- Spark SQL\nCREATE TABLE IF NOT EXISTS lineitem LOCATION 's3://batchtofeldera/lineitem';\nCREATE TABLE IF NOT EXISTS orders LOCATION 's3://batchtofeldera/orders';\nCREATE TABLE IF NOT EXISTS part LOCATION 's3://batchtofeldera/part';\nCREATE TABLE IF NOT EXISTS customer LOCATION 's3://batchtofeldera/customer';\nCREATE TABLE IF NOT EXISTS supplier LOCATION 's3://batchtofeldera/supplier';\nCREATE TABLE IF NOT EXISTS nation LOCATION 's3://batchtofeldera/nation';\nCREATE TABLE IF NOT EXISTS region LOCATION 's3://batchtofeldera/region';\nCREATE TABLE IF NOT EXISTS partsupp LOCATION 's3://batchtofeldera/partsupp';\n\ncreate view q1\nas select\n\tl_returnflag,\n\tl_linestatus,\n\tsum(l_quantity) as sum_qty,\n\tsum(l_extendedprice) as sum_base_price,\n\tsum(l_extendedprice * (1 - l_discount)) as sum_disc_price,\n\tsum(l_extendedprice * (1 - l_discount) * (1 + l_tax)) as sum_charge,\n\tavg(l_quantity) as avg_qty,\n\tavg(l_extendedprice) as avg_price,\n\tavg(l_discount) as avg_disc,\n\tcount(*) as count_order\nfrom\n\tlineitem\nwhere\n\tl_shipdate <= date '1998-12-01' - interval '90' day\ngroup by\n\tl_returnflag,\n\tl_linestatus\norder by\n\tl_returnflag,\n\tl_linestatus;\n\ncreate view q2\nas select\n\ts_acctbal,\n\ts_name,\n\tn_name,\n\tp_partkey,\n\tp_mfgr,\n\ts_address,\n\ts_phone,\n\ts_comment\nfrom\n\tpart,\n\tsupplier,\n\tpartsupp,\n\tnation,\n\tregion\nwhere\n\tp_partkey = ps_partkey\n\tand s_suppkey = ps_suppkey\n\tand p_size = 15\n\tand p_type like '%BRASS'\n\tand s_nationkey = n_nationkey\n\tand n_regionkey = r_regionkey\n\tand r_name = 'EUROPE'\n\tand ps_supplycost = (\n\t\tselect\n\t\t\tmin(ps_supplycost)\n\t\tfrom\n\t\t\tpartsupp,\n\t\t\tsupplier,\n\t\t\tnation,\n\t\t\tregion\n\t\twhere\n\t\t\tp_partkey = ps_partkey\n\t\t\tand s_suppkey = ps_suppkey\n\t\t\tand s_nationkey = n_nationkey\n\t\t\tand n_regionkey = r_regionkey\n\t\t\tand r_name = 'EUROPE'\n\t)\norder by\n\ts_acctbal desc,\n\tn_name,\n\ts_name,\n\tp_partkey\nlimit 100;\n\ncreate view q3\nas select\n\tl_orderkey,\n\tsum(l_extendedprice * (1 - l_discount)) as revenue,\n\to_orderdate,\n\to_shippriority\nfrom\n\tcustomer,\n\torders,\n\tlineitem\nwhere\n\tc_mktsegment = 'BUILDING'\n\tand c_custkey = o_custkey\n\tand l_orderkey = o_orderkey\n\tand o_orderdate < date '1995-03-15'\n\tand l_shipdate > date '1995-03-15'\ngroup by\n\tl_orderkey,\n\to_orderdate,\n\to_shippriority\norder by\n\trevenue desc,\n\to_orderdate\nlimit 10;\n\ncreate view q4\nas select\n\to_orderpriority,\n\tcount(*) as order_count\nfrom\n\torders\nwhere\n\to_orderdate >= date '1993-07-01'\n\tand o_orderdate < date '1993-07-01' + interval '3' month\n\tand exists (\n\t\tselect\n\t\t\t*\n\t\tfrom\n\t\t\tlineitem\n\t\twhere\n\t\t\tl_orderkey = o_orderkey\n\t\t\tand l_commitdate < l_receiptdate\n\t)\ngroup by\n\to_orderpriority\norder by\n\to_orderpriority;\n\ncreate view q5\nas select\n\tn_name,\n\tsum(l_extendedprice * (1 - l_discount)) as revenue\nfrom\n\tcustomer,\n\torders,\n\tlineitem,\n\tsupplier,\n\tnation,\n\tregion\nwhere\n\tc_custkey = o_custkey\n\tand l_orderkey = o_orderkey\n\tand l_suppkey = s_suppkey\n\tand c_nationkey = s_nationkey\n\tand s_nationkey = n_nationkey\n\tand n_regionkey = r_regionkey\n\tand r_name = 'ASIA'\n\tand o_orderdate >= date '1994-01-01'\n\tand o_orderdate < date '1994-01-01' + interval '1' year\ngroup by\n\tn_name\norder by\n\trevenue desc;\n\ncreate view q6\nas select\n\tsum(l_extendedprice * l_discount) as revenue\nfrom\n\tlineitem\nwhere\n\tl_shipdate >= date '1994-01-01'\n\tand l_shipdate < date '1994-01-01' + interval '1' year\n\tand l_discount between .06 - 0.01 and .06 + 0.01\n\tand l_quantity < 24;\n\ncreate view q7\nas select\n\tsupp_nation,\n\tcust_nation,\n\tl_year,\n\tsum(volume) as revenue\nfrom\n\t(\n\t\tselect\n\t\t\tn1.n_name as supp_nation,\n\t\t\tn2.n_name as cust_nation,\n\t\t\tyear(l_shipdate) as l_year,\n\t\t\tl_extendedprice * (1 - l_discount) as volume\n\t\tfrom\n\t\t\tsupplier,\n\t\t\tlineitem,\n\t\t\torders,\n\t\t\tcustomer,\n\t\t\tnation n1,\n\t\t\tnation n2\n\t\twhere\n\t\t\ts_suppkey = l_suppkey\n\t\t\tand o_orderkey = l_orderkey\n\t\t\tand c_custkey = o_custkey\n\t\t\tand s_nationkey = n1.n_nationkey\n\t\t\tand c_nationkey = n2.n_nationkey\n\t\t\tand (\n\t\t\t\t(n1.n_name = 'FRANCE' and n2.n_name = 'GERMANY')\n\t\t\t\tor (n1.n_name = 'GERMANY' and n2.n_name = 'FRANCE')\n\t\t\t)\n\t\t\tand l_shipdate between date '1995-01-01' and date '1996-12-31'\n\t) as shipping\ngroup by\n\tsupp_nation,\n\tcust_nation,\n\tl_year\norder by\n\tsupp_nation,\n\tcust_nation,\n\tl_year;\n\ncreate view q8\nas select\n\to_year,\n\tsum(case\n\t\twhen nation = 'BRAZIL' then volume\n\t\telse 0\n\tend) / sum(volume) as mkt_share\nfrom\n\t(\n\t\tselect\n\t\t\tyear(o_orderdate) as o_year,\n\t\t\tl_extendedprice * (1 - l_discount) as volume,\n\t\t\tn2.n_name as nation\n\t\tfrom\n\t\t\tpart,\n\t\t\tsupplier,\n\t\t\tlineitem,\n\t\t\torders,\n\t\t\tcustomer,\n\t\t\tnation n1,\n\t\t\tnation n2,\n\t\t\tregion\n\t\twhere\n\t\t\tp_partkey = l_partkey\n\t\t\tand s_suppkey = l_suppkey\n\t\t\tand l_orderkey = o_orderkey\n\t\t\tand o_custkey = c_custkey\n\t\t\tand c_nationkey = n1.n_nationkey\n\t\t\tand n1.n_regionkey = r_regionkey\n\t\t\tand r_name = 'AMERICA'\n\t\t\tand s_nationkey = n2.n_nationkey\n\t\t\tand o_orderdate between date '1995-01-01' and date '1996-12-31'\n\t\t\tand p_type = 'ECONOMY ANODIZED STEEL'\n\t) as all_nations\ngroup by\n\to_year\norder by\n\to_year;\n\ncreate view q9\nas select\n\tnation,\n\to_year,\n\tsum(amount) as sum_profit\nfrom\n\t(\n\t\tselect\n\t\t\tn_name as nation,\n\t\t\tyear(o_orderdate) as o_year,\n\t\t\tl_extendedprice * (1 - l_discount) - ps_supplycost * l_quantity as amount\n\t\tfrom\n\t\t\tpart,\n\t\t\tsupplier,\n\t\t\tlineitem,\n\t\t\tpartsupp,\n\t\t\torders,\n\t\t\tnation\n\t\twhere\n\t\t\ts_suppkey = l_suppkey\n\t\t\tand ps_suppkey = l_suppkey\n\t\t\tand ps_partkey = l_partkey\n\t\t\tand p_partkey = l_partkey\n\t\t\tand o_orderkey = l_orderkey\n\t\t\tand s_nationkey = n_nationkey\n\t\t\tand p_name like '%green%'\n\t) as profit\ngroup by\n\tnation,\n\to_year\norder by\n\tnation,\n\to_year desc;\n\ncreate view q10\nas select\n\tc_custkey,\n\tc_name,\n\tsum(l_extendedprice * (1 - l_discount)) as revenue,\n\tc_acctbal,\n\tn_name,\n\tc_address,\n\tc_phone,\n\tc_comment\nfrom\n\tcustomer,\n\torders,\n\tlineitem,\n\tnation\nwhere\n\tc_custkey = o_custkey\n\tand l_orderkey = o_orderkey\n\tand o_orderdate >= date '1993-10-01'\n\tand o_orderdate < date '1993-10-01' + interval '3' month\n\tand l_returnflag = 'R'\n\tand c_nationkey = n_nationkey\ngroup by\n\tc_custkey,\n\tc_name,\n\tc_acctbal,\n\tc_phone,\n\tn_name,\n\tc_address,\n\tc_comment\norder by\n\trevenue desc\nlimit 20;\n"})})]}),"\n",(0,r.jsx)(e.h3,{id:"running-the-batch-job",children:"Running the batch job"}),"\n",(0,r.jsx)(e.p,{children:"Next, we query these views to simulate a batch job:"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-sql",children:"select * from q1;\nselect * from q2;\nselect * from q3;\nselect * from q4;\nselect * from q5;\nselect * from q6;\nselect * from q7;\nselect * from q8;\nselect * from q9;\nselect * from q10;\n"})}),"\n",(0,r.jsx)(e.p,{children:"We run these queries on a Databricks cluster with the following specification:"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{children:"Databricks Runtime Version: 15.4 LTS (includes Apache Spark 3.5.0, Scala 2.12)\nWorkers: 2\nWorker Type: m6i.large, 8 GB Memory, 2 Cores\nDriver Type: m6i.large, 8 GB Memory, 2 Cores\n"})}),"\n",(0,r.jsxs)(e.p,{children:["Runtime: ",(0,r.jsx)(e.strong,{children:"40.99 seconds"}),"."]}),"\n",(0,r.jsx)(e.p,{children:"If we modify the input tables by adding or removing a few records and then rerun\nthe queries, they will still take approximately 40 seconds to complete."}),"\n",(0,r.jsx)(e.h2,{id:"takeaways",children:"Takeaways"}),"\n",(0,r.jsxs)(e.p,{children:["Updating the output of a batch job incurs the same cost as the initial run, even\nwhen the input changes are small. ",(0,r.jsx)(e.strong,{children:"As a result, keeping batch job results up to\ndate can be both time-consuming and expensive."})]})]})}function p(t={}){const{wrapper:e}={...(0,s.R)(),...t.components};return e?(0,r.jsx)(e,{...t,children:(0,r.jsx)(c,{...t})}):c(t)}}}]);