"use strict";(self.webpackChunkfeldera_docs=self.webpackChunkfeldera_docs||[]).push([[8576],{28453:(e,t,n)=>{n.d(t,{R:()=>s,x:()=>a});var i=n(96540);const o={},r=i.createContext(o);function s(e){const t=i.useContext(r);return i.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function a(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:s(e.components),i.createElement(r.Provider,{value:t},e.children)}},42286:(e,t,n)=>{n.r(t),n.d(t,{contentTitle:()=>a,default:()=>c,frontMatter:()=>s,metadata:()=>i,toc:()=>l});const i=JSON.parse('{"type":"api","id":"retrieve-the-program-info-of-a-pipeline","title":"Retrieve the program info of a pipeline.","description":"","slug":"/retrieve-the-program-info-of-a-pipeline","frontMatter":{},"api":{"tags":["Pipeline management"],"operationId":"get_program_info","parameters":[{"name":"pipeline_name","in":"path","description":"Unique pipeline name","required":true,"schema":{"type":"string"}}],"responses":{"200":{"description":"Pipeline retrieved successfully","content":{"application/json":{"schema":{"type":"object","description":"Program information is the output of the SQL compiler.\\n\\nIt includes information needed for Rust compilation (e.g., generated Rust code)\\nas well as only for runtime (e.g., schema, input/output connectors).","required":["schema","input_connectors","output_connectors"],"properties":{"dataflow":{"description":"Dataflow graph of the program."},"input_connectors":{"type":"object","description":"Input connectors derived from the schema.","additionalProperties":{"description":"Describes an input connector configuration","type":"object","required":["stream"],"properties":{"stream":{"type":"string","description":"The name of the input stream of the circuit that this endpoint is\\nconnected to."},"enable_output_buffer":{"type":"boolean","description":"Enable output buffering.\\n\\nThe output buffering mechanism allows decoupling the rate at which the pipeline\\npushes changes to the output transport from the rate of input changes.\\n\\nBy default, output updates produced by the pipeline are pushed directly to\\nthe output transport. Some destinations may prefer to receive updates in fewer\\nbigger batches. For instance, when writing Parquet files, producing\\none bigger file every few minutes is usually better than creating\\nsmall files every few milliseconds.\\n\\nTo achieve such input/output decoupling, users can enable output buffering by\\nsetting the `enable_output_buffer` flag to `true`.  When buffering is enabled, output\\nupdates produced by the pipeline are consolidated in an internal buffer and are\\npushed to the output transport when one of several conditions is satisfied:\\n\\n* data has been accumulated in the buffer for more than `max_output_buffer_time_millis`\\nmilliseconds.\\n* buffer size exceeds `max_output_buffer_size_records` records.\\n\\nThis flag is `false` by default.","default":false},"max_output_buffer_size_records":{"type":"integer","description":"Maximum number of updates to be kept in the output buffer.\\n\\nThis parameter bounds the maximal size of the buffer.\\nNote that the size of the buffer is not always equal to the\\ntotal number of updates output by the pipeline. Updates to the\\nsame record can overwrite or cancel previous updates.\\n\\nBy default, the buffer can grow indefinitely until one of\\nthe other output conditions is satisfied.\\n\\nNOTE: this configuration option requires the `enable_output_buffer` flag\\nto be set.","default":18446744073709552000,"minimum":0},"max_output_buffer_time_millis":{"type":"integer","description":"Maximum time in milliseconds data is kept in the output buffer.\\n\\nBy default, data is kept in the buffer indefinitely until one of\\nthe other output conditions is satisfied.  When this option is\\nset the buffer will be flushed at most every\\n`max_output_buffer_time_millis` milliseconds.\\n\\nNOTE: this configuration option requires the `enable_output_buffer` flag\\nto be set.","default":18446744073709552000,"minimum":0},"format":{"nullable":true,"type":"object","description":"Data format specification used to parse raw data received from the\\nendpoint or to encode data sent to the endpoint.","required":["name"],"properties":{"config":{"type":"object","description":"Format-specific parser or encoder configuration."},"name":{"type":"string","description":"Format name, e.g., \\"csv\\", \\"json\\", \\"bincode\\", etc."}}},"index":{"type":"string","description":"Name of the index that the connector is attached to.\\n\\nThis property is valid for output connectors only.  It is used with data\\ntransports and formats that expect output updates in the form of key/value\\npairs, where the key typically represents a unique id associated with the\\ntable or view.\\n\\nTo support such output formats, an output connector can be attached to an\\nindex created using the SQL CREATE INDEX statement.  An index of a table\\nor view contains the same updates as the table or view itself, indexed by\\none or more key columns.\\n\\nSee individual connector documentation for details on how they work\\nwith indexes.","nullable":true},"labels":{"type":"array","items":{"type":"string"},"description":"Arbitrary user-defined text labels associated with the connector.\\n\\nThese labels can be used in conjunction with the `start_after` property\\nto control the start order of connectors."},"max_batch_size":{"type":"integer","format":"int64","description":"Maximum batch size, in records.\\n\\nThis is the maximum number of records to process in one batch through\\nthe circuit.  The time and space cost of processing a batch is\\nasymptotically superlinear in the size of the batch, but very small\\nbatches are less efficient due to constant factors.\\n\\nThis should usually be less than `max_queued_records`, to give the\\nconnector a round-trip time to restart and refill the buffer while\\nbatches are being processed.\\n\\nSome input adapters might not honor this setting.\\n\\nThe default is 10,000.","minimum":0},"max_queued_records":{"type":"integer","format":"int64","description":"Backpressure threshold.\\n\\nMaximal number of records queued by the endpoint before the endpoint\\nis paused by the backpressure mechanism.\\n\\nFor input endpoints, this setting bounds the number of records that have\\nbeen received from the input transport but haven\'t yet been consumed by\\nthe circuit since the circuit, since the circuit is still busy processing\\nprevious inputs.\\n\\nFor output endpoints, this setting bounds the number of records that have\\nbeen produced by the circuit but not yet sent via the output transport endpoint\\nnor stored in the output buffer (see `enable_output_buffer`).\\n\\nNote that this is not a hard bound: there can be a small delay between\\nthe backpressure mechanism is triggered and the endpoint is paused, during\\nwhich more data may be queued.\\n\\nThe default is 1 million.","minimum":0},"paused":{"type":"boolean","description":"Create connector in paused state.\\n\\nThe default is `false`."},"start_after":{"type":"array","items":{"type":"string"},"description":"Start the connector after all connectors with specified labels.\\n\\nThis property is used to control the start order of connectors.\\nThe connector will not start until all connectors with the specified\\nlabels have finished processing all inputs.","nullable":true},"transport":{"oneOf":[{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","description":"Configuration for reading data from a file with `FileInputTransport`","required":["path"],"properties":{"buffer_size_bytes":{"type":"integer","description":"Read buffer size.\\n\\nDefault: when this parameter is not specified, a platform-specific\\ndefault is used.","nullable":true,"minimum":0},"follow":{"type":"boolean","description":"Enable file following.\\n\\nWhen `false`, the endpoint outputs an `InputConsumer::eoi`\\nmessage and stops upon reaching the end of file.  When `true`, the\\nendpoint will keep watching the file and outputting any new content\\nappended to it."},"path":{"type":"string","description":"File path.\\n\\nThis may be a file name or a `file://` URL with an absolute path."}}},"name":{"type":"string","enum":["file_input"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","description":"Configuration for writing data to a file with `FileOutputTransport`.","required":["path"],"properties":{"path":{"type":"string","description":"File path."}}},"name":{"type":"string","enum":["file_output"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","description":"Configuration for reading data from Kafka topics with `InputTransport`.","required":["topic"],"properties":{"group_join_timeout_secs":{"type":"integer","format":"int32","description":"Maximum timeout in seconds to wait for the endpoint to join the Kafka\\nconsumer group during initialization.","minimum":0},"log_level":{"nullable":true,"type":"string","description":"Kafka logging levels.","enum":["emerg","alert","critical","error","warning","notice","info","debug"]},"partitions":{"type":"array","items":{"type":"integer","format":"int32"},"description":"The list of Kafka partitions to read from.\\n\\nOnly the specified partitions will be consumed. If this field is not set,\\nthe connector will consume from all available partitions.\\n\\nIf `start_from` is set to `offsets` and this field is provided, the\\nnumber of partitions must exactly match the number of offsets, and the\\norder of partitions must correspond to the order of offsets.\\n\\nIf offsets are provided for all partitions, this field can be omitted.","nullable":true},"poller_threads":{"type":"integer","description":"Set to 1 or more to fix the number of threads used to poll\\n`rdkafka`. Multiple threads can increase performance with small Kafka\\nmessages; for large messages, one thread is enough. In either case, too\\nmany threads can harm performance. If unset, the default is 3, which\\nhelps with small messages but will not harm performance with large\\nmessagee","nullable":true,"minimum":0},"region":{"type":"string","description":"The AWS region to use while connecting to AWS Managed Streaming for Kafka (MSK).","nullable":true},"start_from":{"oneOf":[{"type":"string","description":"Start from the beginning of the topic.","enum":["earliest"]},{"type":"string","description":"Start from the current end of the topic.\\n\\nThis will only read any data that is added to the topic after the\\nconnector initializes.","enum":["latest"]},{"type":"object","required":["offsets"],"properties":{"offsets":{"type":"array","items":{"type":"integer","format":"int64"},"description":"Start from particular offsets in the topic.\\n\\nThe number of offsets must match the number of partitions in the topic."}}}],"description":"Where to begin reading a Kafka topic."},"topic":{"type":"string","description":"Topic to subscribe to."}},"additionalProperties":{"type":"string","description":"Options passed directly to `rdkafka`.\\n\\n[`librdkafka` options](https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md)\\nused to configure the Kafka consumer.\\n\\nThis input connector does not use consumer groups, so options related to\\nconsumer groups are rejected, including:\\n\\n* `group.id`, if present, is ignored.\\n* `auto.offset.reset` (use `start_from` instead).\\n* \\"enable.auto.commit\\", if present, must be set to \\"false\\".\\n* \\"enable.auto.offset.store\\", if present, must be set to \\"false\\"."}},"name":{"type":"string","enum":["kafka_input"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","description":"Configuration for writing data to a Kafka topic with `OutputTransport`.","required":["topic"],"properties":{"fault_tolerance":{"nullable":true,"type":"object","description":"Fault tolerance configuration for Kafka output connector.","properties":{"consumer_options":{"type":"object","description":"Options passed to `rdkafka` for consumers only, as documented at\\n[`librdkafka`\\noptions](https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md).\\n\\nThese options override `kafka_options` for consumers, and may be empty.","default":{},"additionalProperties":{"type":"string"}},"producer_options":{"type":"object","description":"Options passed to `rdkafka` for producers only, as documented at\\n[`librdkafka`\\noptions](https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md).\\n\\nThese options override `kafka_options` for producers, and may be empty.","default":{},"additionalProperties":{"type":"string"}}}},"headers":{"type":"array","items":{"type":"object","description":"Kafka message header.","required":["key"],"properties":{"key":{"type":"string"},"value":{"nullable":true,"type":"string","format":"binary","description":"Kafka header value encoded as a UTF-8 string or a byte array."}}},"description":"Kafka headers to be added to each message produced by this connector."},"initialization_timeout_secs":{"type":"integer","format":"int32","description":"Maximum timeout in seconds to wait for the endpoint to connect to\\na Kafka broker.\\n\\nDefaults to 60.","minimum":0},"kafka_service":{"type":"string","description":"If specified, this service is used to provide defaults for the Kafka options.","nullable":true},"log_level":{"nullable":true,"type":"string","description":"Kafka logging levels.","enum":["emerg","alert","critical","error","warning","notice","info","debug"]},"region":{"type":"string","description":"The AWS region to use while connecting to AWS Managed Streaming for Kafka (MSK).","nullable":true},"topic":{"type":"string","description":"Topic to write to."}},"additionalProperties":{"type":"string","description":"Options passed directly to `rdkafka`.\\n\\nSee [`librdkafka` options](https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md)\\nused to configure the Kafka producer."}},"name":{"type":"string","enum":["kafka_output"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","description":"Google Pub/Sub input connector configuration.","required":["subscription"],"properties":{"connect_timeout_seconds":{"type":"integer","format":"int32","description":"gRPC connection timeout.","nullable":true,"minimum":0},"credentials":{"type":"string","description":"The content of a Google Cloud credentials JSON file.\\n\\nWhen this option is specified, the connector will use the provided credentials for\\nauthentication.  Otherwise, it will use Application Default Credentials (ADC) configured\\nin the environment where the Feldera service is running.  See\\n[Google Cloud documentation](https://cloud.google.com/docs/authentication/provide-credentials-adc)\\nfor information on configuring application default credentials.\\n\\nWhen running Feldera in an environment where ADC are not configured,\\ne.g., a Docker container, use this option to ship Google Cloud credentials from another environment.\\nFor example, if you use the\\n[`gcloud auth application-default login`](https://cloud.google.com/pubsub/docs/authentication#client-libs)\\ncommand for authentication in your local development environment, ADC are stored in the\\n`.config/gcloud/application_default_credentials.json` file in your home directory.","nullable":true},"emulator":{"type":"string","description":"Set in order to use a Pub/Sub [emulator](https://cloud.google.com/pubsub/docs/emulator)\\ninstead of the production service, e.g., \'localhost:8681\'.","nullable":true},"endpoint":{"type":"string","description":"Override the default service endpoint \'pubsub.googleapis.com\'","nullable":true},"pool_size":{"type":"integer","format":"int32","description":"gRPC channel pool size.","nullable":true,"minimum":0},"project_id":{"type":"string","description":"Google Cloud project_id.\\n\\nWhen not specified, the connector will use the project id associated\\nwith the authenticated account.","nullable":true},"snapshot":{"type":"string","description":"Reset subscription\'s backlog to a given snapshot on startup,\\nusing the Pub/Sub `Seek` API.\\n\\nThis option is mutually exclusive with the `timestamp` option.","nullable":true},"subscription":{"type":"string","description":"Subscription name."},"timeout_seconds":{"type":"integer","format":"int32","description":"gRPC request timeout.","nullable":true,"minimum":0},"timestamp":{"type":"string","description":"Reset subscription\'s backlog to a given timestamp on startup,\\nusing the Pub/Sub `Seek` API.\\n\\nThe value of this option is an ISO 8601-encoded UTC time, e.g., \\"2024-08-17T16:39:57-08:00\\".\\n\\nThis option is mutually exclusive with the `snapshot` option.","nullable":true}}},"name":{"type":"string","enum":["pub_sub_input"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","description":"Configuration for reading data from an HTTP or HTTPS URL with\\n`UrlInputTransport`.","required":["path"],"properties":{"path":{"type":"string","description":"URL."},"pause_timeout":{"type":"integer","format":"int32","description":"Timeout before disconnection when paused, in seconds.\\n\\nIf the pipeline is paused, or if the input adapter reads data faster\\nthan the pipeline can process it, then the controller will pause the\\ninput adapter. If the input adapter stays paused longer than this\\ntimeout, it will drop the network connection to the server. It will\\nautomatically reconnect when the input adapter starts running again.","minimum":0}}},"name":{"type":"string","enum":["url_input"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","description":"Configuration for reading data from AWS S3.","required":["region","bucket_name"],"properties":{"aws_access_key_id":{"type":"string","description":"AWS Access Key id. This property must be specified unless `no_sign_request` is set to `true`.","nullable":true},"aws_secret_access_key":{"type":"string","description":"Secret Access Key. This property must be specified unless `no_sign_request` is set to `true`.","nullable":true},"bucket_name":{"type":"string","description":"S3 bucket name to access."},"endpoint_url":{"type":"string","description":"The endpoint URL used to communicate with this service. Can be used to make this connector\\ntalk to non-AWS services with an S3 API.","nullable":true},"key":{"type":"string","description":"Read a single object specified by a key.","nullable":true},"max_concurrent_fetches":{"type":"integer","format":"int32","description":"Controls the number of S3 objects fetched in parallel.\\n\\nIncreasing this value can improve throughput by enabling greater concurrency.\\nHowever, higher concurrency may lead to timeouts or increased memory usage due to in-memory buffering.\\n\\nRecommended range: 1\u201310. Default: 8.","minimum":0},"no_sign_request":{"type":"boolean","description":"Do not sign requests. This is equivalent to the `--no-sign-request` flag in the AWS CLI."},"prefix":{"type":"string","description":"Read all objects whose keys match a prefix. Set to an empty string to read all objects in the bucket.","nullable":true},"region":{"type":"string","description":"AWS region."}}},"name":{"type":"string","enum":["s3_input"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","description":"Delta table input connector configuration.","required":["uri","mode"],"properties":{"cdc_delete_filter":{"type":"string","description":"A predicate that determines whether the record represents a deletion.\\n\\nThis setting is only valid in the `cdc` mode. It specifies a predicate applied to\\neach row in the Delta table to determine whether the row represents a deletion event.\\nIts value must be a valid Boolean SQL expression that can be used in a query of the\\nform `SELECT * from <table> WHERE <cdc_delete_filter>`.","nullable":true},"cdc_order_by":{"type":"string","description":"An expression that determines the ordering of updates in the Delta table.\\n\\nThis setting is only valid in the `cdc` mode. It specifies a predicate applied to\\neach row in the Delta table to determine the order in which updates in the table should\\nbe applied. Its value must be a valid SQL expression that can be used in a query of the\\nform `SELECT * from <table> ORDER BY <cdc_order_by>`.","nullable":true},"datetime":{"type":"string","description":"Optional timestamp for the snapshot in the ISO-8601/RFC-3339 format, e.g.,\\n\\"2024-12-09T16:09:53+00:00\\".\\n\\nWhen this option is set, the connector finds and opens the version of the table as of the\\nspecified point in time (based on the server time recorded in the transaction log, not the\\nevent time encoded in the data).  In `snapshot` and `snapshot_and_follow` modes, it\\nretrieves the snapshot of this version of the table.  In `follow`, `snapshot_and_follow`, and\\n`cdc` modes, it follows transaction log records **after** this version.\\n\\nNote: at most one of `version` and `datetime` options can be specified.\\nWhen neither of the two options is specified, the latest committed version of the table\\nis used.","nullable":true},"end_version":{"type":"integer","format":"int64","description":"Optional final table version.\\n\\nValid only when the connector is configured in `follow`, `snapshot_and_follow`, or `cdc` mode.\\n\\nWhen set, the connector will stop scanning the table\u2019s transaction log after reaching this version or any greater version.\\nThis bound is inclusive: if the specified version appears in the log, it will be processed before signaling end-of-input.","nullable":true},"filter":{"type":"string","description":"Optional row filter.\\n\\nWhen specified, only rows that satisfy the filter condition are read from the delta table.\\nThe condition must be a valid SQL Boolean expression that can be used in\\nthe `where` clause of the `select * from my_table where ...` query.","nullable":true},"max_concurrent_readers":{"type":"integer","format":"int32","description":"Maximum number of concurrent object store reads performed by all Delta Lake connectors.\\n\\nThis setting is used to limit the number of concurrent reads of the object store in a\\npipeline with a large number of Delta Lake connectors. When multiple connectors are simultaneously\\nreading from the object store, this can lead to transport timeouts.\\n\\nWhen enabled, this setting limits the number of concurrent reads across all connectors.\\nThis is a global setting that affects all Delta Lake connectors, and not just the connector\\nwhere it is specified. It should therefore be used at most once in a pipeline.  If multiple\\nconnectors specify this setting, they must all use the same value.\\n\\nThe default value is 6.","nullable":true,"minimum":0},"mode":{"type":"string","description":"Delta table read mode.\\n\\nThree options are available:\\n\\n* `snapshot` - read a snapshot of the table and stop.\\n\\n* `follow` - continuously ingest changes to the table, starting from a specified version\\nor timestamp.\\n\\n* `snapshot_and_follow` - read a snapshot of the table before switching to continuous ingestion\\nmode.","enum":["snapshot","follow","snapshot_and_follow","cdc"]},"num_parsers":{"type":"integer","format":"int32","description":"The number of parallel parsing tasks the connector uses to process data read from the\\ntable. Increasing this value can enhance performance by allowing more concurrent processing.\\nRecommended range: 1\u201310. The default is 4.","minimum":0},"skip_unused_columns":{"type":"boolean","description":"Don\'t read unused columns from the Delta table.\\n\\nWhen set to `true`, this option instructs the connector to avoid reading\\ncolumns from the Delta table that are not used in any view definitions.\\nTo be skipped, the columns must be either nullable or have default\\nvalues. This can improve ingestion performance, especially for wide\\ntables.\\n\\nNote: The simplest way to exclude unused columns is to omit them from the Feldera SQL table\\ndeclaration. The connector never reads columns that aren\'t declared in the SQL schema.\\nAdditionally, the SQL compiler emits warnings for declared but unused columns\u2014use these as\\na guide to optimize your schema."},"snapshot_filter":{"type":"string","description":"Optional snapshot filter.\\n\\nThis option is only valid when `mode` is set to `snapshot` or `snapshot_and_follow`.\\n\\nWhen specified, only rows that satisfy the filter condition are included in the\\nsnapshot.  The condition must be a valid SQL Boolean expression that can be used in\\nthe `where` clause of the `select * from snapshot where ...` query.\\n\\nUnlike the `filter` option, which applies to all records retrieved from the table, this\\nfilter only applies to rows in the initial snapshot of the table.\\nFor instance, it can be used to specify the range of event times to include in the snapshot,\\ne.g.: `ts BETWEEN TIMESTAMP \'2005-01-01 00:00:00\' AND TIMESTAMP \'2010-12-31 23:59:59\'`.\\n\\nThis option can be used together with the `filter` option. During the initial snapshot,\\nonly rows that satisfy both `filter` and `snapshot_filter` are retrieved from the Delta table.\\nWhen subsequently following changes in the the transaction log (`mode = snapshot_and_follow`),\\nall rows that meet the `filter` condition are ingested, regardless of `snapshot_filter`.","nullable":true},"timestamp_column":{"type":"string","description":"Table column that serves as an event timestamp.\\n\\nWhen this option is specified, and `mode` is one of `snapshot` or `snapshot_and_follow`,\\ntable rows are ingested in the timestamp order, respecting the\\n[`LATENESS`](https://docs.feldera.com/sql/streaming#lateness-expressions)\\nproperty of the column: each ingested row has a timestamp no more than `LATENESS`\\ntime units earlier than the most recent timestamp of any previously ingested row.\\nThe ingestion is performed by partitioning the table into timestamp ranges of width\\n`LATENESS`. Each range is processed sequentially, in increasing timestamp order.\\n\\n# Example\\n\\nConsider a table with timestamp column of type `TIMESTAMP` and lateness attribute\\n`INTERVAL 1 DAY`. Assuming that the oldest timestamp in the table is\\n`2024-01-01T00:00:00``, the connector will fetch all records with timestamps\\nfrom `2024-01-01`, then all records for `2024-01-02`, `2024-01-03`, etc., until all records\\nin the table have been ingested.\\n\\n# Requirements\\n\\n* The timestamp column must be of a supported type: integer, `DATE`, or `TIMESTAMP`.\\n* The timestamp column must be declared with non-zero `LATENESS`.\\n* For efficient ingest, the table must be optimized for timestamp-based\\nqueries using partitioning, Z-ordering, or liquid clustering.","nullable":true},"uri":{"type":"string","description":"Table URI.\\n\\nExample: \\"s3://feldera-fraud-detection-data/demographics_train\\""},"version":{"type":"integer","format":"int64","description":"Optional table version.\\n\\nWhen this option is set, the connector finds and opens the specified version of the table.\\nIn `snapshot` and `snapshot_and_follow` modes, it retrieves the snapshot of this version of\\nthe table.  In `follow`, `snapshot_and_follow`, and `cdc` modes, it follows transaction log records\\n**after** this version.\\n\\nNote: at most one of `version` and `datetime` options can be specified.\\nWhen neither of the two options is specified, the latest committed version of the table\\nis used.","nullable":true}},"additionalProperties":{"type":"string","description":"Storage options for configuring backend object store.\\n\\nFor specific options available for different storage backends, see:\\n* [Azure options](https://docs.rs/object_store/latest/object_store/azure/enum.AzureConfigKey.html)\\n* [Amazon S3 options](https://docs.rs/object_store/latest/object_store/aws/enum.AmazonS3ConfigKey.html)\\n* [Google Cloud Storage options](https://docs.rs/object_store/latest/object_store/gcp/enum.GoogleConfigKey.html)"}},"name":{"type":"string","enum":["delta_table_input"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","description":"Delta table output connector configuration.","required":["uri"],"properties":{"mode":{"type":"string","description":"Delta table write mode.\\n\\nDetermines how the Delta table connector handles an existing table at the target location.","enum":["append","truncate","error_if_exists"]},"uri":{"type":"string","description":"Table URI."}},"additionalProperties":{"type":"string","description":"Storage options for configuring backend object store.\\n\\nFor specific options available for different storage backends, see:\\n* [Azure options](https://docs.rs/object_store/latest/object_store/azure/enum.AzureConfigKey.html)\\n* [Amazon S3 options](https://docs.rs/object_store/latest/object_store/aws/enum.AmazonS3ConfigKey.html)\\n* [Google Cloud Storage options](https://docs.rs/object_store/latest/object_store/gcp/enum.GoogleConfigKey.html)"}},"name":{"type":"string","enum":["delta_table_output"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","description":"Redis output connector configuration.","required":["connection_string"],"properties":{"connection_string":{"type":"string","description":"The URL format: `redis://[<username>][:<password>@]<hostname>[:port][/[<db>][?protocol=<protocol>]]`\\nThis is parsed by the [redis](https://docs.rs/redis/latest/redis/#connection-parameters) crate."},"key_separator":{"type":"string","description":"Separator used to join multiple components into a single key.\\n\\":\\" by default."}}},"name":{"type":"string","enum":["redis_output"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"description":"Iceberg input connector configuration.","type":"object","properties":{"glue.access-key-id":{"type":"string","description":"Access key id used to access the Glue catalog.","nullable":true},"glue.endpoint":{"type":"string","description":"Configure an alternative endpoint of the Glue service for Glue catalog to access.\\n\\nExample: `\\"https://glue.us-east-1.amazonaws.com\\"`","nullable":true},"glue.id":{"type":"string","description":"The 12-digit ID of the Glue catalog.","nullable":true},"glue.profile-name":{"type":"string","description":"Profile used to access the Glue catalog.","nullable":true},"glue.region":{"type":"string","description":"Region of the Glue catalog.","nullable":true},"glue.secret-access-key":{"type":"string","description":"Secret access key used to access the Glue catalog.","nullable":true},"glue.session-token":{"type":"string","nullable":true},"glue.warehouse":{"type":"string","description":"Location for table metadata.\\n\\nExample: `\\"s3://my-data-warehouse/tables/\\"`","nullable":true},"rest.audience":{"type":"string","description":"Logical name of target resource or service.","nullable":true},"rest.credential":{"type":"string","description":"Credential to use for OAuth2 credential flow when initializing the catalog.\\n\\nA key and secret pair separated by \\":\\" (key is optional).","nullable":true},"rest.headers":{"type":"array","items":{"type":"array","items":{"type":"string"}},"description":"Additional HTTP request headers added to each catalog REST API call.","nullable":true},"rest.oauth2-server-uri":{"type":"string","description":"Authentication URL to use for client credentials authentication (default: uri + \'v1/oauth/tokens\')","nullable":true},"rest.prefix":{"type":"string","description":"Customize table storage paths.\\n\\nWhen combined with the `warehouse` property, the prefix determines\\nhow table data is organized within the storage.","nullable":true},"rest.resource":{"type":"string","description":"URI for the target resource or service.","nullable":true},"rest.scope":{"type":"string","nullable":true},"rest.token":{"type":"string","description":"Bearer token value to use for `Authorization` header.","nullable":true},"rest.uri":{"type":"string","description":"URI identifying the REST catalog server.","nullable":true},"rest.warehouse":{"type":"string","description":"The default location for managed tables created by the catalog.","nullable":true},"catalog_type":{"nullable":true,"type":"string","enum":["rest","glue"]},"datetime":{"type":"string","description":"Optional timestamp for the snapshot in the ISO-8601/RFC-3339 format, e.g.,\\n\\"2024-12-09T16:09:53+00:00\\".\\n\\nWhen this option is set, the connector finds and opens the snapshot of the table as of the\\nspecified point in time (based on the server time recorded in the transaction\\nlog, not the event time encoded in the data).  In `snapshot` and `snapshot_and_follow`\\nmodes, it retrieves this snapshot.  In `follow` and `snapshot_and_follow` modes, it\\nfollows transaction log records **after** this snapshot.\\n\\nNote: at most one of `snapshot_id` and `datetime` options can be specified.\\nWhen neither of the two options is specified, the latest committed version of the table\\nis used.","nullable":true},"metadata_location":{"type":"string","description":"Location of the table metadata JSON file.\\n\\nThis propery is used to access an Iceberg table without a catalog. It is mutually\\nexclusive with the `catalog_type` property.","nullable":true},"mode":{"type":"string","description":"Iceberg table read mode.\\n\\nThree options are available:\\n\\n* `snapshot` - read a snapshot of the table and stop.\\n\\n* `follow` - continuously ingest changes to the table, starting from a specified snapshot\\nor timestamp.\\n\\n* `snapshot_and_follow` - read a snapshot of the table before switching to continuous ingestion\\nmode.","enum":["snapshot","follow","snapshot_and_follow"]},"snapshot_filter":{"type":"string","description":"Optional row filter.\\n\\nThis option is only valid when `mode` is set to `snapshot` or `snapshot_and_follow`.\\n\\nWhen specified, only rows that satisfy the filter condition are included in the\\nsnapshot.  The condition must be a valid SQL Boolean expression that can be used in\\nthe `where` clause of the `select * from snapshot where ...` query.\\n\\nThis option can be used to specify the range of event times to include in the snapshot,\\ne.g.: `ts BETWEEN \'2005-01-01 00:00:00\' AND \'2010-12-31 23:59:59\'`.","nullable":true},"snapshot_id":{"type":"integer","format":"int64","description":"Optional snapshot id.\\n\\nWhen this option is set, the connector finds the specified snapshot of the table.\\nIn `snapshot` and `snapshot_and_follow` modes, it loads this snapshot.\\nIn `follow` and `snapshot_and_follow` modes, it follows table updates\\n**after** this snapshot.\\n\\nNote: at most one of `snapshot_id` and `datetime` options can be specified.\\nWhen neither of the two options is specified, the latest committed version of the table\\nis used.","nullable":true},"table_name":{"type":"string","description":"Specifies the Iceberg table name in the \\"namespace.table\\" format.\\n\\nThis option is applicable when an Iceberg catalog is configured using the `catalog_type` property.","nullable":true},"timestamp_column":{"type":"string","description":"Table column that serves as an event timestamp.\\n\\nWhen this option is specified, and `mode` is one of `snapshot` or `snapshot_and_follow`,\\ntable rows are ingested in the timestamp order, respecting the\\n[`LATENESS`](https://docs.feldera.com/sql/streaming#lateness-expressions)\\nproperty of the column: each ingested row has a timestamp no more than `LATENESS`\\ntime units earlier than the most recent timestamp of any previously ingested row.\\nThe ingestion is performed by partitioning the table into timestamp ranges of width\\n`LATENESS`. Each range is processed sequentially, in increasing timestamp order.\\n\\n# Example\\n\\nConsider a table with timestamp column of type `TIMESTAMP` and lateness attribute\\n`INTERVAL 1 DAY`. Assuming that the oldest timestamp in the table is\\n`2024-01-01T00:00:00``, the connector will fetch all records with timestamps\\nfrom `2024-01-01`, then all records for `2024-01-02`, `2024-01-03`, etc., until all records\\nin the table have been ingested.\\n\\n# Requirements\\n\\n* The timestamp column must be of a supported type: integer, `DATE`, or `TIMESTAMP`.\\n* The timestamp column must be declared with non-zero `LATENESS`.\\n* For efficient ingest, the table must be optimized for timestamp-based\\nqueries using partitioning, Z-ordering, or liquid clustering.","nullable":true}},"required":["mode"],"additionalProperties":{"type":"string","description":"Storage options for configuring backend object store.\\n\\nSee the [list of available options in PyIceberg documentation](https://py.iceberg.apache.org/configuration/#fileio)."}},"name":{"type":"string","enum":["iceberg_input"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","description":"Postgres input connector configuration.","required":["uri","query"],"properties":{"query":{"type":"string","description":"Query that specifies what data to fetch from postgres."},"uri":{"type":"string","description":"Postgres URI.\\nSee: <https://docs.rs/tokio-postgres/0.7.12/tokio_postgres/config/struct.Config.html>"}}},"name":{"type":"string","enum":["postgres_input"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","description":"Postgres output connector configuration.","required":["uri","table"],"properties":{"ssl_ca_pem":{"type":"string","description":"The CA certificate in PEM format.","nullable":true},"ssl_client_key":{"type":"string","description":"The client certificate key in PEM format.","nullable":true},"ssl_client_pem":{"type":"string","description":"The client certificate in PEM format.","nullable":true},"table":{"type":"string","description":"The table to write the output to."},"uri":{"type":"string","description":"Postgres URI.\\nSee: <https://docs.rs/tokio-postgres/0.7.12/tokio_postgres/config/struct.Config.html>"},"verify_hostname":{"type":"boolean","description":"True to enable hostname verification when using TLS. True by default.","nullable":true}}},"name":{"type":"string","enum":["postgres_output"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","description":"Configuration for generating random data for a table.","properties":{"plan":{"type":"array","items":{"type":"object","description":"A random generation plan for a table that generates either a limited amount of rows or runs continuously.","properties":{"fields":{"type":"object","description":"Specifies the values that the generator should produce.","default":{},"additionalProperties":{"type":"object","description":"Configuration for generating random data for a field of a table.","properties":{"e":{"type":"integer","format":"int64","description":"The frequency rank exponent for the Zipf distribution.\\n\\n- This value is only used if the strategy is set to `Zipf`.\\n- The default value is 1.0.","default":1},"fields":{"type":"object","description":"Specifies the values that the generator should produce in case the field is a struct type.","default":null,"additionalProperties":{"$ref":"#/components/schemas/RngFieldSettings"},"nullable":true},"key":{"default":null,"nullable":true,"$ref":"#/components/schemas/RngFieldSettings"},"null_percentage":{"type":"integer","description":"Percentage of records where this field should be set to NULL.\\n\\nIf not set, the generator will produce only records with non-NULL values.\\nIf set to `1..=100`, the generator will produce records with NULL values with the specified percentage.","default":null,"nullable":true,"minimum":0},"range":{"type":"object","description":"An optional, exclusive range [a, b) to limit the range of values the generator should produce.\\n\\n- For integer/floating point types specifies min/max values as an integer.\\nIf not set, the generator will produce values for the entire range of the type for number types.\\n- For string/binary types specifies min/max length as an integer, values are required to be >=0.\\nIf not set, a range of [0, 25) is used by default.\\n- For timestamp types specifies the min/max as two strings in the RFC 3339 format\\n(e.g., [\\"2021-01-01T00:00:00Z\\", \\"2022-01-02T00:00:00Z\\"]).\\nAlternatively, the range values can be specified as a number of non-leap\\nmilliseconds since January 1, 1970 0:00:00.000 UTC (aka \u201cUNIX timestamp\u201d).\\nIf not set, a range of [\\"1970-01-01T00:00:00Z\\", \\"2100-01-01T00:00:00Z\\") or [0, 4102444800000)\\nis used by default.\\n- For time types specifies the min/max as two strings in the \\"HH:MM:SS\\" format.\\nAlternatively, the range values can be specified in milliseconds as two positive integers.\\nIf not set, the range is 24h.\\n- For date types, the min/max range is specified as two strings in the \\"YYYY-MM-DD\\" format.\\nAlternatively, two integers that represent number of days since January 1, 1970 can be used.\\nIf not set, a range of [\\"1970-01-01\\", \\"2100-01-01\\") or [0, 54787) is used by default.\\n- For array types specifies the min/max number of elements as an integer.\\nIf not set, a range of [0, 5) is used by default. Range values are required to be >=0.\\n- For map types specifies the min/max number of key-value pairs as an integer.\\nIf not set, a range of [0, 5) is used by default.\\n- For struct/boolean/null types `range` is ignored."},"scale":{"type":"integer","format":"int64","description":"A scale factor to apply a multiplier to the generated value.\\n\\n- For integer/floating point types, the value is multiplied by the scale factor.\\n- For timestamp types, the generated value (milliseconds) is multiplied by the scale factor.\\n- For time types, the generated value (milliseconds) is multiplied by the scale factor.\\n- For date types, the generated value (days) is multiplied by the scale factor.\\n- For string/binary/array/map/struct/boolean/null types, the scale factor is ignored.\\n\\n- If `values` is specified, the scale factor is ignored.\\n- If `range` is specified and the range is required to be positive (struct, map, array etc.)\\nthe scale factor is required to be positive too.\\n\\nThe default scale factor is 1.","default":1},"strategy":{"default":"increment","type":"string","description":"Strategy used to generate values.","enum":["increment","uniform","zipf","word","words","sentence","sentences","paragraph","paragraphs","first_name","last_name","title","suffix","name","name_with_title","domain_suffix","email","username","password","field","position","seniority","job_title","ipv4","ipv6","ip","mac_address","user_agent","rfc_status_code","valid_status_code","company_suffix","company_name","buzzword","buzzword_middle","buzzword_tail","catch_phrase","bs_verb","bs_adj","bs_noun","bs","profession","industry","currency_code","currency_name","currency_symbol","credit_card_number","city_prefix","city_suffix","city_name","country_name","country_code","street_suffix","street_name","time_zone","state_name","state_abbr","secondary_address_type","secondary_address","zip_code","post_code","building_number","latitude","longitude","isbn","isbn13","isbn10","phone_number","cell_number","file_path","file_name","file_extension","dir_path"]},"value":{"default":null,"nullable":true,"$ref":"#/components/schemas/RngFieldSettings"},"values":{"type":"array","items":{"type":"object"},"description":"An optional set of values the generator will pick from.\\n\\nIf set, the generator will pick values from the specified set.\\nIf not set, the generator will produce values according to the specified range.\\nIf set to an empty set, the generator will produce NULL values.\\nIf set to a single value, the generator will produce only that value.\\n\\nNote that `range` is ignored if `values` is set.","default":null,"nullable":true}},"additionalProperties":false}},"limit":{"type":"integer","description":"Total number of new rows to generate.\\n\\nIf not set, the generator will produce new/unique records as long as the pipeline is running.\\nIf set to 0, the table will always remain empty.\\nIf set, the generator will produce new records until the specified limit is reached.\\n\\nNote that if the table has one or more primary keys that don\'t use the `increment` strategy to\\ngenerate the key there is a potential that an update is generated instead of an insert. In\\nthis case it\'s possible the total number of records is less than the specified limit.","default":null,"nullable":true,"minimum":0},"rate":{"type":"integer","format":"int32","description":"Non-zero number of rows to generate per second.\\n\\nIf not set, the generator will produce rows as fast as possible.","default":null,"nullable":true,"minimum":0},"worker_chunk_size":{"type":"integer","description":"When multiple workers are used, each worker will pick a consecutive \\"chunk\\" of\\nrecords to generate.\\n\\nBy default, if not specified, the generator will use the formula `min(rate, 10_000)`\\nto determine it. This works well in most situations. However, if you\'re\\nrunning tests with lateness and many workers you can e.g., reduce the\\nchunk size to make sure a smaller range of records is being ingested in parallel.\\n\\n# Example\\nAssume you generate a total of 125 records with 4 workers and a chunk size of 25.\\nIn this case, worker A will generate records 0..25, worker B will generate records 25..50,\\netc. A, B, C, and D will generate records in parallel. The first worker to finish its chunk\\nwill pick up the last chunk of records (100..125) to generate.","default":null,"nullable":true,"minimum":0}},"additionalProperties":false},"description":"The sequence of generations to perform.\\n\\nIf not set, the generator will produce a single sequence with default settings.\\nIf set, the generator will produce the specified sequences in sequential order.\\n\\nNote that if one of the sequences before the last one generates an unlimited number of rows\\nthe following sequences will not be executed.","default":[{"rate":null,"limit":null,"worker_chunk_size":null,"fields":{}}]},"seed":{"type":"integer","format":"int64","description":"Optional seed for the random generator.\\n\\nSetting this to a fixed value will make the generator produce the same sequence of records\\nevery time the pipeline is run.\\n\\n# Notes\\n- To ensure the set of generated input records is deterministic across multiple runs,\\napart from setting a seed, `workers` also needs to remain unchanged.\\n- The input will arrive in non-deterministic order if `workers > 1`.","default":null,"nullable":true,"minimum":0},"workers":{"type":"integer","description":"Number of workers to use for generating data.","default":1,"minimum":0}},"additionalProperties":false},"name":{"type":"string","enum":["datagen"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","description":"Configuration for generating Nexmark input data.\\n\\nThis connector must be used exactly three times in a pipeline if it is used\\nat all, once for each [`NexmarkTable`].","required":["table"],"properties":{"options":{"nullable":true,"type":"object","description":"Configuration for generating Nexmark input data.","properties":{"batch_size_per_thread":{"type":"integer","format":"int64","description":"Number of events to generate and submit together, per thread.\\n\\nEach thread generates this many records, which are then combined with\\nthe records generated by the other threads, to form combined input\\nbatches of size `threads \xd7 batch_size_per_thread`.","default":1000,"minimum":0},"events":{"type":"integer","format":"int64","description":"Number of events to generate.","default":100000000,"minimum":0},"max_step_size_per_thread":{"type":"integer","format":"int64","description":"Maximum number of events to submit in a single step, per thread.\\n\\nThis should really be per worker thread, not per generator thread, but\\nthe connector does not know how many worker threads there are.\\n\\nThis stands in for `max_batch_size` from the connector configuration\\nbecause it must be a constant across all three of the nexmark tables.","default":10000,"minimum":0},"threads":{"type":"integer","description":"Number of event generator threads.\\n\\nIt\'s reasonable to choose the same number of generator threads as worker\\nthreads.","default":4,"minimum":0}}},"table":{"type":"string","description":"Table in Nexmark.","enum":["bid","auction","person"]}}},"name":{"type":"string","enum":["nexmark"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","description":"Configuration for reading data via HTTP.\\n\\nHTTP input adapters cannot be usefully configured as part of pipeline\\nconfiguration.  Instead, instantiate them through the REST API as\\n`/pipelines/{pipeline_name}/ingress/{table_name}`.","required":["name"],"properties":{"name":{"type":"string","description":"Autogenerated name."}}},"name":{"type":"string","enum":["http_input"]}}},{"type":"object","required":["name"],"properties":{"name":{"type":"string","enum":["http_output"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","description":"Configuration for inserting data with ad-hoc queries\\n\\nAn ad-hoc input adapters cannot be usefully configured as part of pipeline\\nconfiguration.  Instead, use ad-hoc queries through the UI, the REST API, or\\nthe `fda` command-line tool.","required":["name"],"properties":{"name":{"type":"string","description":"Autogenerated name."}}},"name":{"type":"string","enum":["ad_hoc_input"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","required":["clock_resolution_usecs"],"properties":{"clock_resolution_usecs":{"type":"integer","format":"int64","minimum":0}}},"name":{"type":"string","enum":["clock_input"]}}}],"description":"Transport-specific endpoint configuration passed to\\n`crate::OutputTransport::new_endpoint`\\nand `crate::InputTransport::new_endpoint`.","discriminator":{"propertyName":"name"}}}}},"main_rust":{"type":"string","description":"Generated main program Rust code: main.rs"},"output_connectors":{"type":"object","description":"Output connectors derived from the schema.","additionalProperties":{"description":"Describes an output connector configuration","type":"object","required":["stream"],"properties":{"stream":{"type":"string","description":"The name of the output stream of the circuit that this endpoint is\\nconnected to."},"enable_output_buffer":{"type":"boolean","description":"Enable output buffering.\\n\\nThe output buffering mechanism allows decoupling the rate at which the pipeline\\npushes changes to the output transport from the rate of input changes.\\n\\nBy default, output updates produced by the pipeline are pushed directly to\\nthe output transport. Some destinations may prefer to receive updates in fewer\\nbigger batches. For instance, when writing Parquet files, producing\\none bigger file every few minutes is usually better than creating\\nsmall files every few milliseconds.\\n\\nTo achieve such input/output decoupling, users can enable output buffering by\\nsetting the `enable_output_buffer` flag to `true`.  When buffering is enabled, output\\nupdates produced by the pipeline are consolidated in an internal buffer and are\\npushed to the output transport when one of several conditions is satisfied:\\n\\n* data has been accumulated in the buffer for more than `max_output_buffer_time_millis`\\nmilliseconds.\\n* buffer size exceeds `max_output_buffer_size_records` records.\\n\\nThis flag is `false` by default.","default":false},"max_output_buffer_size_records":{"type":"integer","description":"Maximum number of updates to be kept in the output buffer.\\n\\nThis parameter bounds the maximal size of the buffer.\\nNote that the size of the buffer is not always equal to the\\ntotal number of updates output by the pipeline. Updates to the\\nsame record can overwrite or cancel previous updates.\\n\\nBy default, the buffer can grow indefinitely until one of\\nthe other output conditions is satisfied.\\n\\nNOTE: this configuration option requires the `enable_output_buffer` flag\\nto be set.","default":18446744073709552000,"minimum":0},"max_output_buffer_time_millis":{"type":"integer","description":"Maximum time in milliseconds data is kept in the output buffer.\\n\\nBy default, data is kept in the buffer indefinitely until one of\\nthe other output conditions is satisfied.  When this option is\\nset the buffer will be flushed at most every\\n`max_output_buffer_time_millis` milliseconds.\\n\\nNOTE: this configuration option requires the `enable_output_buffer` flag\\nto be set.","default":18446744073709552000,"minimum":0},"format":{"nullable":true,"type":"object","description":"Data format specification used to parse raw data received from the\\nendpoint or to encode data sent to the endpoint.","required":["name"],"properties":{"config":{"type":"object","description":"Format-specific parser or encoder configuration."},"name":{"type":"string","description":"Format name, e.g., \\"csv\\", \\"json\\", \\"bincode\\", etc."}}},"index":{"type":"string","description":"Name of the index that the connector is attached to.\\n\\nThis property is valid for output connectors only.  It is used with data\\ntransports and formats that expect output updates in the form of key/value\\npairs, where the key typically represents a unique id associated with the\\ntable or view.\\n\\nTo support such output formats, an output connector can be attached to an\\nindex created using the SQL CREATE INDEX statement.  An index of a table\\nor view contains the same updates as the table or view itself, indexed by\\none or more key columns.\\n\\nSee individual connector documentation for details on how they work\\nwith indexes.","nullable":true},"labels":{"type":"array","items":{"type":"string"},"description":"Arbitrary user-defined text labels associated with the connector.\\n\\nThese labels can be used in conjunction with the `start_after` property\\nto control the start order of connectors."},"max_batch_size":{"type":"integer","format":"int64","description":"Maximum batch size, in records.\\n\\nThis is the maximum number of records to process in one batch through\\nthe circuit.  The time and space cost of processing a batch is\\nasymptotically superlinear in the size of the batch, but very small\\nbatches are less efficient due to constant factors.\\n\\nThis should usually be less than `max_queued_records`, to give the\\nconnector a round-trip time to restart and refill the buffer while\\nbatches are being processed.\\n\\nSome input adapters might not honor this setting.\\n\\nThe default is 10,000.","minimum":0},"max_queued_records":{"type":"integer","format":"int64","description":"Backpressure threshold.\\n\\nMaximal number of records queued by the endpoint before the endpoint\\nis paused by the backpressure mechanism.\\n\\nFor input endpoints, this setting bounds the number of records that have\\nbeen received from the input transport but haven\'t yet been consumed by\\nthe circuit since the circuit, since the circuit is still busy processing\\nprevious inputs.\\n\\nFor output endpoints, this setting bounds the number of records that have\\nbeen produced by the circuit but not yet sent via the output transport endpoint\\nnor stored in the output buffer (see `enable_output_buffer`).\\n\\nNote that this is not a hard bound: there can be a small delay between\\nthe backpressure mechanism is triggered and the endpoint is paused, during\\nwhich more data may be queued.\\n\\nThe default is 1 million.","minimum":0},"paused":{"type":"boolean","description":"Create connector in paused state.\\n\\nThe default is `false`."},"start_after":{"type":"array","items":{"type":"string"},"description":"Start the connector after all connectors with specified labels.\\n\\nThis property is used to control the start order of connectors.\\nThe connector will not start until all connectors with the specified\\nlabels have finished processing all inputs.","nullable":true},"transport":{"oneOf":[{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","description":"Configuration for reading data from a file with `FileInputTransport`","required":["path"],"properties":{"buffer_size_bytes":{"type":"integer","description":"Read buffer size.\\n\\nDefault: when this parameter is not specified, a platform-specific\\ndefault is used.","nullable":true,"minimum":0},"follow":{"type":"boolean","description":"Enable file following.\\n\\nWhen `false`, the endpoint outputs an `InputConsumer::eoi`\\nmessage and stops upon reaching the end of file.  When `true`, the\\nendpoint will keep watching the file and outputting any new content\\nappended to it."},"path":{"type":"string","description":"File path.\\n\\nThis may be a file name or a `file://` URL with an absolute path."}}},"name":{"type":"string","enum":["file_input"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","description":"Configuration for writing data to a file with `FileOutputTransport`.","required":["path"],"properties":{"path":{"type":"string","description":"File path."}}},"name":{"type":"string","enum":["file_output"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","description":"Configuration for reading data from Kafka topics with `InputTransport`.","required":["topic"],"properties":{"group_join_timeout_secs":{"type":"integer","format":"int32","description":"Maximum timeout in seconds to wait for the endpoint to join the Kafka\\nconsumer group during initialization.","minimum":0},"log_level":{"nullable":true,"type":"string","description":"Kafka logging levels.","enum":["emerg","alert","critical","error","warning","notice","info","debug"]},"partitions":{"type":"array","items":{"type":"integer","format":"int32"},"description":"The list of Kafka partitions to read from.\\n\\nOnly the specified partitions will be consumed. If this field is not set,\\nthe connector will consume from all available partitions.\\n\\nIf `start_from` is set to `offsets` and this field is provided, the\\nnumber of partitions must exactly match the number of offsets, and the\\norder of partitions must correspond to the order of offsets.\\n\\nIf offsets are provided for all partitions, this field can be omitted.","nullable":true},"poller_threads":{"type":"integer","description":"Set to 1 or more to fix the number of threads used to poll\\n`rdkafka`. Multiple threads can increase performance with small Kafka\\nmessages; for large messages, one thread is enough. In either case, too\\nmany threads can harm performance. If unset, the default is 3, which\\nhelps with small messages but will not harm performance with large\\nmessagee","nullable":true,"minimum":0},"region":{"type":"string","description":"The AWS region to use while connecting to AWS Managed Streaming for Kafka (MSK).","nullable":true},"start_from":{"oneOf":[{"type":"string","description":"Start from the beginning of the topic.","enum":["earliest"]},{"type":"string","description":"Start from the current end of the topic.\\n\\nThis will only read any data that is added to the topic after the\\nconnector initializes.","enum":["latest"]},{"type":"object","required":["offsets"],"properties":{"offsets":{"type":"array","items":{"type":"integer","format":"int64"},"description":"Start from particular offsets in the topic.\\n\\nThe number of offsets must match the number of partitions in the topic."}}}],"description":"Where to begin reading a Kafka topic."},"topic":{"type":"string","description":"Topic to subscribe to."}},"additionalProperties":{"type":"string","description":"Options passed directly to `rdkafka`.\\n\\n[`librdkafka` options](https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md)\\nused to configure the Kafka consumer.\\n\\nThis input connector does not use consumer groups, so options related to\\nconsumer groups are rejected, including:\\n\\n* `group.id`, if present, is ignored.\\n* `auto.offset.reset` (use `start_from` instead).\\n* \\"enable.auto.commit\\", if present, must be set to \\"false\\".\\n* \\"enable.auto.offset.store\\", if present, must be set to \\"false\\"."}},"name":{"type":"string","enum":["kafka_input"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","description":"Configuration for writing data to a Kafka topic with `OutputTransport`.","required":["topic"],"properties":{"fault_tolerance":{"nullable":true,"type":"object","description":"Fault tolerance configuration for Kafka output connector.","properties":{"consumer_options":{"type":"object","description":"Options passed to `rdkafka` for consumers only, as documented at\\n[`librdkafka`\\noptions](https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md).\\n\\nThese options override `kafka_options` for consumers, and may be empty.","default":{},"additionalProperties":{"type":"string"}},"producer_options":{"type":"object","description":"Options passed to `rdkafka` for producers only, as documented at\\n[`librdkafka`\\noptions](https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md).\\n\\nThese options override `kafka_options` for producers, and may be empty.","default":{},"additionalProperties":{"type":"string"}}}},"headers":{"type":"array","items":{"type":"object","description":"Kafka message header.","required":["key"],"properties":{"key":{"type":"string"},"value":{"nullable":true,"type":"string","format":"binary","description":"Kafka header value encoded as a UTF-8 string or a byte array."}}},"description":"Kafka headers to be added to each message produced by this connector."},"initialization_timeout_secs":{"type":"integer","format":"int32","description":"Maximum timeout in seconds to wait for the endpoint to connect to\\na Kafka broker.\\n\\nDefaults to 60.","minimum":0},"kafka_service":{"type":"string","description":"If specified, this service is used to provide defaults for the Kafka options.","nullable":true},"log_level":{"nullable":true,"type":"string","description":"Kafka logging levels.","enum":["emerg","alert","critical","error","warning","notice","info","debug"]},"region":{"type":"string","description":"The AWS region to use while connecting to AWS Managed Streaming for Kafka (MSK).","nullable":true},"topic":{"type":"string","description":"Topic to write to."}},"additionalProperties":{"type":"string","description":"Options passed directly to `rdkafka`.\\n\\nSee [`librdkafka` options](https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md)\\nused to configure the Kafka producer."}},"name":{"type":"string","enum":["kafka_output"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","description":"Google Pub/Sub input connector configuration.","required":["subscription"],"properties":{"connect_timeout_seconds":{"type":"integer","format":"int32","description":"gRPC connection timeout.","nullable":true,"minimum":0},"credentials":{"type":"string","description":"The content of a Google Cloud credentials JSON file.\\n\\nWhen this option is specified, the connector will use the provided credentials for\\nauthentication.  Otherwise, it will use Application Default Credentials (ADC) configured\\nin the environment where the Feldera service is running.  See\\n[Google Cloud documentation](https://cloud.google.com/docs/authentication/provide-credentials-adc)\\nfor information on configuring application default credentials.\\n\\nWhen running Feldera in an environment where ADC are not configured,\\ne.g., a Docker container, use this option to ship Google Cloud credentials from another environment.\\nFor example, if you use the\\n[`gcloud auth application-default login`](https://cloud.google.com/pubsub/docs/authentication#client-libs)\\ncommand for authentication in your local development environment, ADC are stored in the\\n`.config/gcloud/application_default_credentials.json` file in your home directory.","nullable":true},"emulator":{"type":"string","description":"Set in order to use a Pub/Sub [emulator](https://cloud.google.com/pubsub/docs/emulator)\\ninstead of the production service, e.g., \'localhost:8681\'.","nullable":true},"endpoint":{"type":"string","description":"Override the default service endpoint \'pubsub.googleapis.com\'","nullable":true},"pool_size":{"type":"integer","format":"int32","description":"gRPC channel pool size.","nullable":true,"minimum":0},"project_id":{"type":"string","description":"Google Cloud project_id.\\n\\nWhen not specified, the connector will use the project id associated\\nwith the authenticated account.","nullable":true},"snapshot":{"type":"string","description":"Reset subscription\'s backlog to a given snapshot on startup,\\nusing the Pub/Sub `Seek` API.\\n\\nThis option is mutually exclusive with the `timestamp` option.","nullable":true},"subscription":{"type":"string","description":"Subscription name."},"timeout_seconds":{"type":"integer","format":"int32","description":"gRPC request timeout.","nullable":true,"minimum":0},"timestamp":{"type":"string","description":"Reset subscription\'s backlog to a given timestamp on startup,\\nusing the Pub/Sub `Seek` API.\\n\\nThe value of this option is an ISO 8601-encoded UTC time, e.g., \\"2024-08-17T16:39:57-08:00\\".\\n\\nThis option is mutually exclusive with the `snapshot` option.","nullable":true}}},"name":{"type":"string","enum":["pub_sub_input"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","description":"Configuration for reading data from an HTTP or HTTPS URL with\\n`UrlInputTransport`.","required":["path"],"properties":{"path":{"type":"string","description":"URL."},"pause_timeout":{"type":"integer","format":"int32","description":"Timeout before disconnection when paused, in seconds.\\n\\nIf the pipeline is paused, or if the input adapter reads data faster\\nthan the pipeline can process it, then the controller will pause the\\ninput adapter. If the input adapter stays paused longer than this\\ntimeout, it will drop the network connection to the server. It will\\nautomatically reconnect when the input adapter starts running again.","minimum":0}}},"name":{"type":"string","enum":["url_input"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","description":"Configuration for reading data from AWS S3.","required":["region","bucket_name"],"properties":{"aws_access_key_id":{"type":"string","description":"AWS Access Key id. This property must be specified unless `no_sign_request` is set to `true`.","nullable":true},"aws_secret_access_key":{"type":"string","description":"Secret Access Key. This property must be specified unless `no_sign_request` is set to `true`.","nullable":true},"bucket_name":{"type":"string","description":"S3 bucket name to access."},"endpoint_url":{"type":"string","description":"The endpoint URL used to communicate with this service. Can be used to make this connector\\ntalk to non-AWS services with an S3 API.","nullable":true},"key":{"type":"string","description":"Read a single object specified by a key.","nullable":true},"max_concurrent_fetches":{"type":"integer","format":"int32","description":"Controls the number of S3 objects fetched in parallel.\\n\\nIncreasing this value can improve throughput by enabling greater concurrency.\\nHowever, higher concurrency may lead to timeouts or increased memory usage due to in-memory buffering.\\n\\nRecommended range: 1\u201310. Default: 8.","minimum":0},"no_sign_request":{"type":"boolean","description":"Do not sign requests. This is equivalent to the `--no-sign-request` flag in the AWS CLI."},"prefix":{"type":"string","description":"Read all objects whose keys match a prefix. Set to an empty string to read all objects in the bucket.","nullable":true},"region":{"type":"string","description":"AWS region."}}},"name":{"type":"string","enum":["s3_input"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","description":"Delta table input connector configuration.","required":["uri","mode"],"properties":{"cdc_delete_filter":{"type":"string","description":"A predicate that determines whether the record represents a deletion.\\n\\nThis setting is only valid in the `cdc` mode. It specifies a predicate applied to\\neach row in the Delta table to determine whether the row represents a deletion event.\\nIts value must be a valid Boolean SQL expression that can be used in a query of the\\nform `SELECT * from <table> WHERE <cdc_delete_filter>`.","nullable":true},"cdc_order_by":{"type":"string","description":"An expression that determines the ordering of updates in the Delta table.\\n\\nThis setting is only valid in the `cdc` mode. It specifies a predicate applied to\\neach row in the Delta table to determine the order in which updates in the table should\\nbe applied. Its value must be a valid SQL expression that can be used in a query of the\\nform `SELECT * from <table> ORDER BY <cdc_order_by>`.","nullable":true},"datetime":{"type":"string","description":"Optional timestamp for the snapshot in the ISO-8601/RFC-3339 format, e.g.,\\n\\"2024-12-09T16:09:53+00:00\\".\\n\\nWhen this option is set, the connector finds and opens the version of the table as of the\\nspecified point in time (based on the server time recorded in the transaction log, not the\\nevent time encoded in the data).  In `snapshot` and `snapshot_and_follow` modes, it\\nretrieves the snapshot of this version of the table.  In `follow`, `snapshot_and_follow`, and\\n`cdc` modes, it follows transaction log records **after** this version.\\n\\nNote: at most one of `version` and `datetime` options can be specified.\\nWhen neither of the two options is specified, the latest committed version of the table\\nis used.","nullable":true},"end_version":{"type":"integer","format":"int64","description":"Optional final table version.\\n\\nValid only when the connector is configured in `follow`, `snapshot_and_follow`, or `cdc` mode.\\n\\nWhen set, the connector will stop scanning the table\u2019s transaction log after reaching this version or any greater version.\\nThis bound is inclusive: if the specified version appears in the log, it will be processed before signaling end-of-input.","nullable":true},"filter":{"type":"string","description":"Optional row filter.\\n\\nWhen specified, only rows that satisfy the filter condition are read from the delta table.\\nThe condition must be a valid SQL Boolean expression that can be used in\\nthe `where` clause of the `select * from my_table where ...` query.","nullable":true},"max_concurrent_readers":{"type":"integer","format":"int32","description":"Maximum number of concurrent object store reads performed by all Delta Lake connectors.\\n\\nThis setting is used to limit the number of concurrent reads of the object store in a\\npipeline with a large number of Delta Lake connectors. When multiple connectors are simultaneously\\nreading from the object store, this can lead to transport timeouts.\\n\\nWhen enabled, this setting limits the number of concurrent reads across all connectors.\\nThis is a global setting that affects all Delta Lake connectors, and not just the connector\\nwhere it is specified. It should therefore be used at most once in a pipeline.  If multiple\\nconnectors specify this setting, they must all use the same value.\\n\\nThe default value is 6.","nullable":true,"minimum":0},"mode":{"type":"string","description":"Delta table read mode.\\n\\nThree options are available:\\n\\n* `snapshot` - read a snapshot of the table and stop.\\n\\n* `follow` - continuously ingest changes to the table, starting from a specified version\\nor timestamp.\\n\\n* `snapshot_and_follow` - read a snapshot of the table before switching to continuous ingestion\\nmode.","enum":["snapshot","follow","snapshot_and_follow","cdc"]},"num_parsers":{"type":"integer","format":"int32","description":"The number of parallel parsing tasks the connector uses to process data read from the\\ntable. Increasing this value can enhance performance by allowing more concurrent processing.\\nRecommended range: 1\u201310. The default is 4.","minimum":0},"skip_unused_columns":{"type":"boolean","description":"Don\'t read unused columns from the Delta table.\\n\\nWhen set to `true`, this option instructs the connector to avoid reading\\ncolumns from the Delta table that are not used in any view definitions.\\nTo be skipped, the columns must be either nullable or have default\\nvalues. This can improve ingestion performance, especially for wide\\ntables.\\n\\nNote: The simplest way to exclude unused columns is to omit them from the Feldera SQL table\\ndeclaration. The connector never reads columns that aren\'t declared in the SQL schema.\\nAdditionally, the SQL compiler emits warnings for declared but unused columns\u2014use these as\\na guide to optimize your schema."},"snapshot_filter":{"type":"string","description":"Optional snapshot filter.\\n\\nThis option is only valid when `mode` is set to `snapshot` or `snapshot_and_follow`.\\n\\nWhen specified, only rows that satisfy the filter condition are included in the\\nsnapshot.  The condition must be a valid SQL Boolean expression that can be used in\\nthe `where` clause of the `select * from snapshot where ...` query.\\n\\nUnlike the `filter` option, which applies to all records retrieved from the table, this\\nfilter only applies to rows in the initial snapshot of the table.\\nFor instance, it can be used to specify the range of event times to include in the snapshot,\\ne.g.: `ts BETWEEN TIMESTAMP \'2005-01-01 00:00:00\' AND TIMESTAMP \'2010-12-31 23:59:59\'`.\\n\\nThis option can be used together with the `filter` option. During the initial snapshot,\\nonly rows that satisfy both `filter` and `snapshot_filter` are retrieved from the Delta table.\\nWhen subsequently following changes in the the transaction log (`mode = snapshot_and_follow`),\\nall rows that meet the `filter` condition are ingested, regardless of `snapshot_filter`.","nullable":true},"timestamp_column":{"type":"string","description":"Table column that serves as an event timestamp.\\n\\nWhen this option is specified, and `mode` is one of `snapshot` or `snapshot_and_follow`,\\ntable rows are ingested in the timestamp order, respecting the\\n[`LATENESS`](https://docs.feldera.com/sql/streaming#lateness-expressions)\\nproperty of the column: each ingested row has a timestamp no more than `LATENESS`\\ntime units earlier than the most recent timestamp of any previously ingested row.\\nThe ingestion is performed by partitioning the table into timestamp ranges of width\\n`LATENESS`. Each range is processed sequentially, in increasing timestamp order.\\n\\n# Example\\n\\nConsider a table with timestamp column of type `TIMESTAMP` and lateness attribute\\n`INTERVAL 1 DAY`. Assuming that the oldest timestamp in the table is\\n`2024-01-01T00:00:00``, the connector will fetch all records with timestamps\\nfrom `2024-01-01`, then all records for `2024-01-02`, `2024-01-03`, etc., until all records\\nin the table have been ingested.\\n\\n# Requirements\\n\\n* The timestamp column must be of a supported type: integer, `DATE`, or `TIMESTAMP`.\\n* The timestamp column must be declared with non-zero `LATENESS`.\\n* For efficient ingest, the table must be optimized for timestamp-based\\nqueries using partitioning, Z-ordering, or liquid clustering.","nullable":true},"uri":{"type":"string","description":"Table URI.\\n\\nExample: \\"s3://feldera-fraud-detection-data/demographics_train\\""},"version":{"type":"integer","format":"int64","description":"Optional table version.\\n\\nWhen this option is set, the connector finds and opens the specified version of the table.\\nIn `snapshot` and `snapshot_and_follow` modes, it retrieves the snapshot of this version of\\nthe table.  In `follow`, `snapshot_and_follow`, and `cdc` modes, it follows transaction log records\\n**after** this version.\\n\\nNote: at most one of `version` and `datetime` options can be specified.\\nWhen neither of the two options is specified, the latest committed version of the table\\nis used.","nullable":true}},"additionalProperties":{"type":"string","description":"Storage options for configuring backend object store.\\n\\nFor specific options available for different storage backends, see:\\n* [Azure options](https://docs.rs/object_store/latest/object_store/azure/enum.AzureConfigKey.html)\\n* [Amazon S3 options](https://docs.rs/object_store/latest/object_store/aws/enum.AmazonS3ConfigKey.html)\\n* [Google Cloud Storage options](https://docs.rs/object_store/latest/object_store/gcp/enum.GoogleConfigKey.html)"}},"name":{"type":"string","enum":["delta_table_input"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","description":"Delta table output connector configuration.","required":["uri"],"properties":{"mode":{"type":"string","description":"Delta table write mode.\\n\\nDetermines how the Delta table connector handles an existing table at the target location.","enum":["append","truncate","error_if_exists"]},"uri":{"type":"string","description":"Table URI."}},"additionalProperties":{"type":"string","description":"Storage options for configuring backend object store.\\n\\nFor specific options available for different storage backends, see:\\n* [Azure options](https://docs.rs/object_store/latest/object_store/azure/enum.AzureConfigKey.html)\\n* [Amazon S3 options](https://docs.rs/object_store/latest/object_store/aws/enum.AmazonS3ConfigKey.html)\\n* [Google Cloud Storage options](https://docs.rs/object_store/latest/object_store/gcp/enum.GoogleConfigKey.html)"}},"name":{"type":"string","enum":["delta_table_output"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","description":"Redis output connector configuration.","required":["connection_string"],"properties":{"connection_string":{"type":"string","description":"The URL format: `redis://[<username>][:<password>@]<hostname>[:port][/[<db>][?protocol=<protocol>]]`\\nThis is parsed by the [redis](https://docs.rs/redis/latest/redis/#connection-parameters) crate."},"key_separator":{"type":"string","description":"Separator used to join multiple components into a single key.\\n\\":\\" by default."}}},"name":{"type":"string","enum":["redis_output"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"description":"Iceberg input connector configuration.","type":"object","properties":{"glue.access-key-id":{"type":"string","description":"Access key id used to access the Glue catalog.","nullable":true},"glue.endpoint":{"type":"string","description":"Configure an alternative endpoint of the Glue service for Glue catalog to access.\\n\\nExample: `\\"https://glue.us-east-1.amazonaws.com\\"`","nullable":true},"glue.id":{"type":"string","description":"The 12-digit ID of the Glue catalog.","nullable":true},"glue.profile-name":{"type":"string","description":"Profile used to access the Glue catalog.","nullable":true},"glue.region":{"type":"string","description":"Region of the Glue catalog.","nullable":true},"glue.secret-access-key":{"type":"string","description":"Secret access key used to access the Glue catalog.","nullable":true},"glue.session-token":{"type":"string","nullable":true},"glue.warehouse":{"type":"string","description":"Location for table metadata.\\n\\nExample: `\\"s3://my-data-warehouse/tables/\\"`","nullable":true},"rest.audience":{"type":"string","description":"Logical name of target resource or service.","nullable":true},"rest.credential":{"type":"string","description":"Credential to use for OAuth2 credential flow when initializing the catalog.\\n\\nA key and secret pair separated by \\":\\" (key is optional).","nullable":true},"rest.headers":{"type":"array","items":{"type":"array","items":{"type":"string"}},"description":"Additional HTTP request headers added to each catalog REST API call.","nullable":true},"rest.oauth2-server-uri":{"type":"string","description":"Authentication URL to use for client credentials authentication (default: uri + \'v1/oauth/tokens\')","nullable":true},"rest.prefix":{"type":"string","description":"Customize table storage paths.\\n\\nWhen combined with the `warehouse` property, the prefix determines\\nhow table data is organized within the storage.","nullable":true},"rest.resource":{"type":"string","description":"URI for the target resource or service.","nullable":true},"rest.scope":{"type":"string","nullable":true},"rest.token":{"type":"string","description":"Bearer token value to use for `Authorization` header.","nullable":true},"rest.uri":{"type":"string","description":"URI identifying the REST catalog server.","nullable":true},"rest.warehouse":{"type":"string","description":"The default location for managed tables created by the catalog.","nullable":true},"catalog_type":{"nullable":true,"type":"string","enum":["rest","glue"]},"datetime":{"type":"string","description":"Optional timestamp for the snapshot in the ISO-8601/RFC-3339 format, e.g.,\\n\\"2024-12-09T16:09:53+00:00\\".\\n\\nWhen this option is set, the connector finds and opens the snapshot of the table as of the\\nspecified point in time (based on the server time recorded in the transaction\\nlog, not the event time encoded in the data).  In `snapshot` and `snapshot_and_follow`\\nmodes, it retrieves this snapshot.  In `follow` and `snapshot_and_follow` modes, it\\nfollows transaction log records **after** this snapshot.\\n\\nNote: at most one of `snapshot_id` and `datetime` options can be specified.\\nWhen neither of the two options is specified, the latest committed version of the table\\nis used.","nullable":true},"metadata_location":{"type":"string","description":"Location of the table metadata JSON file.\\n\\nThis propery is used to access an Iceberg table without a catalog. It is mutually\\nexclusive with the `catalog_type` property.","nullable":true},"mode":{"type":"string","description":"Iceberg table read mode.\\n\\nThree options are available:\\n\\n* `snapshot` - read a snapshot of the table and stop.\\n\\n* `follow` - continuously ingest changes to the table, starting from a specified snapshot\\nor timestamp.\\n\\n* `snapshot_and_follow` - read a snapshot of the table before switching to continuous ingestion\\nmode.","enum":["snapshot","follow","snapshot_and_follow"]},"snapshot_filter":{"type":"string","description":"Optional row filter.\\n\\nThis option is only valid when `mode` is set to `snapshot` or `snapshot_and_follow`.\\n\\nWhen specified, only rows that satisfy the filter condition are included in the\\nsnapshot.  The condition must be a valid SQL Boolean expression that can be used in\\nthe `where` clause of the `select * from snapshot where ...` query.\\n\\nThis option can be used to specify the range of event times to include in the snapshot,\\ne.g.: `ts BETWEEN \'2005-01-01 00:00:00\' AND \'2010-12-31 23:59:59\'`.","nullable":true},"snapshot_id":{"type":"integer","format":"int64","description":"Optional snapshot id.\\n\\nWhen this option is set, the connector finds the specified snapshot of the table.\\nIn `snapshot` and `snapshot_and_follow` modes, it loads this snapshot.\\nIn `follow` and `snapshot_and_follow` modes, it follows table updates\\n**after** this snapshot.\\n\\nNote: at most one of `snapshot_id` and `datetime` options can be specified.\\nWhen neither of the two options is specified, the latest committed version of the table\\nis used.","nullable":true},"table_name":{"type":"string","description":"Specifies the Iceberg table name in the \\"namespace.table\\" format.\\n\\nThis option is applicable when an Iceberg catalog is configured using the `catalog_type` property.","nullable":true},"timestamp_column":{"type":"string","description":"Table column that serves as an event timestamp.\\n\\nWhen this option is specified, and `mode` is one of `snapshot` or `snapshot_and_follow`,\\ntable rows are ingested in the timestamp order, respecting the\\n[`LATENESS`](https://docs.feldera.com/sql/streaming#lateness-expressions)\\nproperty of the column: each ingested row has a timestamp no more than `LATENESS`\\ntime units earlier than the most recent timestamp of any previously ingested row.\\nThe ingestion is performed by partitioning the table into timestamp ranges of width\\n`LATENESS`. Each range is processed sequentially, in increasing timestamp order.\\n\\n# Example\\n\\nConsider a table with timestamp column of type `TIMESTAMP` and lateness attribute\\n`INTERVAL 1 DAY`. Assuming that the oldest timestamp in the table is\\n`2024-01-01T00:00:00``, the connector will fetch all records with timestamps\\nfrom `2024-01-01`, then all records for `2024-01-02`, `2024-01-03`, etc., until all records\\nin the table have been ingested.\\n\\n# Requirements\\n\\n* The timestamp column must be of a supported type: integer, `DATE`, or `TIMESTAMP`.\\n* The timestamp column must be declared with non-zero `LATENESS`.\\n* For efficient ingest, the table must be optimized for timestamp-based\\nqueries using partitioning, Z-ordering, or liquid clustering.","nullable":true}},"required":["mode"],"additionalProperties":{"type":"string","description":"Storage options for configuring backend object store.\\n\\nSee the [list of available options in PyIceberg documentation](https://py.iceberg.apache.org/configuration/#fileio)."}},"name":{"type":"string","enum":["iceberg_input"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","description":"Postgres input connector configuration.","required":["uri","query"],"properties":{"query":{"type":"string","description":"Query that specifies what data to fetch from postgres."},"uri":{"type":"string","description":"Postgres URI.\\nSee: <https://docs.rs/tokio-postgres/0.7.12/tokio_postgres/config/struct.Config.html>"}}},"name":{"type":"string","enum":["postgres_input"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","description":"Postgres output connector configuration.","required":["uri","table"],"properties":{"ssl_ca_pem":{"type":"string","description":"The CA certificate in PEM format.","nullable":true},"ssl_client_key":{"type":"string","description":"The client certificate key in PEM format.","nullable":true},"ssl_client_pem":{"type":"string","description":"The client certificate in PEM format.","nullable":true},"table":{"type":"string","description":"The table to write the output to."},"uri":{"type":"string","description":"Postgres URI.\\nSee: <https://docs.rs/tokio-postgres/0.7.12/tokio_postgres/config/struct.Config.html>"},"verify_hostname":{"type":"boolean","description":"True to enable hostname verification when using TLS. True by default.","nullable":true}}},"name":{"type":"string","enum":["postgres_output"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","description":"Configuration for generating random data for a table.","properties":{"plan":{"type":"array","items":{"type":"object","description":"A random generation plan for a table that generates either a limited amount of rows or runs continuously.","properties":{"fields":{"type":"object","description":"Specifies the values that the generator should produce.","default":{},"additionalProperties":{"type":"object","description":"Configuration for generating random data for a field of a table.","properties":{"e":{"type":"integer","format":"int64","description":"The frequency rank exponent for the Zipf distribution.\\n\\n- This value is only used if the strategy is set to `Zipf`.\\n- The default value is 1.0.","default":1},"fields":{"type":"object","description":"Specifies the values that the generator should produce in case the field is a struct type.","default":null,"additionalProperties":{"$ref":"#/components/schemas/RngFieldSettings"},"nullable":true},"key":{"default":null,"nullable":true,"$ref":"#/components/schemas/RngFieldSettings"},"null_percentage":{"type":"integer","description":"Percentage of records where this field should be set to NULL.\\n\\nIf not set, the generator will produce only records with non-NULL values.\\nIf set to `1..=100`, the generator will produce records with NULL values with the specified percentage.","default":null,"nullable":true,"minimum":0},"range":{"type":"object","description":"An optional, exclusive range [a, b) to limit the range of values the generator should produce.\\n\\n- For integer/floating point types specifies min/max values as an integer.\\nIf not set, the generator will produce values for the entire range of the type for number types.\\n- For string/binary types specifies min/max length as an integer, values are required to be >=0.\\nIf not set, a range of [0, 25) is used by default.\\n- For timestamp types specifies the min/max as two strings in the RFC 3339 format\\n(e.g., [\\"2021-01-01T00:00:00Z\\", \\"2022-01-02T00:00:00Z\\"]).\\nAlternatively, the range values can be specified as a number of non-leap\\nmilliseconds since January 1, 1970 0:00:00.000 UTC (aka \u201cUNIX timestamp\u201d).\\nIf not set, a range of [\\"1970-01-01T00:00:00Z\\", \\"2100-01-01T00:00:00Z\\") or [0, 4102444800000)\\nis used by default.\\n- For time types specifies the min/max as two strings in the \\"HH:MM:SS\\" format.\\nAlternatively, the range values can be specified in milliseconds as two positive integers.\\nIf not set, the range is 24h.\\n- For date types, the min/max range is specified as two strings in the \\"YYYY-MM-DD\\" format.\\nAlternatively, two integers that represent number of days since January 1, 1970 can be used.\\nIf not set, a range of [\\"1970-01-01\\", \\"2100-01-01\\") or [0, 54787) is used by default.\\n- For array types specifies the min/max number of elements as an integer.\\nIf not set, a range of [0, 5) is used by default. Range values are required to be >=0.\\n- For map types specifies the min/max number of key-value pairs as an integer.\\nIf not set, a range of [0, 5) is used by default.\\n- For struct/boolean/null types `range` is ignored."},"scale":{"type":"integer","format":"int64","description":"A scale factor to apply a multiplier to the generated value.\\n\\n- For integer/floating point types, the value is multiplied by the scale factor.\\n- For timestamp types, the generated value (milliseconds) is multiplied by the scale factor.\\n- For time types, the generated value (milliseconds) is multiplied by the scale factor.\\n- For date types, the generated value (days) is multiplied by the scale factor.\\n- For string/binary/array/map/struct/boolean/null types, the scale factor is ignored.\\n\\n- If `values` is specified, the scale factor is ignored.\\n- If `range` is specified and the range is required to be positive (struct, map, array etc.)\\nthe scale factor is required to be positive too.\\n\\nThe default scale factor is 1.","default":1},"strategy":{"default":"increment","type":"string","description":"Strategy used to generate values.","enum":["increment","uniform","zipf","word","words","sentence","sentences","paragraph","paragraphs","first_name","last_name","title","suffix","name","name_with_title","domain_suffix","email","username","password","field","position","seniority","job_title","ipv4","ipv6","ip","mac_address","user_agent","rfc_status_code","valid_status_code","company_suffix","company_name","buzzword","buzzword_middle","buzzword_tail","catch_phrase","bs_verb","bs_adj","bs_noun","bs","profession","industry","currency_code","currency_name","currency_symbol","credit_card_number","city_prefix","city_suffix","city_name","country_name","country_code","street_suffix","street_name","time_zone","state_name","state_abbr","secondary_address_type","secondary_address","zip_code","post_code","building_number","latitude","longitude","isbn","isbn13","isbn10","phone_number","cell_number","file_path","file_name","file_extension","dir_path"]},"value":{"default":null,"nullable":true,"$ref":"#/components/schemas/RngFieldSettings"},"values":{"type":"array","items":{"type":"object"},"description":"An optional set of values the generator will pick from.\\n\\nIf set, the generator will pick values from the specified set.\\nIf not set, the generator will produce values according to the specified range.\\nIf set to an empty set, the generator will produce NULL values.\\nIf set to a single value, the generator will produce only that value.\\n\\nNote that `range` is ignored if `values` is set.","default":null,"nullable":true}},"additionalProperties":false}},"limit":{"type":"integer","description":"Total number of new rows to generate.\\n\\nIf not set, the generator will produce new/unique records as long as the pipeline is running.\\nIf set to 0, the table will always remain empty.\\nIf set, the generator will produce new records until the specified limit is reached.\\n\\nNote that if the table has one or more primary keys that don\'t use the `increment` strategy to\\ngenerate the key there is a potential that an update is generated instead of an insert. In\\nthis case it\'s possible the total number of records is less than the specified limit.","default":null,"nullable":true,"minimum":0},"rate":{"type":"integer","format":"int32","description":"Non-zero number of rows to generate per second.\\n\\nIf not set, the generator will produce rows as fast as possible.","default":null,"nullable":true,"minimum":0},"worker_chunk_size":{"type":"integer","description":"When multiple workers are used, each worker will pick a consecutive \\"chunk\\" of\\nrecords to generate.\\n\\nBy default, if not specified, the generator will use the formula `min(rate, 10_000)`\\nto determine it. This works well in most situations. However, if you\'re\\nrunning tests with lateness and many workers you can e.g., reduce the\\nchunk size to make sure a smaller range of records is being ingested in parallel.\\n\\n# Example\\nAssume you generate a total of 125 records with 4 workers and a chunk size of 25.\\nIn this case, worker A will generate records 0..25, worker B will generate records 25..50,\\netc. A, B, C, and D will generate records in parallel. The first worker to finish its chunk\\nwill pick up the last chunk of records (100..125) to generate.","default":null,"nullable":true,"minimum":0}},"additionalProperties":false},"description":"The sequence of generations to perform.\\n\\nIf not set, the generator will produce a single sequence with default settings.\\nIf set, the generator will produce the specified sequences in sequential order.\\n\\nNote that if one of the sequences before the last one generates an unlimited number of rows\\nthe following sequences will not be executed.","default":[{"rate":null,"limit":null,"worker_chunk_size":null,"fields":{}}]},"seed":{"type":"integer","format":"int64","description":"Optional seed for the random generator.\\n\\nSetting this to a fixed value will make the generator produce the same sequence of records\\nevery time the pipeline is run.\\n\\n# Notes\\n- To ensure the set of generated input records is deterministic across multiple runs,\\napart from setting a seed, `workers` also needs to remain unchanged.\\n- The input will arrive in non-deterministic order if `workers > 1`.","default":null,"nullable":true,"minimum":0},"workers":{"type":"integer","description":"Number of workers to use for generating data.","default":1,"minimum":0}},"additionalProperties":false},"name":{"type":"string","enum":["datagen"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","description":"Configuration for generating Nexmark input data.\\n\\nThis connector must be used exactly three times in a pipeline if it is used\\nat all, once for each [`NexmarkTable`].","required":["table"],"properties":{"options":{"nullable":true,"type":"object","description":"Configuration for generating Nexmark input data.","properties":{"batch_size_per_thread":{"type":"integer","format":"int64","description":"Number of events to generate and submit together, per thread.\\n\\nEach thread generates this many records, which are then combined with\\nthe records generated by the other threads, to form combined input\\nbatches of size `threads \xd7 batch_size_per_thread`.","default":1000,"minimum":0},"events":{"type":"integer","format":"int64","description":"Number of events to generate.","default":100000000,"minimum":0},"max_step_size_per_thread":{"type":"integer","format":"int64","description":"Maximum number of events to submit in a single step, per thread.\\n\\nThis should really be per worker thread, not per generator thread, but\\nthe connector does not know how many worker threads there are.\\n\\nThis stands in for `max_batch_size` from the connector configuration\\nbecause it must be a constant across all three of the nexmark tables.","default":10000,"minimum":0},"threads":{"type":"integer","description":"Number of event generator threads.\\n\\nIt\'s reasonable to choose the same number of generator threads as worker\\nthreads.","default":4,"minimum":0}}},"table":{"type":"string","description":"Table in Nexmark.","enum":["bid","auction","person"]}}},"name":{"type":"string","enum":["nexmark"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","description":"Configuration for reading data via HTTP.\\n\\nHTTP input adapters cannot be usefully configured as part of pipeline\\nconfiguration.  Instead, instantiate them through the REST API as\\n`/pipelines/{pipeline_name}/ingress/{table_name}`.","required":["name"],"properties":{"name":{"type":"string","description":"Autogenerated name."}}},"name":{"type":"string","enum":["http_input"]}}},{"type":"object","required":["name"],"properties":{"name":{"type":"string","enum":["http_output"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","description":"Configuration for inserting data with ad-hoc queries\\n\\nAn ad-hoc input adapters cannot be usefully configured as part of pipeline\\nconfiguration.  Instead, use ad-hoc queries through the UI, the REST API, or\\nthe `fda` command-line tool.","required":["name"],"properties":{"name":{"type":"string","description":"Autogenerated name."}}},"name":{"type":"string","enum":["ad_hoc_input"]}}},{"type":"object","required":["name","config"],"properties":{"config":{"type":"object","required":["clock_resolution_usecs"],"properties":{"clock_resolution_usecs":{"type":"integer","format":"int64","minimum":0}}},"name":{"type":"string","enum":["clock_input"]}}}],"description":"Transport-specific endpoint configuration passed to\\n`crate::OutputTransport::new_endpoint`\\nand `crate::InputTransport::new_endpoint`.","discriminator":{"propertyName":"name"}}}}},"schema":{"type":"object","description":"A struct containing the tables (inputs) and views for a program.\\n\\nParse from the JSON data-type of the DDL generated by the SQL compiler.","required":["inputs","outputs"],"properties":{"inputs":{"type":"array","items":{"description":"A SQL table or view. It has a name and a list of fields.\\n\\nMatches the Calcite JSON format.","type":"object","required":["name","case_sensitive","fields"],"properties":{"case_sensitive":{"type":"boolean"},"name":{"type":"string"},"fields":{"type":"array","items":{"description":"A SQL field.\\n\\nMatches the SQL compiler JSON format.","type":"object","required":["name","case_sensitive","columntype","unused"],"properties":{"case_sensitive":{"type":"boolean"},"name":{"type":"string"},"columntype":{"$ref":"#/components/schemas/ColumnType"},"default":{"type":"string","nullable":true},"lateness":{"type":"string","nullable":true},"unused":{"type":"boolean"},"watermark":{"type":"string","nullable":true}}}},"materialized":{"type":"boolean"},"properties":{"type":"object","additionalProperties":{"type":"object","required":["value","key_position","value_position"],"properties":{"key_position":{"type":"object","required":["start_line_number","start_column","end_line_number","end_column"],"properties":{"end_column":{"type":"integer","minimum":0},"end_line_number":{"type":"integer","minimum":0},"start_column":{"type":"integer","minimum":0},"start_line_number":{"type":"integer","minimum":0}}},"value":{"type":"string"},"value_position":{"type":"object","required":["start_line_number","start_column","end_line_number","end_column"],"properties":{"end_column":{"type":"integer","minimum":0},"end_line_number":{"type":"integer","minimum":0},"start_column":{"type":"integer","minimum":0},"start_line_number":{"type":"integer","minimum":0}}}}}}}}},"outputs":{"type":"array","items":{"description":"A SQL table or view. It has a name and a list of fields.\\n\\nMatches the Calcite JSON format.","type":"object","required":["name","case_sensitive","fields"],"properties":{"case_sensitive":{"type":"boolean"},"name":{"type":"string"},"fields":{"type":"array","items":{"description":"A SQL field.\\n\\nMatches the SQL compiler JSON format.","type":"object","required":["name","case_sensitive","columntype","unused"],"properties":{"case_sensitive":{"type":"boolean"},"name":{"type":"string"},"columntype":{"$ref":"#/components/schemas/ColumnType"},"default":{"type":"string","nullable":true},"lateness":{"type":"string","nullable":true},"unused":{"type":"boolean"},"watermark":{"type":"string","nullable":true}}}},"materialized":{"type":"boolean"},"properties":{"type":"object","additionalProperties":{"type":"object","required":["value","key_position","value_position"],"properties":{"key_position":{"type":"object","required":["start_line_number","start_column","end_line_number","end_column"],"properties":{"end_column":{"type":"integer","minimum":0},"end_line_number":{"type":"integer","minimum":0},"start_column":{"type":"integer","minimum":0},"start_line_number":{"type":"integer","minimum":0}}},"value":{"type":"string"},"value_position":{"type":"object","required":["start_line_number","start_column","end_line_number","end_column"],"properties":{"end_column":{"type":"integer","minimum":0},"end_line_number":{"type":"integer","minimum":0},"start_column":{"type":"integer","minimum":0},"start_line_number":{"type":"integer","minimum":0}}}}}}}}}}},"udf_stubs":{"type":"string","description":"Generated user defined function (UDF) stubs Rust code: stubs.rs"}}},"example":{"id":"67e55044-10b1-426f-9247-bb680e5fe0c8","name":"example1","description":"Description of the pipeline example1","created_at":"1970-01-01T00:00:00Z","version":4,"platform_version":"v0","runtime_config":{"workers":16,"storage":{"backend":{"name":"default"},"min_storage_bytes":null,"min_step_storage_bytes":null,"compression":"default","cache_mib":null},"fault_tolerance":{"model":"none","checkpoint_interval_secs":60},"cpu_profiler":true,"tracing":false,"tracing_endpoint_jaeger":"","min_batch_size_records":0,"max_buffering_delay_usecs":0,"resources":{"cpu_cores_min":null,"cpu_cores_max":null,"memory_mb_min":null,"memory_mb_max":null,"storage_mb_max":null,"storage_class":null},"clock_resolution_usecs":1000000,"pin_cpus":[],"provisioning_timeout_secs":null,"max_parallel_connector_init":null,"init_containers":null,"checkpoint_during_suspend":true,"http_workers":null,"io_workers":null,"dev_tweaks":{},"logging":null},"program_code":"CREATE TABLE table1 ( col1 INT );","udf_rust":"","udf_toml":"","program_config":{"profile":"optimized","cache":true,"runtime_version":null},"program_version":2,"program_status":"Pending","program_status_since":"1970-01-01T00:00:00Z","program_error":{"sql_compilation":null,"rust_compilation":null,"system_error":null},"program_info":null,"deployment_status":"Stopped","deployment_status_since":"1970-01-01T00:00:00Z","deployment_desired_status":"Stopped","deployment_error":null,"refresh_version":4,"storage_status":"Cleared"}}}},"404":{"description":"Pipeline with that name does not exist","content":{"application/json":{"schema":{"type":"object","description":"Information returned by REST API endpoints on error.","required":["message","error_code","details"],"properties":{"details":{"type":"object","description":"Detailed error metadata.\\nThe contents of this field is determined by `error_code`."},"error_code":{"type":"string","description":"Error code is a string that specifies this error type.","example":"CodeSpecifyingErrorType"},"message":{"type":"string","description":"Human-readable error message.","example":"Explanation of the error that occurred."}}},"example":{"message":"Unknown pipeline name \'non-existent-pipeline\'","error_code":"UnknownPipelineName","details":{"pipeline_name":"non-existent-pipeline"}}}}},"500":{"description":"","content":{"application/json":{"schema":{"type":"object","description":"Information returned by REST API endpoints on error.","required":["message","error_code","details"],"properties":{"details":{"type":"object","description":"Detailed error metadata.\\nThe contents of this field is determined by `error_code`."},"error_code":{"type":"string","description":"Error code is a string that specifies this error type.","example":"CodeSpecifyingErrorType"},"message":{"type":"string","description":"Human-readable error message.","example":"Explanation of the error that occurred."}}}}}}},"security":[{"JSON web token (JWT) or API key":[]}],"description":"Retrieve the program info of a pipeline.","method":"get","path":"/v0/pipelines/{pipeline_name}/program_info","securitySchemes":{"JSON web token (JWT) or API key":{"type":"http","scheme":"bearer","bearerFormat":"JWT","description":"Use a JWT token obtained via an OAuth2/OIDC\\n                               login workflow or an API key obtained via\\n                               the `/v0/api-keys` endpoint."}},"info":{"title":"Feldera API","description":"\\nWith Feldera, users create data pipelines out of SQL programs.\\nA SQL program comprises tables and views, and includes as well the definition of\\ninput and output connectors for each respectively. A connector defines a data\\nsource or data sink to feed input data into tables or receive output data\\ncomputed by the views respectively.\\n\\n## Pipeline\\n\\nThe API is centered around the **pipeline**, which most importantly consists\\nout of the SQL program, but also has accompanying metadata and configuration parameters\\n(e.g., compilation profile, number of workers, etc.).\\n\\n* A pipeline is identified and referred to by its user-provided unique name.\\n* The pipeline program is asynchronously compiled when the pipeline is first created or\\n  when its program is subsequently updated.\\n* Pipeline deployment is only possible once the program is successfully compiled.\\n* A pipeline cannot be updated while it is deployed.\\n\\n## Concurrency\\n\\nEach pipeline has a version, which is incremented each time its core fields are updated.\\nThe version is monotonically increasing. There is additionally a program version which covers\\nonly the program-related core fields, and is used by the compiler to discern when to recompile.","contact":{"name":"Feldera Team","email":"dev@feldera.com"},"license":{"name":"MIT OR Apache-2.0"},"version":"0.117.0"},"postman":{"name":"Retrieve the program info of a pipeline.","description":{"type":"text/plain"},"url":{"path":["v0","pipelines",":pipeline_name","program_info"],"host":["{{baseUrl}}"],"query":[],"variable":[{"disabled":false,"description":{"content":"(Required) Unique pipeline name","type":"text/plain"},"type":"any","value":"<string>","key":"pipeline_name"}]},"header":[{"key":"Accept","value":"application/json"}],"method":"GET","auth":{"type":"bearer","bearer":[{"type":"any","value":"{{bearerToken}}","key":"token"}]}}},"source":"@site/../openapi.json","sourceDirName":".","permalink":"/api/retrieve-the-program-info-of-a-pipeline","previous":{"title":"Pause the pipeline asynchronously by updating the desired state.","permalink":"/api/pause-the-pipeline-asynchronously-by-updating-the-desired-state"},"next":{"title":"Execute an ad-hoc SQL query in a running or paused pipeline.","permalink":"/api/execute-an-ad-hoc-sql-query-in-a-running-or-paused-pipeline"}}');var o=n(74848),r=n(28453);const s={},a="Retrieve the program info of a pipeline.",l=[];function p(e){const t={code:"code",h1:"h1",header:"header",p:"p",...(0,r.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(t.header,{children:(0,o.jsx)(t.h1,{id:"retrieve-the-program-info-of-a-pipeline",children:"Retrieve the program info of a pipeline."})}),"\n",(0,o.jsx)(t.p,{children:"Retrieve the program info of a pipeline."}),"\n",(0,o.jsxs)("table",{style:{display:"table",width:"100%"},children:[(0,o.jsx)("thead",{children:(0,o.jsx)("tr",{children:(0,o.jsx)("th",{style:{textAlign:"left"},children:"Path Parameters"})})}),(0,o.jsx)("tbody",{children:(0,o.jsx)("tr",{children:(0,o.jsxs)("td",{children:[(0,o.jsx)("code",{children:"pipeline_name"}),(0,o.jsx)("span",{style:{opacity:"0.6"},children:" string"}),(0,o.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,o.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-required)"},children:" REQUIRED"}),(0,o.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,o.jsx)(t.p,{children:"Unique pipeline name"})})]})})})]}),"\n",(0,o.jsxs)("table",{style:{display:"table",width:"100%"},children:[(0,o.jsx)("thead",{children:(0,o.jsx)("tr",{children:(0,o.jsx)("th",{style:{textAlign:"left"},children:"Responses"})})}),(0,o.jsxs)("tbody",{children:[(0,o.jsx)("tr",{children:(0,o.jsxs)("td",{children:[(0,o.jsxs)("div",{style:{display:"flex"},children:[(0,o.jsx)("div",{style:{marginRight:"var(--ifm-table-cell-padding)"},children:(0,o.jsx)("code",{children:"200"})}),(0,o.jsx)("div",{children:(0,o.jsx)(t.p,{children:"Pipeline retrieved successfully"})})]}),(0,o.jsx)("div",{children:(0,o.jsxs)("table",{style:{display:"table",width:"100%",marginTop:"var(--ifm-table-cell-padding)",marginBottom:"0px"},children:[(0,o.jsx)("thead",{children:(0,o.jsx)("tr",{children:(0,o.jsxs)("th",{style:{textAlign:"left"},children:[(0,o.jsx)("span",{children:"Schema "}),(0,o.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,o.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"}),(0,o.jsx)("div",{})]})})}),(0,o.jsxs)("tbody",{children:[(0,o.jsx)("tr",{children:(0,o.jsxs)("td",{children:[(0,o.jsx)("code",{children:"dataflow"}),(0,o.jsx)("span",{style:{opacity:"0.6"}}),(0,o.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,o.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"}),(0,o.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,o.jsx)(t.p,{children:"Dataflow graph of the program."})})]})}),(0,o.jsx)("tr",{children:(0,o.jsxs)("td",{children:[(0,o.jsx)("code",{children:"input_connectors"}),(0,o.jsx)("span",{style:{opacity:"0.6"},children:" object"}),(0,o.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,o.jsx)(t.p,{children:"Input connectors derived from the schema."})})]})}),(0,o.jsx)("tr",{children:(0,o.jsxs)("td",{children:[(0,o.jsx)("code",{children:"main_rust"}),(0,o.jsx)("span",{style:{opacity:"0.6"},children:" string"}),(0,o.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,o.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"}),(0,o.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,o.jsx)(t.p,{children:"Generated main program Rust code: main.rs"})})]})}),(0,o.jsx)("tr",{children:(0,o.jsxs)("td",{children:[(0,o.jsx)("code",{children:"output_connectors"}),(0,o.jsx)("span",{style:{opacity:"0.6"},children:" object"}),(0,o.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,o.jsx)(t.p,{children:"Output connectors derived from the schema."})})]})}),(0,o.jsx)("tr",{children:(0,o.jsxs)("td",{children:[(0,o.jsx)("code",{children:"schema"}),(0,o.jsx)("span",{style:{opacity:"0.6"},children:" object"}),(0,o.jsxs)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:[(0,o.jsx)(t.p,{children:"A struct containing the tables (inputs) and views for a program."}),(0,o.jsx)(t.p,{children:"Parse from the JSON data-type of the DDL generated by the SQL compiler."})]}),(0,o.jsx)("table",{style:{display:"table",width:"100%",marginTop:"var(--ifm-table-cell-padding)",marginBottom:"0px"},children:(0,o.jsxs)("tbody",{children:[(0,o.jsx)("tr",{children:(0,o.jsxs)("td",{children:[(0,o.jsx)("code",{children:"inputs"}),(0,o.jsx)("span",{style:{opacity:"0.6"},children:" object[]"}),(0,o.jsx)("table",{style:{display:"table",width:"100%",marginTop:"var(--ifm-table-cell-padding)",marginBottom:"0px"},children:(0,o.jsxs)("tbody",{children:[(0,o.jsx)("tr",{children:(0,o.jsxs)("td",{children:[(0,o.jsx)("code",{children:"case_sensitive"}),(0,o.jsx)("span",{style:{opacity:"0.6"},children:" boolean"})]})}),(0,o.jsx)("tr",{children:(0,o.jsxs)("td",{children:[(0,o.jsx)("code",{children:"name"}),(0,o.jsx)("span",{style:{opacity:"0.6"},children:" string"})]})}),(0,o.jsx)("tr",{children:(0,o.jsxs)("td",{children:[(0,o.jsx)("code",{children:"fields"}),(0,o.jsx)("span",{style:{opacity:"0.6"},children:" object[]"}),(0,o.jsx)("table",{style:{display:"table",width:"100%",marginTop:"var(--ifm-table-cell-padding)",marginBottom:"0px"},children:(0,o.jsxs)("tbody",{children:[(0,o.jsx)("tr",{children:(0,o.jsxs)("td",{children:[(0,o.jsx)("code",{children:"case_sensitive"}),(0,o.jsx)("span",{style:{opacity:"0.6"},children:" boolean"})]})}),(0,o.jsx)("tr",{children:(0,o.jsxs)("td",{children:[(0,o.jsx)("code",{children:"name"}),(0,o.jsx)("span",{style:{opacity:"0.6"},children:" string"})]})}),(0,o.jsx)("tr",{children:(0,o.jsxs)("td",{children:[(0,o.jsx)("code",{children:"columntype"}),(0,o.jsx)("span",{style:{opacity:"0.6"},children:"  (circular)"})]})}),(0,o.jsx)("tr",{children:(0,o.jsxs)("td",{children:[(0,o.jsx)("code",{children:"default"}),(0,o.jsx)("span",{style:{opacity:"0.6"},children:" string"}),(0,o.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,o.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"})]})}),(0,o.jsx)("tr",{children:(0,o.jsxs)("td",{children:[(0,o.jsx)("code",{children:"lateness"}),(0,o.jsx)("span",{style:{opacity:"0.6"},children:" string"}),(0,o.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,o.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"})]})}),(0,o.jsx)("tr",{children:(0,o.jsxs)("td",{children:[(0,o.jsx)("code",{children:"unused"}),(0,o.jsx)("span",{style:{opacity:"0.6"},children:" boolean"})]})}),(0,o.jsx)("tr",{children:(0,o.jsxs)("td",{children:[(0,o.jsx)("code",{children:"watermark"}),(0,o.jsx)("span",{style:{opacity:"0.6"},children:" string"}),(0,o.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,o.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"})]})})]})})]})}),(0,o.jsx)("tr",{children:(0,o.jsxs)("td",{children:[(0,o.jsx)("code",{children:"materialized"}),(0,o.jsx)("span",{style:{opacity:"0.6"},children:" boolean"}),(0,o.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,o.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"})]})}),(0,o.jsx)("tr",{children:(0,o.jsxs)("td",{children:[(0,o.jsx)("code",{children:"properties"}),(0,o.jsx)("span",{style:{opacity:"0.6"},children:" object"}),(0,o.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,o.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"})]})})]})})]})}),(0,o.jsx)("tr",{children:(0,o.jsxs)("td",{children:[(0,o.jsx)("code",{children:"outputs"}),(0,o.jsx)("span",{style:{opacity:"0.6"},children:" object[]"}),(0,o.jsx)("table",{style:{display:"table",width:"100%",marginTop:"var(--ifm-table-cell-padding)",marginBottom:"0px"},children:(0,o.jsxs)("tbody",{children:[(0,o.jsx)("tr",{children:(0,o.jsxs)("td",{children:[(0,o.jsx)("code",{children:"case_sensitive"}),(0,o.jsx)("span",{style:{opacity:"0.6"},children:" boolean"})]})}),(0,o.jsx)("tr",{children:(0,o.jsxs)("td",{children:[(0,o.jsx)("code",{children:"name"}),(0,o.jsx)("span",{style:{opacity:"0.6"},children:" string"})]})}),(0,o.jsx)("tr",{children:(0,o.jsxs)("td",{children:[(0,o.jsx)("code",{children:"fields"}),(0,o.jsx)("span",{style:{opacity:"0.6"},children:" object[]"}),(0,o.jsx)("table",{style:{display:"table",width:"100%",marginTop:"var(--ifm-table-cell-padding)",marginBottom:"0px"},children:(0,o.jsxs)("tbody",{children:[(0,o.jsx)("tr",{children:(0,o.jsxs)("td",{children:[(0,o.jsx)("code",{children:"case_sensitive"}),(0,o.jsx)("span",{style:{opacity:"0.6"},children:" boolean"})]})}),(0,o.jsx)("tr",{children:(0,o.jsxs)("td",{children:[(0,o.jsx)("code",{children:"name"}),(0,o.jsx)("span",{style:{opacity:"0.6"},children:" string"})]})}),(0,o.jsx)("tr",{children:(0,o.jsxs)("td",{children:[(0,o.jsx)("code",{children:"columntype"}),(0,o.jsx)("span",{style:{opacity:"0.6"},children:"  (circular)"})]})}),(0,o.jsx)("tr",{children:(0,o.jsxs)("td",{children:[(0,o.jsx)("code",{children:"default"}),(0,o.jsx)("span",{style:{opacity:"0.6"},children:" string"}),(0,o.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,o.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"})]})}),(0,o.jsx)("tr",{children:(0,o.jsxs)("td",{children:[(0,o.jsx)("code",{children:"lateness"}),(0,o.jsx)("span",{style:{opacity:"0.6"},children:" string"}),(0,o.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,o.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"})]})}),(0,o.jsx)("tr",{children:(0,o.jsxs)("td",{children:[(0,o.jsx)("code",{children:"unused"}),(0,o.jsx)("span",{style:{opacity:"0.6"},children:" boolean"})]})}),(0,o.jsx)("tr",{children:(0,o.jsxs)("td",{children:[(0,o.jsx)("code",{children:"watermark"}),(0,o.jsx)("span",{style:{opacity:"0.6"},children:" string"}),(0,o.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,o.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"})]})})]})})]})}),(0,o.jsx)("tr",{children:(0,o.jsxs)("td",{children:[(0,o.jsx)("code",{children:"materialized"}),(0,o.jsx)("span",{style:{opacity:"0.6"},children:" boolean"}),(0,o.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,o.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"})]})}),(0,o.jsx)("tr",{children:(0,o.jsxs)("td",{children:[(0,o.jsx)("code",{children:"properties"}),(0,o.jsx)("span",{style:{opacity:"0.6"},children:" object"}),(0,o.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,o.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"})]})})]})})]})})]})})]})}),(0,o.jsx)("tr",{children:(0,o.jsxs)("td",{children:[(0,o.jsx)("code",{children:"udf_stubs"}),(0,o.jsx)("span",{style:{opacity:"0.6"},children:" string"}),(0,o.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,o.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"}),(0,o.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,o.jsx)(t.p,{children:"Generated user defined function (UDF) stubs Rust code: stubs.rs"})})]})})]})]})})]})}),(0,o.jsx)("tr",{children:(0,o.jsxs)("td",{children:[(0,o.jsxs)("div",{style:{display:"flex"},children:[(0,o.jsx)("div",{style:{marginRight:"var(--ifm-table-cell-padding)"},children:(0,o.jsx)("code",{children:"404"})}),(0,o.jsx)("div",{children:(0,o.jsx)(t.p,{children:"Pipeline with that name does not exist"})})]}),(0,o.jsx)("div",{children:(0,o.jsxs)("table",{style:{display:"table",width:"100%",marginTop:"var(--ifm-table-cell-padding)",marginBottom:"0px"},children:[(0,o.jsx)("thead",{children:(0,o.jsx)("tr",{children:(0,o.jsxs)("th",{style:{textAlign:"left"},children:[(0,o.jsx)("span",{children:"Schema "}),(0,o.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,o.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"}),(0,o.jsx)("div",{})]})})}),(0,o.jsxs)("tbody",{children:[(0,o.jsx)("tr",{children:(0,o.jsxs)("td",{children:[(0,o.jsx)("code",{children:"details"}),(0,o.jsx)("span",{style:{opacity:"0.6"},children:" object"}),(0,o.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,o.jsxs)(t.p,{children:["Detailed error metadata.\nThe contents of this field is determined by ",(0,o.jsx)(t.code,{children:"error_code"}),"."]})})]})}),(0,o.jsx)("tr",{children:(0,o.jsxs)("td",{children:[(0,o.jsx)("code",{children:"error_code"}),(0,o.jsx)("span",{style:{opacity:"0.6"},children:" string"}),(0,o.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,o.jsx)(t.p,{children:"Error code is a string that specifies this error type."})})]})}),(0,o.jsx)("tr",{children:(0,o.jsxs)("td",{children:[(0,o.jsx)("code",{children:"message"}),(0,o.jsx)("span",{style:{opacity:"0.6"},children:" string"}),(0,o.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,o.jsx)(t.p,{children:"Human-readable error message."})})]})})]})]})})]})}),(0,o.jsx)("tr",{children:(0,o.jsxs)("td",{children:[(0,o.jsxs)("div",{style:{display:"flex"},children:[(0,o.jsx)("div",{style:{marginRight:"var(--ifm-table-cell-padding)"},children:(0,o.jsx)("code",{children:"500"})}),(0,o.jsx)("div",{})]}),(0,o.jsx)("div",{children:(0,o.jsxs)("table",{style:{display:"table",width:"100%",marginTop:"var(--ifm-table-cell-padding)",marginBottom:"0px"},children:[(0,o.jsx)("thead",{children:(0,o.jsx)("tr",{children:(0,o.jsxs)("th",{style:{textAlign:"left"},children:[(0,o.jsx)("span",{children:"Schema "}),(0,o.jsx)("span",{style:{opacity:"0.6"},children:" \u2014 "}),(0,o.jsx)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-optional)"},children:" OPTIONAL"}),(0,o.jsx)("div",{})]})})}),(0,o.jsxs)("tbody",{children:[(0,o.jsx)("tr",{children:(0,o.jsxs)("td",{children:[(0,o.jsx)("code",{children:"details"}),(0,o.jsx)("span",{style:{opacity:"0.6"},children:" object"}),(0,o.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,o.jsxs)(t.p,{children:["Detailed error metadata.\nThe contents of this field is determined by ",(0,o.jsx)(t.code,{children:"error_code"}),"."]})})]})}),(0,o.jsx)("tr",{children:(0,o.jsxs)("td",{children:[(0,o.jsx)("code",{children:"error_code"}),(0,o.jsx)("span",{style:{opacity:"0.6"},children:" string"}),(0,o.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,o.jsx)(t.p,{children:"Error code is a string that specifies this error type."})})]})}),(0,o.jsx)("tr",{children:(0,o.jsxs)("td",{children:[(0,o.jsx)("code",{children:"message"}),(0,o.jsx)("span",{style:{opacity:"0.6"},children:" string"}),(0,o.jsx)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"},children:(0,o.jsx)(t.p,{children:"Human-readable error message."})})]})})]})]})})]})})]})]})]})}function c(e={}){const{wrapper:t}={...(0,r.R)(),...e.components};return t?(0,o.jsx)(t,{...e,children:(0,o.jsx)(p,{...e})}):p(e)}}}]);