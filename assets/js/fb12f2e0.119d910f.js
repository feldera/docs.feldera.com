"use strict";(self.webpackChunkfeldera_docs=self.webpackChunkfeldera_docs||[]).push([[5121],{28453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>o});var s=t(96540);const i={},a=s.createContext(i);function r(e){const n=s.useContext(a);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),s.createElement(a.Provider,{value:n},e.children)}},31313:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"connectors/sources/kafka","title":"Kafka input connector","description":"Feldera can consume a stream of changes to a SQL table from Kafka with","source":"@site/docs/connectors/sources/kafka.md","sourceDirName":"connectors/sources","slug":"/connectors/sources/kafka","permalink":"/connectors/sources/kafka","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"docsSidebar","previous":{"title":"Apache Iceberg","permalink":"/connectors/sources/iceberg"},"next":{"title":"Google Pub/Sub","permalink":"/connectors/sources/pubsub"}}');var i=t(74848),a=t(28453);const r={},o="Kafka input connector",c={},d=[{value:"Kafka Input Connector Configuration",id:"kafka-input-connector-configuration",level:2},{value:"Example usage",id:"example-usage",level:2},{value:"Starting from a specific offset",id:"starting-from-a-specific-offset",level:3},{value:"How to write connector config",id:"how-to-write-connector-config",level:3},{value:"No authentication and plaintext (no encryption)",id:"no-authentication-and-plaintext-no-encryption",level:4},{value:"SASL authentication and plaintext (no encryption)",id:"sasl-authentication-and-plaintext-no-encryption",level:4},{value:"No authentication and with encryption (SSL)",id:"no-authentication-and-with-encryption-ssl",level:4},{value:"SASL authentication and with encryption (SSL)",id:"sasl-authentication-and-with-encryption-ssl",level:4},{value:"SSL with PEM keys",id:"ssl-with-pem-keys",level:4},{value:"Multiple certificates",id:"multiple-certificates",level:5},{value:"Using Kafka as a Debezium transport",id:"using-kafka-as-a-debezium-transport",level:3},{value:"Connecting to AWS MSK with IAM SASL",id:"connecting-to-aws-msk-with-iam-sasl",level:3},{value:"<a></a>Accessing Kafka metadata",id:"accessing-kafka-metadata",level:2},{value:"Converting Kafka header values to strings",id:"converting-kafka-header-values-to-strings",level:3},{value:"Tolerating missing data on resume",id:"tolerating-missing-data-on-resume",level:2},{value:"Partition changes",id:"partition-changes",level:2},{value:"Additional resources",id:"additional-resources",level:2}];function l(e){const n={a:"a",admonition:"admonition",blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",h5:"h5",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,a.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"kafka-input-connector",children:"Kafka input connector"})}),"\n",(0,i.jsxs)(n.p,{children:["Feldera can consume a stream of changes to a SQL table from Kafka with\nthe ",(0,i.jsx)(n.code,{children:"kafka_input"})," connector."]}),"\n",(0,i.jsxs)(n.p,{children:["The Kafka input connector supports ",(0,i.jsx)(n.a,{href:"/pipelines/fault-tolerance",children:"fault\ntolerance"}),"."]}),"\n",(0,i.jsx)(n.h2,{id:"kafka-input-connector-configuration",children:"Kafka Input Connector Configuration"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Property"}),(0,i.jsx)(n.th,{children:"Type"}),(0,i.jsx)(n.th,{children:"Default"}),(0,i.jsx)(n.th,{children:"Description"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsxs)(n.td,{children:[(0,i.jsx)(n.code,{children:"topic"})," (required)"]}),(0,i.jsx)(n.td,{children:"string"}),(0,i.jsx)(n.td,{}),(0,i.jsx)(n.td,{children:"The Kafka topic to subscribe to."})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsxs)(n.td,{children:[(0,i.jsx)(n.code,{children:"bootstrap.servers"})," (required)"]}),(0,i.jsx)(n.td,{children:"string"}),(0,i.jsx)(n.td,{}),(0,i.jsx)(n.td,{children:"A comma separated list of Kafka brokers to connect to."})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"start_from"})}),(0,i.jsx)(n.td,{children:"variant"}),(0,i.jsx)(n.td,{children:"latest"}),(0,i.jsxs)(n.td,{children:["The starting point for reading from the topic, one of ",(0,i.jsx)(n.code,{children:'"earliest"'}),", ",(0,i.jsx)(n.code,{children:'"latest"'}),", ",(0,i.jsx)(n.code,{children:'{"offsets": [1, 2, 3, ...]}'}),", where ",(0,i.jsx)(n.code,{children:"[1, 2, 3, ...]"})," are particular offsets within each partition in the topic, or ",(0,i.jsx)(n.code,{children:'{"timestamp": <timestamp>}'})," where ",(0,i.jsx)(n.code,{children:"<timestamp>"})," is a Kafka timestamp as an integer number of milliseconds since the epoch."]})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"partitions"})}),(0,i.jsx)(n.td,{children:"integer list"}),(0,i.jsx)(n.td,{}),(0,i.jsxs)(n.td,{children:[(0,i.jsx)("p",{children:" The list of Kafka partitions to read from. "})," ",(0,i.jsx)("p",{children:" Only the specified partitions will be consumed. If this field is not set, the connector will consume from all available partitions. "}),(0,i.jsxs)("p",{children:[" If ",(0,i.jsx)(n.code,{children:"start_from"})," is set to ",(0,i.jsx)(n.code,{children:"offsets"})," and this field is provided, the number of partitions must exactly match the number of offsets, and the order of partitions must correspond to the order of offsets. "]}),(0,i.jsx)("p",{children:" If offsets are provided for all partitions, this field can be omitted. "})]})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"log_level"})}),(0,i.jsx)(n.td,{children:"string"}),(0,i.jsx)(n.td,{}),(0,i.jsx)(n.td,{children:"The log level for the Kafka client."})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"group_join_timeout_secs"})}),(0,i.jsx)(n.td,{children:"seconds"}),(0,i.jsx)(n.td,{children:"10"}),(0,i.jsx)(n.td,{children:"Maximum timeout (in seconds) for the endpoint to join the Kafka consumer group during initialization."})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"poller_threads"})}),(0,i.jsx)(n.td,{children:"positive integer"}),(0,i.jsx)(n.td,{children:"3"}),(0,i.jsx)(n.td,{children:"Number of threads used to poll Kafka messages. Setting it to multiple threads can improve performance with small messages. Default is 3."})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"resume_earliest_if_data_expires"})}),(0,i.jsx)(n.td,{children:"boolean"}),(0,i.jsx)(n.td,{children:"false"}),(0,i.jsxs)(n.td,{children:["See ",(0,i.jsx)(n.a,{href:"#tolerating-missing-data-on-resume",children:"Tolerating missing data on resume"}),"."]})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"include_headers"})}),(0,i.jsx)(n.td,{children:"boolean"}),(0,i.jsx)(n.td,{children:"false"}),(0,i.jsxs)(n.td,{children:["Whether to include Kafka headers in connector metadata (see ",(0,i.jsx)(n.a,{href:"#metadata",children:"Accessing Kafka metadata"}),")."]})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"include_topic"})}),(0,i.jsx)(n.td,{children:"boolean"}),(0,i.jsx)(n.td,{children:"false"}),(0,i.jsxs)(n.td,{children:["Whether to include Kafka topic name in connector metadata (see ",(0,i.jsx)(n.a,{href:"#metadata",children:"Accessing Kafka metadata"}),")."]})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"include_partition"})}),(0,i.jsx)(n.td,{children:"boolean"}),(0,i.jsx)(n.td,{children:"false"}),(0,i.jsxs)(n.td,{children:["Whether to include Kafka partition in connector metadata (see ",(0,i.jsx)(n.a,{href:"#metadata",children:"Accessing Kafka metadata"}),")."]})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"include_offset"})}),(0,i.jsx)(n.td,{children:"boolean"}),(0,i.jsx)(n.td,{children:"false"}),(0,i.jsxs)(n.td,{children:["Whether to include Kafka offset name in connector metadata (see ",(0,i.jsx)(n.a,{href:"#metadata",children:"Accessing Kafka metadata"}),")."]})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"include_timestamp"})}),(0,i.jsx)(n.td,{children:"boolean"}),(0,i.jsx)(n.td,{children:"false"}),(0,i.jsxs)(n.td,{children:["Whether to include Kafka timestamp in connector metadata (see ",(0,i.jsx)(n.a,{href:"#metadata",children:"Accessing Kafka metadata"}),")."]})]})]})]}),"\n",(0,i.jsxs)(n.p,{children:["The connector passes additional options directly to ",(0,i.jsx)(n.a,{href:"https://github.com/confluentinc/librdkafka/blob/master/CONFIGURATION.md",children:(0,i.jsx)(n.strong,{children:"librdkafka"})}),".  Some of the relevant options:"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"group.id"})," is ignored.  The connector never uses consumer groups, so ",(0,i.jsx)(n.code,{children:"enable.auto.commit"}),", ",(0,i.jsx)(n.code,{children:"enable.auto.offset.store"}),", and other options related to consumer groups are also ignored."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"auto.offset.reset"})," is ignored.  Use ",(0,i.jsx)(n.code,{children:"start_from"})," (described in the table above) instead."]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"example-usage",children:"Example usage"}),"\n",(0,i.jsxs)(n.p,{children:["We will create a Kafka connector named ",(0,i.jsx)(n.code,{children:"book-fair-sales"}),".\nKafka broker is located at ",(0,i.jsx)(n.code,{children:"example.com:9092"})," and the topic is ",(0,i.jsx)(n.code,{children:"book-fair-sales"}),"."]}),"\n",(0,i.jsx)(n.p,{children:"Format in this example is newline-delimited JSON (NDJSON).\nFor example, there can be two messages containing three rows:"}),"\n",(0,i.jsx)(n.p,{children:"Message 1:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-text",children:'{"insert": {"sid": 123, pid": 2, "sold_at": "2024-01-01 12:00:04", "price": 5.0}}\n{"insert": {"sid": 124, pid": 12, "sold_at": "2024-01-01 12:00:08", "price": 20.5}}\n'})}),"\n",(0,i.jsx)(n.p,{children:"Message 2:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'{"insert": {"sid": 125, pid": 8, "sold_at": "2024-01-01 12:01:02", "price": 1.10}}\n'})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:'CREATE TABLE INPUT (\n   ... -- columns omitted\n) WITH (\n  \'connectors\' = \'[\n    {\n      "transport": {\n          "name": "kafka_input",\n          "config": {\n              "topic": "book-fair-sales",\n              "start_from": "earliest",\n              "bootstrap.servers": "example.com:9092"\n          }\n      },\n      "format": {\n          "name": "json",\n          "config": {\n              "update_format": "insert_delete",\n              "array": false\n          }\n      }\n  }]\'\n)\n'})}),"\n",(0,i.jsx)(n.h3,{id:"starting-from-a-specific-offset",children:"Starting from a specific offset"}),"\n",(0,i.jsx)(n.p,{children:"Feldera supports starting a Kafka connector from a specific offset in a specific\npartition."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:'CREATE TABLE INPUT (\n   ... -- columns omitted\n) WITH (\n  \'connectors\' = \'[\n    {\n      "transport": {\n          "name": "kafka_input",\n          "config": {\n              "topic": "book-fair-sales",\n              "start_from": {"offsets": [42]},\n              "bootstrap.servers": "example.com:9092"\n          }\n      },\n      "format": {\n          "name": "json",\n          "config": {\n              "update_format": "insert_delete",\n              "array": false\n          }\n      }\n  }]\'\n)\n'})}),"\n",(0,i.jsx)(n.h3,{id:"how-to-write-connector-config",children:"How to write connector config"}),"\n",(0,i.jsxs)(n.p,{children:["Below are a couple of examples on how to connect to a Kafka broker\nby specifying\n",(0,i.jsx)(n.a,{href:"https://github.com/confluentinc/librdkafka/blob/master/CONFIGURATION.md",children:"the options passed to librdkafka"}),"\nunderneath.\nNote that it is strongly recommended outside of test/dev\nenvironments to have encrypted and authenticated communication.\nIn addition, in order for connectors to not have secrets in plaintext in their configuration,\nit is recommended to use ",(0,i.jsx)(n.a,{href:"/connectors/secret-references",children:"secret references"}),"."]}),"\n",(0,i.jsx)(n.h4,{id:"no-authentication-and-plaintext-no-encryption",children:"No authentication and plaintext (no encryption)"}),"\n",(0,i.jsxs)(n.p,{children:["Default value for ",(0,i.jsx)(n.code,{children:"security.protocol"})," is ",(0,i.jsx)(n.code,{children:"PLAINTEXT"}),", as such\nit does not need to be explicitly specified."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-json",children:'"config": {\n    "topic": "book-fair-sales",\n    "start_from": "earliest",\n    "bootstrap.servers": "example.com:9092"\n}\n'})}),"\n",(0,i.jsx)(n.h4,{id:"sasl-authentication-and-plaintext-no-encryption",children:"SASL authentication and plaintext (no encryption)"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-json",children:'"config": {\n    "topic": "book-fair-sales",\n    "start_from": "earliest",\n    "bootstrap.servers": "example.com:9092",\n    "security.protocol": "SASL_PLAINTEXT",\n    "sasl.mechanism": "SCRAM-SHA-256",\n    "sasl.username": "<USER>",\n    "sasl.password": "<PASSWORD>"\n}\n'})}),"\n",(0,i.jsx)(n.h4,{id:"no-authentication-and-with-encryption-ssl",children:"No authentication and with encryption (SSL)"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-json",children:'"config": {\n    "topic": "book-fair-sales",\n    "start_from": "earliest",\n    "bootstrap.servers": "example.com:9092",\n    "security.protocol": "SSL"\n}\n'})}),"\n",(0,i.jsx)(n.h4,{id:"sasl-authentication-and-with-encryption-ssl",children:"SASL authentication and with encryption (SSL)"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-json",children:'"config": {\n    "topic": "book-fair-sales",\n    "start_from": "earliest",\n    "bootstrap.servers": "example.com:9092",\n    "security.protocol": "SASL_SSL",\n    "sasl.mechanism": "SCRAM-SHA-256",\n    "sasl.username": "<USER>",\n    "sasl.password": "<PASSWORD>"\n}\n'})}),"\n",(0,i.jsxs)(n.p,{children:["For example, at the time of writing (15 May 2024),\nConfluent Cloud Kafka C client tutorial configuration\nuses ",(0,i.jsx)(n.code,{children:"SASL_SSL"})," with SASL mechanism ",(0,i.jsx)(n.code,{children:"PLAIN"})," (see\n",(0,i.jsx)(n.a,{href:"https://developer.confluent.io/get-started/c/#kafka-setup",children:"their tutorial"}),",\nselect Kafka Location: Confluent Cloud, and then go to the Configuration tab)."]}),"\n",(0,i.jsx)(n.h4,{id:"ssl-with-pem-keys",children:"SSL with PEM keys"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-json",children:'"config": {\n    "topic": "book-fair-sales",\n    "start_from": "earliest",\n    "bootstrap.servers": "example.com:9092",\n    "security.protocol": "SSL",\n    "ssl.ca.pem": "-----BEGIN CERTIFICATE-----TOPSECRET0\\n-----END CERTIFICATE-----\\n",\n    "ssl.key.pem": "-----BEGIN CERTIFICATE-----TOPSECRET1\\n-----END CERTIFICATE-----\\n",\n    "ssl.certificate.pem": "-----BEGIN CERTIFICATE-----TOPSECRET2\\n-----END CERTIFICATE-----\\n"\n}\n'})}),"\n",(0,i.jsxs)(n.p,{children:["PEM-encoded certificates can be passed directly in the configuration using\n",(0,i.jsx)(n.code,{children:"ssl.*.pem"})," keys."]}),"\n",(0,i.jsx)(n.h5,{id:"multiple-certificates",children:"Multiple certificates"}),"\n",(0,i.jsxs)(n.p,{children:["librdkafka only accepts multiple certificates when provided via\n",(0,i.jsx)(n.code,{children:"ssl.certificate.location"})," keys (file paths) rather than directly\nwith ",(0,i.jsx)(n.code,{children:"ssl.certificate.pem"}),"."]}),"\n",(0,i.jsxs)(n.p,{children:["This is documented in\n",(0,i.jsx)(n.a,{href:"https://github.com/confluentinc/librdkafka/issues/3225",children:"librdkafka issue #3225"}),"."]}),"\n",(0,i.jsx)(n.p,{children:"To work around this limitation, Feldera handles PEM-encoded certificates and\nkeys by:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["Storing the value passed to ",(0,i.jsx)(n.code,{children:"ssl.certificate.pem"})," into a file."]}),"\n",(0,i.jsx)(n.li,{children:"Naming the file using the SHA256 hash of the data."}),"\n",(0,i.jsxs)(n.li,{children:["Replacing the ",(0,i.jsx)(n.code,{children:"ssl.certificate.pem"})," configuration option with\n",(0,i.jsx)(n.code,{children:"ssl.certificate.location"})," option that points to the newly saved file."]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Example:"})}),"\n",(0,i.jsx)(n.p,{children:"The updated configuration would look like:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-json",children:'"config": {\n    ...,\n    "ssl.certificate.location": "path/to/certificate.pem"\n}\n'})}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsxs)(n.p,{children:["\u26a0\ufe0f"," If both ",(0,i.jsx)(n.code,{children:"ssl.certificate.pem"})," and ",(0,i.jsx)(n.code,{children:"ssl.certificate.location"})," are set\nthe latter will be overwritten."]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"using-kafka-as-a-debezium-transport",children:"Using Kafka as a Debezium transport"}),"\n",(0,i.jsxs)(n.p,{children:["The Kafka connector can be used to ingest data from a source via\nDebezium.  For information on how to setup Debezium integration for Feldera, see\n",(0,i.jsx)(n.a,{href:"debezium",children:"Debezium connector documentation"}),"."]}),"\n",(0,i.jsx)(n.h3,{id:"connecting-to-aws-msk-with-iam-sasl",children:"Connecting to AWS MSK with IAM SASL"}),"\n",(0,i.jsx)(n.p,{children:"Example of reading data from AWS MSK with IAM SASL."}),"\n",(0,i.jsxs)(n.admonition,{type:"important",children:[(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["AWS credentials must either be set as Environment Variables or present in ",(0,i.jsx)(n.code,{children:"~/.aws/credentials"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"sasl.mechanism"})," must be set to ",(0,i.jsx)(n.code,{children:"OAUTHBEARER"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"security.protocol"})," must be set to ",(0,i.jsx)(n.code,{children:"SASL_SSL"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["When the ",(0,i.jsx)(n.code,{children:"sasl.mechanism"})," is ",(0,i.jsx)(n.code,{children:"OAUTHBEARER"}),", the AWS region for MSK must either be set via the environment\nvariable ",(0,i.jsx)(n.code,{children:"AWS_REGION"})," or the ",(0,i.jsx)(n.code,{children:"region"})," field in connector definition as in the example below."]}),"\n"]}),(0,i.jsx)(n.p,{children:"Other protocols and mechanisms aren't supported."})]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:'CREATE TABLE INPUT (\n   ... -- columns omitted\n) WITH (\n   \'connectors\' = \'[\n    {\n      "transport": {\n          "name": "kafka_input",\n          "config": {\n              "bootstrap.servers": "broker-1.kafka.region.amazonaws.com:9098,broker-2.kafka.region.amazonaws.com:9098",\n              "sasl.mechanism": "OAUTHBEARER",\n              "security.protocol": "SASL_SSL",\n              "region": "<AWS_REGION>",\n              "topic": "<TOPIC>"\n          }\n      },\n      "format": {\n          "name": "json",\n          "config": {\n              "update_format": "insert_delete",\n              "array": false\n          }\n      }\n   }\n   ]\'\n);\n'})}),"\n",(0,i.jsxs)(n.h2,{id:"accessing-kafka-metadata",children:[(0,i.jsx)("a",{name:"metadata"}),"Accessing Kafka metadata"]}),"\n",(0,i.jsx)(n.p,{children:"Kafka messages include several metadata attributes in addition to the payload. These can be extracted by the Kafka connector and accessed from SQL:"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Metadata attribute"}),(0,i.jsx)(n.th,{children:"SQL type"}),(0,i.jsxs)(n.th,{children:[(0,i.jsx)(n.code,{children:"CONNECTOR_METADATA()"})," field"]}),(0,i.jsx)(n.th,{children:"Configuration option"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Message headers"}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"MAP<STRING, VARBINARY>"})}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"kafka_headers"})}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"include_headers"})})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Topic name"}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"VARCHAR"})}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"kafka_topic"})}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"include_topic"})})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Partition"}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"INT"})}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"kafka_partition"})}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"include_partition"})})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Message offset"}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"BIGINT"})}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"kafka_offset"})}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"include_offset"})})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Timestamp"}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"TIMESTAMP"})}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"kafka_timestamp"})}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"include_timestamp"})})]})]})]}),"\n",(0,i.jsx)(n.p,{children:"Some applications need to ingest and store these attributes alongside the message payload.\nThe steps below describe how to extract and use Kafka metadata in SQL tables."}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Enable metadata extraction in the Kafka connector."}),"\nUse the configuration options listed in the table above to enable only the metadata fields your application needs.\nExtracting unnecessary attributes adds overhead to ingestion and processing."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Use metadata values to populate table columns."}),"\nEnabled metadata attributes are exposed via the ",(0,i.jsx)(n.code,{children:"CONNECTOR_METADATA()"})," function, which returns a\n",(0,i.jsx)(n.code,{children:"VARIANT"})," containing a map with all selected attributes. You can reference these values in ",(0,i.jsx)(n.code,{children:"DEFAULT"}),"\nexpressions to initialize table columns:"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:'create table my_table(\n    x int,\n    kafka_headers MAP<STRING, VARBINARY> DEFAULT CAST(CONNECTOR_METADATA()[\'kafka_headers\'] as MAP<STRING, VARBINARY>),\n    kafka_timestamp TIMESTAMP DEFAULT CAST(CONNECTOR_METADATA()[\'kafka_timestamp\'] as TIMESTAMP),\n    kafka_topic VARCHAR DEFAULT CAST(CONNECTOR_METADATA()[\'kafka_topic\'] AS VARCHAR),\n    kafka_offset BIGINT DEFAULT CAST(CONNECTOR_METADATA()[\'kafka_offset\'] AS BIGINT),\n    kafka_partition INT DEFAULT CAST(CONNECTOR_METADATA()[\'kafka_partition\'] AS INT)\n) with (\n    \'materialized\' = \'true\',\n    \'connectors\' = \'[{\n      "transport": {\n          "name": "kafka_input",\n          "config": {\n              "topic": "meta_topic",\n              "start_from": "earliest",\n              "bootstrap.servers": "localhost:19092",\n              "include_headers": true,\n              "include_topic": true,\n              "include_offset": true,\n              "include_partition": true,\n              "include_timestamp": true\n          }\n      },\n      "format": {\n          "name": "json",\n          "config": {\n              "update_format": "raw",\n              "array": false\n          }\n      }\n  }]\');\n'})}),"\n",(0,i.jsx)(n.h3,{id:"converting-kafka-header-values-to-strings",children:"Converting Kafka header values to strings"}),"\n",(0,i.jsxs)(n.p,{children:["Kafka headers can contain arbitrary byte arrays, but in practice they typically hold UTF-8\u2013encoded strings.\nUse the ",(0,i.jsx)(n.code,{children:"BIN2UTF8"})," function to convert binary values to text:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:"create materialized view v as\nselect\n  BIN2UTF8(kafka_headers['my_header']) as my_header\nfrom t;\n"})}),"\n",(0,i.jsx)(n.h2,{id:"tolerating-missing-data-on-resume",children:"Tolerating missing data on resume"}),"\n",(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(n.code,{children:"resume_earliest_if_data_expires"})," setting controls how the Kafka\ninput connector behaves when a pipeline configured with at-least-once\n",(0,i.jsx)(n.a,{href:"/pipelines/fault-tolerance",children:"fault tolerance"})," resumes from a\ncheckpoint and the configured Kafka topic no longer has data at the\noffsets saved in the checkpoints:"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:["If ",(0,i.jsx)(n.code,{children:"resume_earliest_if_data_expires"})," is false, which is the default,\nthen the connector will report an error and the pipeline will fail\nto start.  This behavior makes sense because it is no longer\npossible to continue the pipeline from where it left off."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:["If ",(0,i.jsx)(n.code,{children:"resume_earliest_if_data_expires"})," is true, then the connector\nwill log a warning and start reading data from the earliest offsets\nnow available in the topic.  This is reasonable behavior in the\nspecial case where some errors were detected in the data in the\nKafka topic and the topic was deleted and recreated with correct\ndata starting at the point of an earlier checkpoint, and the\npipeline was restarted from that checkpoint."]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"This setting only has an effect when a Kafka topic cannot be read at\nthe checkpointed offsets.  It has no effect if fault tolerance is not\nenabled, or if exactly once fault tolerance is enabled, or at any time\nother than the point of resuming from a checkpoint."}),"\n",(0,i.jsx)(n.h2,{id:"partition-changes",children:"Partition changes"}),"\n",(0,i.jsx)(n.p,{children:"Kafka supports increasing, but not decreasing, the number of\npartitions in a topic.  Feldera will only read partitions that existed\nin a topic at the time that the pipeline was started or resumed from a\ncheckpoint.  To make a running Feldera pipeline start to read newly\nadded partitions, stop the pipeline with a checkpoint and then resume\nit."}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsxs)(n.p,{children:["If ",(0,i.jsx)(n.code,{children:"partitions"})," is set to a list of partition numbers or\n",(0,i.jsx)(n.code,{children:"start_from"})," is set to a list of partition offsets, this is not\npossible.  Instead, force-stop the pipeline, clear its storage, change\nthe configuration, and restart the pipeline from an empty state."]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"additional-resources",children:"Additional resources"}),"\n",(0,i.jsx)(n.p,{children:"For more information, see:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.a,{href:"/tutorials/basics/part3#step-2-configure-kafkaredpanda-connectors",children:"Tutorial section"})," which involves\ncreating a Kafka input connector."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:["Data formats such as ",(0,i.jsx)(n.a,{href:"/formats/json",children:"JSON"})," and\n",(0,i.jsx)(n.a,{href:"/formats/csv",children:"CSV"})]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:["Overview of Kafka configuration options:\n",(0,i.jsx)(n.a,{href:"https://github.com/confluentinc/librdkafka/blob/master/CONFIGURATION.md",children:"librdkafka options"})]}),"\n"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(l,{...e})}):l(e)}}}]);