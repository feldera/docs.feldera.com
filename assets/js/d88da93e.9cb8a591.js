"use strict";(self.webpackChunkfeldera_docs=self.webpackChunkfeldera_docs||[]).push([[1457],{28453:(e,n,t)=>{t.d(n,{R:()=>s,x:()=>a});var r=t(96540);const o={},c=r.createContext(o);function s(e){const n=r.useContext(c);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:s(e.components),r.createElement(c.Provider,{value:n},e.children)}},84166:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>i,contentTitle:()=>a,default:()=>h,frontMatter:()=>s,metadata:()=>r,toc:()=>l});const r=JSON.parse('{"id":"connectors/sinks/confluent-jdbc","title":"Confluent JDBC output connector","description":"This page describes configuration options specific to integration with the Confluent JDBC sink connector.","source":"@site/docs/connectors/sinks/confluent-jdbc.md","sourceDirName":"connectors/sinks","slug":"/connectors/sinks/confluent-jdbc","permalink":"/connectors/sinks/confluent-jdbc","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"docsSidebar","previous":{"title":"Kafka","permalink":"/connectors/sinks/kafka"},"next":{"title":"Redis","permalink":"/connectors/sinks/redis"}}');var o=t(74848),c=t(28453);const s={},a="Confluent JDBC output connector",i={},l=[{value:"Prerequisites",id:"prerequisites",level:2},{value:"Step 1: Create Kafka topics",id:"step-1-create-kafka-topics",level:2},{value:"Step 2: Create a JDBC Sink Connector",id:"step-2-create-a-jdbc-sink-connector",level:2},{value:"Step 3: Create Feldera output connectors",id:"step-3-create-feldera-output-connectors",level:2}];function d(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,c.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"confluent-jdbc-output-connector",children:"Confluent JDBC output connector"})}),"\n",(0,o.jsx)(n.admonition,{type:"note",children:(0,o.jsxs)(n.p,{children:["This page describes configuration options specific to integration with the Confluent JDBC sink connector.\nSee ",(0,o.jsx)(n.a,{href:"/connectors/",children:"top-level connector documentation"})," for general information\nabout configuring input and output connectors."]})}),"\n",(0,o.jsxs)(n.p,{children:["The ",(0,o.jsx)(n.a,{href:"https://docs.confluent.io/kafka-connectors/jdbc/current/sink-connector/overview.html",children:"Confluent JDBC Sink Connector"})," for\n",(0,o.jsx)(n.a,{href:"https://docs.confluent.io/platform/current/connect/",children:"Kafka Connect"})," allows exporting data from Kafka\ntopics to any relational database with a JDBC driver.  Feldera integrates with this connector\nto write a stream of changes to a SQL view to an external database via a Kafka topic."]}),"\n",(0,o.jsxs)(n.p,{children:["The Confluent JDBC connector supports JSON and Avro-based data formats.  Feldera currently only supports the\nAvro format expected by this connector.  In this format, the key of the Kafka message contains the primary key\nof the target table. The value component of the message contains the new or updated value for this primary key\nor ",(0,o.jsx)(n.code,{children:"null"})," if the key is to be deleted from the table."]}),"\n",(0,o.jsxs)(n.p,{children:["Because this connector uses the ",(0,o.jsx)(n.a,{href:"kafka",children:"Kafka output connector"}),", it\nsupports ",(0,o.jsx)(n.a,{href:"/pipelines/fault-tolerance",children:"fault tolerance"})," too."]}),"\n",(0,o.jsx)(n.p,{children:"Setting up a Confluent JDBC Sink Connector integration involves three steps:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.a,{href:"#step-1-create-kafka-topics",children:(0,o.jsx)(n.strong,{children:"Create Kafka topics"})})}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.a,{href:"#step-2-create-a-jdbc-sink-connector",children:(0,o.jsx)(n.strong,{children:"Create a JDBC Sink Connector"})})}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.a,{href:"#step-3-create-feldera-output-connectors",children:(0,o.jsx)(n.strong,{children:"Configure Feldera output connectors"})})}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,o.jsx)(n.p,{children:"The Confluent JDBC Sink connector is built on top of the Kafka Connect platform.\nYou will need to configure the following services:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Kafka"}),"\n",(0,o.jsx)(n.li,{children:"Kafka Connect"}),"\n",(0,o.jsx)(n.li,{children:"A Kafka schema registry"}),"\n",(0,o.jsxs)(n.li,{children:["Confluent JDBC plugin for Kafka Connect (see ",(0,o.jsx)(n.a,{href:"https://docs.confluent.io/kafka-connectors/jdbc/current/sink-connector/",children:"Confluent documentation"})," for details)."]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"step-1-create-kafka-topics",children:"Step 1: Create Kafka topics"}),"\n",(0,o.jsx)(n.p,{children:"Create a separate Kafka topic for each SQL view you would like to connect to an external database."}),"\n",(0,o.jsx)(n.h2,{id:"step-2-create-a-jdbc-sink-connector",children:"Step 2: Create a JDBC Sink Connector"}),"\n",(0,o.jsx)(n.p,{children:"Use the Kafka Connect REST API to create a sink connector to stream\nchanges to the database change from Kafka topics.  The following command was\ntested with the Confluent JDBC Sink Connector v10.7.11 and Kafka Connect v3.7.0:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:'curl -i -X \\\n  POST -H "Accept:application/json" -H "Content-Type:application/json" \\\n  [KAFKA CONNECT HOSTNAME:PORT]/connectors/ -d \\\n  \'{\n      "name": "my-connector",\n      "config": {\n            "connector.class": "io.confluent.connect.jdbc.JdbcSinkConnector",\n            "dialect.name": "PostgreSqlDatabaseDialect",\n            "connection.url": "jdbc:postgresql://postgres:5432/my_database",\n            "connection.user": "postgres",\n            "connection.password": "postgres",\n            "insert.mode": "upsert",\n            "delete.enabled": true,\n            "pk.mode": "record_key",\n            "auto.create": true,\n            "auto.evolve": true,\n            "schema.evolution": "basic",\n            "database.time_zone": "UTC",\n            "topics": "my_table1,my_table2,my_table3",\n            "errors.deadletterqueue.topic.name": "dlq",\n            "errors.deadletterqueue.context.headers.enable": true,\n            "errors.deadletterqueue.topic.replication.factor": 1,\n            "errors.tolerance": "all",\n            "key.converter.key.subject.name.strategy": "io.confluent.kafka.serializers.subject.TopicNameStrategy",\n            "value.converter.value.subject.name.strategy": "io.confluent.kafka.serializers.subject.TopicNameStrategy",\n            "key.converter": "io.confluent.connect.avro.AvroConverter",\n            "value.converter": "io.confluent.connect.avro.AvroConverter",\n            "key.converter.schemas.enable": true,\n            "value.converter.schemas.enable": true,\n            "key.converter.schema.registry.url": "http://redpanda:8081",\n            "value.converter.schema.registry.url": "http://redpanda:8081"\n        }\n  }\'\n'})}),"\n",(0,o.jsx)(n.p,{children:"We can break this down into required settings, that must be used for the connector to work correctly with Feldera,\nand recommended settings, which can be tuned to your specific use case."}),"\n",(0,o.jsx)(n.p,{children:"Required settings:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["Use the JDBC Sink connector plugin:","\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.code,{children:'"connector.class": "io.confluent.connect.jdbc.JdbcSinkConnector"'})}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["Use Avro encoding for keys and values; retrieve Avro schemas from a schema registry:","\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.code,{children:'"key.converter": "io.confluent.connect.avro.AvroConverter"'})}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.code,{children:'"value.converter": "io.confluent.connect.avro.AvroConverter"'})}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.code,{children:'"key.converter.schemas.enable": true'})}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.code,{children:'"value.converter.schemas.enable": true'})}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.code,{children:'"key.converter.schema.registry.url": "<schema_registry_url>"'})}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.code,{children:'"value.converter.schema.registry.url": "<schema_registry_url>"'})}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["Derive schema subject name from the Kafka topic name:","\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.code,{children:'"key.converter.key.subject.name.strategy": "io.confluent.kafka.serializers.subject.TopicNameStrategy"'})}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.code,{children:'"value.converter.value.subject.name.strategy": "io.confluent.kafka.serializers.subject.TopicNameStrategy"'})}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["Configure the connector to support arbitrary database changes (inserts, deletes, and updates); retrieve\nprimary key value for the target table from the Kafka message key:","\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.code,{children:'"insert.mode": "upsert"'})}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.code,{children:'"delete.enabled": true'})}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.code,{children:'"pk.mode": "record_key"'})}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["Database connectivity (specific settings depend on your database type)","\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:'"dialect.name": "PostgreSqlDatabaseDialect"'})," - specify your database type"]}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.code,{children:'"connection.url": "jdbc:postgresql://postgres:5432/my_database"'})}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.code,{children:'"connection.user": "postgres"'})}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.code,{children:'"connection.password": "postgres"'})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"Recommended settings:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["Automatically derive","\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.code,{children:'"auto.create": true'})}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.code,{children:'"auto.evolve": true'})}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.code,{children:'"schema.evolution": "basic"'})}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["Configure Dead Letter Queue for Kafka messages that couldn't be successfully processed by the JDBC connector:","\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.code,{children:'"errors.deadletterqueue.topic.name": "dlq"'})}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.code,{children:'"errors.deadletterqueue.context.headers.enable": true'})}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.code,{children:'"errors.deadletterqueue.topic.replication.factor": 1'})}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.code,{children:'"errors.tolerance": "all"'})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"step-3-create-feldera-output-connectors",children:"Step 3: Create Feldera output connectors"}),"\n",(0,o.jsxs)(n.p,{children:["Configure an output connector for each Feldera SQL view that will send changes to the JDBC connector.\nUse the ",(0,o.jsx)(n.code,{children:"kafka_output"})," transport with ",(0,o.jsx)(n.code,{children:"avro"})," output format. Set the following Avro encoder parameters:"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:'"update_format": "confluent_jdbc"'})," - use Confluent JDBC connector-compatible Avro encoding."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:'"registry_urls": ["<registry_url2>", "<registry_url2>", ...]'})," - the connector will publish Avro\nschemas for key and value components of Kafka messages to the specified schema registry."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:'"key_fields": ["key_field1", "key_field2", ...]'})," - list of SQL view columns that form the primary\nkey in the external database table. When not specified, the encoder assumes that all columns in the\nview are part of the primary key."]}),"\n"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-sql",children:'create view my_view\nWITH (\n  \'connectors\' = \'[{\n    "transport": {\n      "name": "kafka_output",\n      "config": {\n        "bootstrap.servers": "redpanda:9092",\n        "topic": "my_topic"\n      }\n    },\n    "format": {\n      "name": "avro",\n      "config": {\n        "update_format": "confluent_jdbc",\n        "registry_urls": ["http://redpanda:8081"],\n        "key_fields": ["id"]\n      }\n    }\n  }]\'\n)\nas select * from test_table;\n'})}),"\n",(0,o.jsx)(n.admonition,{type:"note",children:(0,o.jsxs)(n.p,{children:["The user is responsible for selecting a set of columns for the ",(0,o.jsx)(n.code,{children:"key_fields"})," property that are\nguaranteed to have unique values.  Failure to choose a unique key may lead to data loss."]})}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["For more details on Avro support in Feldera, please refer to the ",(0,o.jsx)(n.a,{href:"/formats/avro",children:"Avro Format Documentation"}),"."]}),"\n",(0,o.jsxs)(n.li,{children:["For more information on configuring Kafka transport, visit the ",(0,o.jsx)(n.a,{href:"/connectors/sinks/kafka",children:"Kafka Sink Connector Documentation"}),"."]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,c.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}}}]);